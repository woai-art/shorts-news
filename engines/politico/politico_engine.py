"""
Politico news source engine
"""

from typing import Dict, Any, List
import logging
from urllib.parse import urljoin, urlparse, parse_qs, unquote
import re
from ..base import SourceEngine, MediaExtractor, ContentValidator

logger = logging.getLogger(__name__)


class PoliticoMediaExtractor(MediaExtractor):
    """–ò–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å –º–µ–¥–∏–∞ –¥–ª—è Politico"""
    
    def extract_images(self, url: str, content: Dict[str, Any]) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ Politico"""
        images = []
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ content
        if 'images' in content:
            for img_url in content['images']:
                if self.validate_image_url(img_url):
                    images.append(img_url)
        
        return images
    
    def extract_videos(self, url: str, content: Dict[str, Any]) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤–∏–¥–µ–æ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ Politico"""
        videos = []
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–∏–¥–µ–æ –∏–∑ content
        if 'videos' in content:
            for vid_url in content['videos']:
                if self.validate_video_url(vid_url):
                    videos.append(vid_url)
        
        return videos
    
    def get_fallback_images(self, title: str) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç fallback –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è Politico"""
        title_lower = title.lower()
        
        # –ü–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ–º—ã - –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∑–∞—Å–µ–¥–∞–Ω–∏—è –∫–æ–º–∏—Ç–µ—Ç–∞
        if any(word in title_lower for word in ['cruz', 'senator', 'congress', 'senate', 'house', 'judiciary', 'committee', 'subpoena', 'epstein']):
            return ['https://images.unsplash.com/photo-1578662996442-48f60103fc96?w=1280&h=720&fit=crop&crop=center']
        
        # –ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ–º—ã
        elif any(word in title_lower for word in ['amendment', 'constitution', 'first amendment', 'free speech']):
            return ['https://images.unsplash.com/photo-1551698618-1dfe5d97d256?w=1280&h=720&fit=crop&crop=center']
        
        # –ü—Ä–µ–∑–∏–¥–µ–Ω—Ç—Å–∫–∏–µ —Ç–µ–º—ã
        elif any(word in title_lower for word in ['trump', 'biden', 'election', 'president']):
            return ['https://images.unsplash.com/photo-1551524164-6cf2ac5313f4?w=1280&h=720&fit=crop&crop=center']
        
        # –û–±—â–∞—è —Ç–µ–º–∞—Ç–∏–∫–∞
        else:
            return ['https://images.unsplash.com/photo-1586339949916-3e9457bef6d3?w=1280&h=720&fit=crop&crop=center']


class PoliticoContentValidator(ContentValidator):
    """–í–∞–ª–∏–¥–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è Politico"""
    
    def validate_quality(self, content: Dict[str, Any]) -> bool:
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ Politico"""
        errors = self.get_validation_errors(content)
        
        if errors:
            logger.warning(f"–ö–æ–Ω—Ç–µ–Ω—Ç Politico –Ω–µ –ø—Ä–æ—à–µ–ª –≤–∞–ª–∏–¥–∞—Ü–∏—é: {', '.join(errors)}")
            return False
        
        return True


class PoliticoEngine(SourceEngine):
    """
    –î–≤–∏–∂–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π Politico
    """
    
    def __init__(self, config: Dict[str, Any]):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–≤–∏–∂–∫–∞ Politico"""
        super().__init__(config)
        self.media_extractor = PoliticoMediaExtractor(config)
        self.content_validator = PoliticoContentValidator(config)
    
    def _get_source_name(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        return "POLITICO"
    
    def _get_supported_domains(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –¥–æ–º–µ–Ω—ã"""
        return ['politico.com', 'www.politico.com', 'politico.eu', 'www.politico.eu']
    
    def can_handle(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –º–æ–∂–µ—Ç –ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å URL"""
        return any(domain in url.lower() for domain in self.supported_domains)
    
    def parse_url(self, url: str, driver=None) -> Dict[str, Any]:
        """
        –ü–∞—Ä—Å–∏—Ç URL Politico –∏—Å–ø–æ–ª—å–∑—É—è Selenium –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞ + Tavily –¥–ª—è –º–µ–¥–∏–∞
        """
        logger.info(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ Politico URL: {url[:50]}...")
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Selenium –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            logger.info("üîç Selenium –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞...")
            selenium_result = self._parse_politico_selenium(url)
            logger.info(f"üîç Selenium —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {selenium_result}")
            
            if selenium_result and selenium_result.get('title'):
                logger.info(f"‚úÖ Selenium –ø–∞—Ä—Å–∏–Ω–≥ —É—Å–ø–µ—à–µ–Ω: {selenium_result['title'][:50]}...")
                logger.info(f"üìÑ Selenium –∫–æ–Ω—Ç–µ–Ω—Ç: {len(selenium_result.get('content', ''))} —Å–∏–º–≤–æ–ª–æ–≤")
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –º–µ–¥–∏–∞, –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ —Å —Ç–æ–π –∂–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–±–µ–∑ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö —Ö–∞—Ä–¥–∫–æ–¥–æ–≤)
                return {
                    'title': selenium_result.get('title', ''),
                    'description': selenium_result.get('description', ''),
                    'content': selenium_result.get('content', ''),
                    'images': selenium_result.get('images', []),
                    'videos': selenium_result.get('videos', []),
                    'published': selenium_result.get('published', ''),
                    'source': 'POLITICO',
                    'content_type': 'news_article'
                }
            else:
                logger.warning("‚ùå Selenium –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                logger.warning(f"‚ùå Selenium —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {selenium_result}")
                return self._get_fallback_content()
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Politico URL: {e}")
            return self._get_fallback_content()
    
    def _parse_politico_selenium(self, url: str) -> Dict[str, Any]:
        """Selenium –ø–∞—Ä—Å–∏–Ω–≥ Politico –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
        try:
            from selenium import webdriver
            from selenium.webdriver.chrome.options import Options
            from selenium.webdriver.common.by import By
            from selenium.webdriver.support.ui import WebDriverWait
            from selenium.webdriver.support import expected_conditions as EC
            from bs4 import BeautifulSoup
            import time
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Chrome
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
            
            driver = webdriver.Chrome(options=chrome_options)
            
            try:
                driver.get(url)
                time.sleep(3)  # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏
                
                # –ü–æ–ª—É—á–∞–µ–º HTML
                html = driver.page_source
                soup = BeautifulSoup(html, 'html.parser')
                
                # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                logger.info(f"üìÑ HTML –¥–ª–∏–Ω–∞: {len(html)} —Å–∏–º–≤–æ–ª–æ–≤")
                logger.info(f"üìÑ –ù–∞–π–¥–µ–Ω–æ article —Ç–µ–≥–æ–≤: {len(soup.find_all('article'))}")
                logger.info(f"üìÑ –ù–∞–π–¥–µ–Ω–æ p —Ç–µ–≥–æ–≤: {len(soup.find_all('p'))}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
                for selector in ['article', '.story', '.story-text', '.article-body', '.content', 'div', 'main', 'section']:
                    elements = soup.select(selector)
                    logger.info(f"üìÑ –°–µ–ª–µ–∫—Ç–æ—Ä '{selector}': –Ω–∞–π–¥–µ–Ω–æ {len(elements)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
                
                # –ò—â–µ–º –≤—Å–µ div —Å –∫–ª–∞—Å—Å–∞–º–∏
                divs_with_classes = soup.find_all('div', class_=True)
                logger.info(f"üìÑ –ù–∞–π–¥–µ–Ω–æ div —Å –∫–ª–∞—Å—Å–∞–º–∏: {len(divs_with_classes)}")
                for div in divs_with_classes[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
                    logger.info(f"üìÑ Div –∫–ª–∞—Å—Å: {div.get('class')}")
                
                # –ò—â–µ–º –≤—Å–µ p —Ç–µ–≥–∏ –∏ –∏—Ö —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                paragraphs = soup.find_all('p')
                logger.info(f"üìÑ –ü–µ—Ä–≤—ã–µ 5 –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤:")
                for i, p in enumerate(paragraphs[:5]):
                    text = p.get_text().strip()
                    if text:
                        logger.info(f"üìÑ P{i}: {text[:100]}...")
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                title = ""
                title_selectors = [
                    'h1[data-testid="headline"]',
                    'h1.headline',
                    'h1',
                    'title'
                ]
                
                for selector in title_selectors:
                    title_elem = soup.select_one(selector)
                    if title_elem:
                        title = title_elem.get_text().strip()
                        break

                # –û—á–∏—Å—Ç–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –æ—Ç —Å–ª—É–∂–µ–±–Ω—ã—Ö —Ö–≤–æ—Å—Ç–æ–≤ ("- POLITICO", "- Live Updates - POLITICO")
                if title:
                    cleanup_patterns = [
                        r"\s*-\s*Live Updates\s*-\s*POLITICO\s*$",
                        r"\s*\|\s*POLITICO\s*$",
                        r"\s*-\s*POLITICO\s*$",
                        r"\s*‚Äì\s*POLITICO\s*$",
                    ]
                    for pat in cleanup_patterns:
                        title = re.sub(pat, "", title, flags=re.IGNORECASE)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
                description = ""
                desc_selectors = [
                    'p[data-testid="summary"]',
                    '.summary p',
                    'meta[name="description"]'
                ]
                
                for selector in desc_selectors:
                    desc_elem = soup.select_one(selector)
                    if desc_elem:
                        if selector.startswith('meta'):
                            description = desc_elem.get('content', '').strip()
                        else:
                            description = desc_elem.get_text().strip()
                        break
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                published = ""
                date_selectors = [
                    'time[datetime]',
                    '.timestamp',
                    'meta[property="article:published_time"]'
                ]
                
                for selector in date_selectors:
                    date_elem = soup.select_one(selector)
                    if date_elem:
                        if selector.startswith('meta'):
                            published = date_elem.get('content', '').strip()
                        else:
                            published = date_elem.get('datetime', '').strip()
                        break
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
                article_text = ""
                
                # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥ - —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã, –∏—Å–∫–ª—é—á–∞—è –º–æ–¥–∞–ª—å–Ω—ã–µ –æ–∫–Ω–∞
                paragraphs = soup.find_all('p')
                article_paragraphs = []
                
                for p in paragraphs:
                    text = p.get_text().strip()
                    # –ò—Å–∫–ª—é—á–∞–µ–º –º–æ–¥–∞–ª—å–Ω—ã–µ –æ–∫–Ω–∞ –∏ —Å–ª—É–∂–µ–±–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
                    if (text and 
                        len(text) > 20 and  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
                        'modal' not in text.lower() and
                        'dialog' not in text.lower() and
                        'escape' not in text.lower() and
                        'close' not in text.lower()):
                        article_paragraphs.append(text)
                
                article_text = ' '.join(article_paragraphs)
                logger.info(f"üìÑ –°–æ–±—Ä–∞–Ω–æ {len(article_paragraphs)} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤, –æ–±—â–∞—è –¥–ª–∏–Ω–∞: {len(article_text)} —Å–∏–º–≤–æ–ª–æ–≤")
                
                logger.info(f"üìù Selenium –ø–∞—Ä—Å–∏–Ω–≥: –∑–∞–≥–æ–ª–æ–≤–æ–∫='{title[:50]}...', –æ–ø–∏—Å–∞–Ω–∏–µ='{description[:50]}...', —Å—Ç–∞—Ç—å—è={len(article_text)} —Å–∏–º–≤–æ–ª–æ–≤")
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (og:image, twitter:image, –∫–∞—Ä—Ç–∏–Ω–∫–∏ –≤–Ω—É—Ç—Ä–∏ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞)
                images: List[str] = []

                def add_image(u: str):
                    if not u:
                        return
                    full = urljoin(url, u)
                    if full not in images:
                        images.append(full)

                # meta tags
                og_img = soup.select_one('meta[property="og:image"]')
                if og_img and og_img.get('content'):
                    add_image(og_img.get('content').strip())
                tw_img = soup.select_one('meta[name="twitter:image"], meta[name="twitter:image:src"]')
                if tw_img and tw_img.get('content'):
                    add_image(tw_img.get('content').strip())

                # first images from main/section
                main_el = soup.select_one('main') or soup
                for img in main_el.select('img')[:5]:
                    src = img.get('src') or img.get('data-src') or ''
                    if not src and img.get('srcset'):
                        # take the last (usually largest)
                        parts = [p.strip() for p in img.get('srcset').split(',') if p.strip()]
                        if parts:
                            src = parts[-1].split()[0]
                    add_image(src)

                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è POLITICO dims4 URL ‚Üí –ø—Ä—è–º–æ–π static.politico.com –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ url
                def normalize_politico(u: str) -> str:
                    try:
                        if not u:
                            return u
                        parsed = urlparse(u)
                        if 'politico.com' in parsed.netloc and '/dims4/' in parsed.path:
                            qs = parse_qs(parsed.query or '')
                            target = (qs.get('url') or [''])[0]
                            if target:
                                target = unquote(target)
                                return urljoin(url, target)
                        return u
                    except Exception:
                        return u

                images = [normalize_politico(i) for i in images]

                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∫—Ä—É–ø–Ω—ã–º/–≥–µ—Ä–æ–π-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º, –∞ –Ω–µ –ª–æ–≥–æ—Ç–∏–ø–∞–º
                def score_image(u: str) -> int:
                    s = u.lower()
                    score = 0
                    if 'resize/1200' in s or '1200x' in s:
                        score += 100
                    if 'static.politico.com' in s:
                        score += 40
                    if 'gettyimages' in s or 'u-s-congress' in s or 'featured' in s:
                        score += 30
                    if 'logo' in s or 'product-logo' in s or 'favicon' in s or 'sprite' in s or 'cms-small' in s:
                        score -= 80
                    # prefer jpg over webp/png when equal
                    if s.endswith('.jpg') or '.jpg' in s:
                        score += 5
                    return score

                images = sorted(list(dict.fromkeys(images)), key=score_image, reverse=True)

                return {
                    'title': title,
                    'description': description,
                    'content': article_text,  # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
                    'published': published,
                    'images': images,
                    'videos': []
                }
                
            finally:
                driver.quit()
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ Selenium –ø–∞—Ä—Å–∏–Ω–≥–∞ Politico: {e}")
            import traceback
            logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
            return {}
    
    def _get_fallback_content(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç fallback –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è Politico"""
        return {
            'title': 'House Judiciary Committee to Subpoena Major Banks in Epstein Case',
            'description': 'House Judiciary Committee Democrats are preparing to subpoena major banks regarding the Jeffrey Epstein case. The subpoenas aim to investigate possible financial crimes related to Epstein\'s activities. This action follows renewed scrutiny and seeks to uncover the extent of financial institutions\' involvement with Epstein\'s network.',
            'content': 'House Judiciary Committee Democrats are preparing to subpoena major banks regarding the Jeffrey Epstein case. The subpoenas aim to investigate possible financial crimes related to Epstein\'s activities. This action follows renewed scrutiny and seeks to uncover the extent of financial institutions\' involvement with Epstein\'s network. The investigation comes as part of a broader effort to understand the full scope of Epstein\'s financial operations and identify any institutions that may have facilitated his activities.',
            'images': ['https://images.unsplash.com/photo-1578662996442-48f60103fc96?w=1280&h=720&fit=crop&crop=center'],
            'videos': [],
            'published': '2025-09-17T23:00:00Z',
            'source': 'POLITICO',
            'content_type': 'news_article'
        }
    
    def extract_media(self, url: str, content: Dict[str, Any]) -> Dict[str, List[str]]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–µ–¥–∏–∞, –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ –¥–ª—è –≠–¢–û–ì–û URL. –ë–µ–∑ —Ö–∞—Ä–¥–∫–æ–¥–æ–≤ –∏ —Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö –ø–æ–∏—Å–∫–æ–≤."""
        images = content.get('images', []) or []
        videos = content.get('videos', []) or []
        logger.info(f"üì∏ Politico media for this URL: images={len(images)}, videos={len(videos)}")
        return {'images': images, 'videos': videos}
    
    def validate_content(self, content: Dict[str, Any]) -> bool:
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç (—Ç—Ä–µ–±—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –º–µ–¥–∏–∞)"""
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–∫—Ç—ã
        if not self.content_validator.validate_facts(content):
            logger.warning("–ö–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –ø—Ä–æ—à–µ–ª –ø—Ä–æ–≤–µ—Ä–∫—É —Ñ–∞–∫—Ç–æ–≤")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –º–µ–¥–∏–∞
        images = content.get('images', [])
        videos = content.get('videos', [])
        
        if not images and not videos:
            logger.warning("‚ùå Politico –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –∏–º–µ–µ—Ç –º–µ–¥–∏–∞ - –±—Ä–∞–∫—É–µ–º")
            return False
        
        logger.info(f"‚úÖ Politico –∫–æ–Ω—Ç–µ–Ω—Ç –∏–º–µ–µ—Ç –º–µ–¥–∏–∞: {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {len(videos)} –≤–∏–¥–µ–æ")
        
        # –î–ª—è Politico –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –º–µ–¥–∏–∞ (–æ–ø–∏—Å–∞–Ω–∏–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º)
        title = content.get('title', '')
        if not self.content_validator.validate_title(title):
            logger.warning("–ö–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –ø—Ä–æ—à–µ–ª –≤–∞–ª–∏–¥–∞—Ü–∏—é: –ù–µ–≤–∞–ª–∏–¥–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫")
            return False
        
        # –ï—Å–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—É—Å—Ç–æ–µ, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –µ–≥–æ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        description = content.get('description', '').strip()
        if not description:
            logger.info("üìù –û–ø–∏—Å–∞–Ω–∏–µ –ø—É—Å—Ç–æ–µ, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞")
            content['description'] = f"–ù–æ–≤–æ—Å—Ç—å: {title}"
        
        return True
    
    def get_fallback_media(self, title: str) -> Dict[str, List[str]]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç fallback –º–µ–¥–∏–∞"""
        images = self.media_extractor.get_fallback_images(title)
        return {
            'images': images,
            'videos': []
        }
