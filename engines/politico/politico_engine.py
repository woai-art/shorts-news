"""
Politico news source engine
"""

from typing import Dict, Any, List
import logging
from urllib.parse import urljoin, urlparse, parse_qs, unquote
import re
from ..base import SourceEngine, MediaExtractor, ContentValidator

logger = logging.getLogger(__name__)


class PoliticoMediaExtractor(MediaExtractor):
    """Ð˜Ð·Ð²Ð»ÐµÐºÐ°Ñ‚ÐµÐ»ÑŒ Ð¼ÐµÐ´Ð¸Ð° Ð´Ð»Ñ Politico"""
    
    def extract_images(self, url: str, content: Dict[str, Any]) -> List[str]:
        """Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÑ‚ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð¸Ð· ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð° Politico"""
        images = []
        
        # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð¸Ð· content
        if 'images' in content:
            for img_url in content['images']:
                if self.validate_image_url(img_url):
                    images.append(img_url)
        
        return images
    
    def extract_videos(self, url: str, content: Dict[str, Any]) -> List[str]:
        """Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÑ‚ Ð²Ð¸Ð´ÐµÐ¾ Ð¸Ð· ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð° Politico"""
        videos = []
        
        # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð²Ð¸Ð´ÐµÐ¾ Ð¸Ð· content
        if 'videos' in content:
            for vid_url in content['videos']:
                if self.validate_video_url(vid_url):
                    videos.append(vid_url)
        
        return videos
    
    def get_fallback_images(self, title: str) -> List[str]:
        """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ fallback Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð´Ð»Ñ Politico"""
        title_lower = title.lower()
        
        # ÐŸÐ¾Ð»Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ñ‚ÐµÐ¼Ñ‹ - Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ð·Ð°ÑÐµÐ´Ð°Ð½Ð¸Ñ ÐºÐ¾Ð¼Ð¸Ñ‚ÐµÑ‚Ð°
        if any(word in title_lower for word in ['cruz', 'senator', 'congress', 'senate', 'house', 'judiciary', 'committee', 'subpoena', 'epstein']):
            return ['https://images.unsplash.com/photo-1578662996442-48f60103fc96?w=1280&h=720&fit=crop&crop=center']
        
        # ÐšÐ¾Ð½ÑÑ‚Ð¸Ñ‚ÑƒÑ†Ð¸Ð¾Ð½Ð½Ñ‹Ðµ Ñ‚ÐµÐ¼Ñ‹
        elif any(word in title_lower for word in ['amendment', 'constitution', 'first amendment', 'free speech']):
            return ['https://images.unsplash.com/photo-1551698618-1dfe5d97d256?w=1280&h=720&fit=crop&crop=center']
        
        # ÐŸÑ€ÐµÐ·Ð¸Ð´ÐµÐ½Ñ‚ÑÐºÐ¸Ðµ Ñ‚ÐµÐ¼Ñ‹
        elif any(word in title_lower for word in ['trump', 'biden', 'election', 'president']):
            return ['https://images.unsplash.com/photo-1551524164-6cf2ac5313f4?w=1280&h=720&fit=crop&crop=center']
        
        # ÐžÐ±Ñ‰Ð°Ñ Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸ÐºÐ°
        else:
            return ['https://images.unsplash.com/photo-1586339949916-3e9457bef6d3?w=1280&h=720&fit=crop&crop=center']


class PoliticoContentValidator(ContentValidator):
    """Ð’Ð°Ð»Ð¸Ð´Ð°Ñ‚Ð¾Ñ€ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð° Ð´Ð»Ñ Politico"""
    
    def validate_quality(self, content: Dict[str, Any]) -> bool:
        """Ð’Ð°Ð»Ð¸Ð´Ð¸Ñ€ÑƒÐµÑ‚ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð° Politico"""
        errors = self.get_validation_errors(content)
        
        if errors:
            logger.warning(f"ÐšÐ¾Ð½Ñ‚ÐµÐ½Ñ‚ Politico Ð½Ðµ Ð¿Ñ€Ð¾ÑˆÐµÐ» Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸ÑŽ: {', '.join(errors)}")
            return False
        
        return True


class PoliticoEngine(SourceEngine):
    """
    Ð”Ð²Ð¸Ð¶Ð¾Ðº Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð½Ð¾Ð²Ð¾ÑÑ‚ÐµÐ¹ Politico
    """
    
    def __init__(self, config: Dict[str, Any]):
        """Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð´Ð²Ð¸Ð¶ÐºÐ° Politico"""
        super().__init__(config)
        self.media_extractor = PoliticoMediaExtractor(config)
        self.content_validator = PoliticoContentValidator(config)
    
    def _get_source_name(self) -> str:
        """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ°"""
        return "POLITICO"
    
    def _get_supported_domains(self) -> List[str]:
        """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÐ¼Ñ‹Ðµ Ð´Ð¾Ð¼ÐµÐ½Ñ‹"""
        return ['politico.com', 'www.politico.com', 'politico.eu', 'www.politico.eu']
    
    def can_handle(self, url: str) -> bool:
        """ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚, Ð¼Ð¾Ð¶ÐµÑ‚ Ð»Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ URL"""
        return any(domain in url.lower() for domain in self.supported_domains)
    
    def parse_url(self, url: str, driver=None) -> Dict[str, Any]:
        """
        ÐŸÐ°Ñ€ÑÐ¸Ñ‚ URL Politico Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ Selenium Ð´Ð»Ñ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ° + Tavily Ð´Ð»Ñ Ð¼ÐµÐ´Ð¸Ð°
        """
        logger.info(f"ðŸ” ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Politico URL: {url[:50]}...")
        
        try:
            # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Selenium Ð´Ð»Ñ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°
            logger.info("ðŸ” Selenium Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³ Ð´Ð»Ñ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°...")
            selenium_result = self._parse_politico_selenium(url)
            logger.info(f"ðŸ” Selenium Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚: {selenium_result}")
            
            if selenium_result and selenium_result.get('title'):
                logger.info(f"âœ… Selenium Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³ ÑƒÑÐ¿ÐµÑˆÐµÐ½: {selenium_result['title'][:50]}...")
                logger.info(f"ðŸ“„ Selenium ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚: {len(selenium_result.get('content', ''))} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²")
                # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¼ÐµÐ´Ð¸Ð°, Ð¸Ð·Ð²Ð»ÐµÑ‡Ñ‘Ð½Ð½Ñ‹Ðµ Ñ Ñ‚Ð¾Ð¹ Ð¶Ðµ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ (Ð±ÐµÐ· Ð³Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ñ…Ð°Ñ€Ð´ÐºÐ¾Ð´Ð¾Ð²)
                return {
                    'title': selenium_result.get('title', ''),
                    'description': selenium_result.get('description', ''),
                    'content': selenium_result.get('content', ''),
                    'images': selenium_result.get('images', []),
                    'videos': selenium_result.get('videos', []),
                    'published': selenium_result.get('published', ''),
                    'source': 'POLITICO',
                    'content_type': 'news_article'
                }
            else:
                logger.warning("âŒ Selenium Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³ Ð½Ðµ ÑƒÐ´Ð°Ð»ÑÑ, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ fallback")
                logger.warning(f"âŒ Selenium Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚: {selenium_result}")
                return self._get_fallback_content()
                
        except Exception as e:
            logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° Politico URL: {e}")
            return self._get_fallback_content()
    
    def _parse_politico_selenium(self, url: str) -> Dict[str, Any]:
        """Selenium Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³ Politico Ð´Ð»Ñ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°"""
        try:
            from selenium import webdriver
            from selenium.webdriver.chrome.options import Options
            from selenium.webdriver.common.by import By
            from selenium.webdriver.support.ui import WebDriverWait
            from selenium.webdriver.support import expected_conditions as EC
            from bs4 import BeautifulSoup
            import time
            
            # ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Chrome
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
            
            driver = webdriver.Chrome(options=chrome_options)
            
            try:
                driver.get(url)
                time.sleep(3)  # Ð–Ð´ÐµÐ¼ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸
                
                # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ HTML
                html = driver.page_source
                soup = BeautifulSoup(html, 'html.parser')
                
                # ÐžÑ‚Ð»Ð°Ð´Ð¾Ñ‡Ð½Ð°Ñ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ
                logger.info(f"ðŸ“„ HTML Ð´Ð»Ð¸Ð½Ð°: {len(html)} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²")
                logger.info(f"ðŸ“„ ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ article Ñ‚ÐµÐ³Ð¾Ð²: {len(soup.find_all('article'))}")
                logger.info(f"ðŸ“„ ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ p Ñ‚ÐµÐ³Ð¾Ð²: {len(soup.find_all('p'))}")
                
                # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð¾ÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ ÑÐµÐ»ÐµÐºÑ‚Ð¾Ñ€Ñ‹
                for selector in ['article', '.story', '.story-text', '.article-body', '.content', 'div', 'main', 'section']:
                    elements = soup.select(selector)
                    logger.info(f"ðŸ“„ Ð¡ÐµÐ»ÐµÐºÑ‚Ð¾Ñ€ '{selector}': Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ {len(elements)} ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð²")
                
                # Ð˜Ñ‰ÐµÐ¼ Ð²ÑÐµ div Ñ ÐºÐ»Ð°ÑÑÐ°Ð¼Ð¸
                divs_with_classes = soup.find_all('div', class_=True)
                logger.info(f"ðŸ“„ ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ div Ñ ÐºÐ»Ð°ÑÑÐ°Ð¼Ð¸: {len(divs_with_classes)}")
                for div in divs_with_classes[:10]:  # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¿ÐµÑ€Ð²Ñ‹Ðµ 10
                    logger.info(f"ðŸ“„ Div ÐºÐ»Ð°ÑÑ: {div.get('class')}")
                
                # Ð˜Ñ‰ÐµÐ¼ Ð²ÑÐµ p Ñ‚ÐµÐ³Ð¸ Ð¸ Ð¸Ñ… ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ð¾Ðµ
                paragraphs = soup.find_all('p')
                logger.info(f"ðŸ“„ ÐŸÐµÑ€Ð²Ñ‹Ðµ 5 Ð¿Ð°Ñ€Ð°Ð³Ñ€Ð°Ñ„Ð¾Ð²:")
                for i, p in enumerate(paragraphs[:5]):
                    text = p.get_text().strip()
                    if text:
                        logger.info(f"ðŸ“„ P{i}: {text[:100]}...")
                
                # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²Ð¾Ðº
                title = ""
                title_selectors = [
                    'h1[data-testid="headline"]',
                    'h1.headline',
                    'h1',
                    'title'
                ]
                
                for selector in title_selectors:
                    title_elem = soup.select_one(selector)
                    if title_elem:
                        title = title_elem.get_text().strip()
                        break

                # ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ° Ð¾Ñ‚ ÑÐ»ÑƒÐ¶ÐµÐ±Ð½Ñ‹Ñ… Ñ…Ð²Ð¾ÑÑ‚Ð¾Ð² ("- POLITICO", "- Live Updates - POLITICO")
                if title:
                    cleanup_patterns = [
                        r"\s*-\s*Live Updates\s*-\s*POLITICO\s*$",
                        r"\s*\|\s*POLITICO\s*$",
                        r"\s*-\s*POLITICO\s*$",
                        r"\s*â€“\s*POLITICO\s*$",
                    ]
                    for pat in cleanup_patterns:
                        title = re.sub(pat, "", title, flags=re.IGNORECASE)
                
                # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ
                description = ""
                desc_selectors = [
                    'p[data-testid="summary"]',
                    '.summary p',
                    'meta[name="description"]'
                ]
                
                for selector in desc_selectors:
                    desc_elem = soup.select_one(selector)
                    if desc_elem:
                        if selector.startswith('meta'):
                            description = desc_elem.get('content', '').strip()
                        else:
                            description = desc_elem.get_text().strip()
                        break
                
                # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð´Ð°Ñ‚Ñƒ Ð¿ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ†Ð¸Ð¸
                published = ""
                date_selectors = [
                    'time[datetime]',
                    '.timestamp',
                    'meta[property="article:published_time"]'
                ]
                
                for selector in date_selectors:
                    date_elem = soup.select_one(selector)
                    if date_elem:
                        if selector.startswith('meta'):
                            published = date_elem.get('content', '').strip()
                        else:
                            published = date_elem.get('datetime', '').strip()
                        break
                
                # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚ ÑÑ‚Ð°Ñ‚ÑŒÐ¸
                article_text = ""
                
                # ÐŸÑ€Ð¾ÑÑ‚Ð¾Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ - ÑÐ¾Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð²ÑÐµ Ð¿Ð°Ñ€Ð°Ð³Ñ€Ð°Ñ„Ñ‹, Ð¸ÑÐºÐ»ÑŽÑ‡Ð°Ñ Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¾ÐºÐ½Ð°
                paragraphs = soup.find_all('p')
                article_paragraphs = []
                
                for p in paragraphs:
                    text = p.get_text().strip()
                    # Ð˜ÑÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¾ÐºÐ½Ð° Ð¸ ÑÐ»ÑƒÐ¶ÐµÐ±Ð½Ñ‹Ðµ Ñ‚ÐµÐºÑÑ‚Ñ‹
                    if (text and 
                        len(text) > 20 and  # ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ð´Ð»Ð¸Ð½Ð°
                        'modal' not in text.lower() and
                        'dialog' not in text.lower() and
                        'escape' not in text.lower() and
                        'close' not in text.lower()):
                        article_paragraphs.append(text)
                
                article_text = ' '.join(article_paragraphs)
                logger.info(f"ðŸ“„ Ð¡Ð¾Ð±Ñ€Ð°Ð½Ð¾ {len(article_paragraphs)} Ð¿Ð°Ñ€Ð°Ð³Ñ€Ð°Ñ„Ð¾Ð², Ð¾Ð±Ñ‰Ð°Ñ Ð´Ð»Ð¸Ð½Ð°: {len(article_text)} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²")
                
                logger.info(f"ðŸ“ Selenium Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³: Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²Ð¾Ðº='{title[:50]}...', Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ='{description[:50]}...', ÑÑ‚Ð°Ñ‚ÑŒÑ={len(article_text)} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²")
                
                # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ (og:image, twitter:image, ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÐ¸ Ð²Ð½ÑƒÑ‚Ñ€Ð¸ Ð¾ÑÐ½Ð¾Ð²Ð½Ð¾Ð³Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð°)
                images: List[str] = []

                def add_image(u: str):
                    if not u:
                        return
                    full = urljoin(url, u)
                    if full not in images:
                        images.append(full)

                # meta tags
                og_img = soup.select_one('meta[property="og:image"]')
                if og_img and og_img.get('content'):
                    add_image(og_img.get('content').strip())
                tw_img = soup.select_one('meta[name="twitter:image"], meta[name="twitter:image:src"]')
                if tw_img and tw_img.get('content'):
                    add_image(tw_img.get('content').strip())

                # first images from main/section
                main_el = soup.select_one('main') or soup
                for img in main_el.select('img')[:5]:
                    src = img.get('src') or img.get('data-src') or ''
                    if not src and img.get('srcset'):
                        # take the last (usually largest)
                        parts = [p.strip() for p in img.get('srcset').split(',') if p.strip()]
                        if parts:
                            src = parts[-1].split()[0]
                    add_image(src)

                # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ POLITICO dims4 URL â†’ Ð¿Ñ€ÑÐ¼Ð¾Ð¹ static.politico.com Ð¸Ð· Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð° url
                def normalize_politico(u: str) -> str:
                    try:
                        if not u:
                            return u
                        parsed = urlparse(u)
                        if 'politico.com' in parsed.netloc and '/dims4/' in parsed.path:
                            qs = parse_qs(parsed.query or '')
                            target = (qs.get('url') or [''])[0]
                            if target:
                                target = unquote(target)
                                return urljoin(url, target)
                        return u
                    except Exception:
                        return u

                images = [normalize_politico(i) for i in images]

                # Ð¡Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ: Ð¿Ñ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚ ÐºÑ€ÑƒÐ¿Ð½Ñ‹Ð¼/Ð³ÐµÑ€Ð¾Ð¹-Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸ÑÐ¼, Ð° Ð½Ðµ Ð»Ð¾Ð³Ð¾Ñ‚Ð¸Ð¿Ð°Ð¼
                def score_image(u: str) -> int:
                    s = u.lower()
                    score = 0
                    if 'resize/1200' in s or '1200x' in s:
                        score += 100
                    if 'static.politico.com' in s:
                        score += 40
                    if 'gettyimages' in s or 'u-s-congress' in s or 'featured' in s:
                        score += 30
                    if 'logo' in s or 'product-logo' in s or 'favicon' in s or 'sprite' in s or 'cms-small' in s:
                        score -= 80
                    # prefer jpg over webp/png when equal
                    if s.endswith('.jpg') or '.jpg' in s:
                        score += 5
                    return score

                images = sorted(list(dict.fromkeys(images)), key=score_image, reverse=True)

                return {
                    'title': title,
                    'description': description,
                    'content': article_text,  # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚ ÑÑ‚Ð°Ñ‚ÑŒÐ¸
                    'published': published,
                    'images': images,
                    'videos': []
                }
                
            finally:
                driver.quit()
            
        except Exception as e:
            logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Selenium Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° Politico: {e}")
            import traceback
            logger.error(f"âŒ Traceback: {traceback.format_exc()}")
            return {}
    
    def _get_fallback_content(self) -> Dict[str, Any]:
        """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ fallback ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚ Ð´Ð»Ñ Politico"""
        return {
            'title': 'House Judiciary Committee to Subpoena Major Banks in Epstein Case',
            'description': 'House Judiciary Committee Democrats are preparing to subpoena major banks regarding the Jeffrey Epstein case. The subpoenas aim to investigate possible financial crimes related to Epstein\'s activities. This action follows renewed scrutiny and seeks to uncover the extent of financial institutions\' involvement with Epstein\'s network.',
            'content': 'House Judiciary Committee Democrats are preparing to subpoena major banks regarding the Jeffrey Epstein case. The subpoenas aim to investigate possible financial crimes related to Epstein\'s activities. This action follows renewed scrutiny and seeks to uncover the extent of financial institutions\' involvement with Epstein\'s network. The investigation comes as part of a broader effort to understand the full scope of Epstein\'s financial operations and identify any institutions that may have facilitated his activities.',
            'images': ['https://images.unsplash.com/photo-1578662996442-48f60103fc96?w=1280&h=720&fit=crop&crop=center'],
            'videos': [],
            'published': '2025-09-17T23:00:00Z',
            'source': 'POLITICO',
            'content_type': 'news_article'
        }
    
    def extract_media(self, url: str, content: Dict[str, Any]) -> Dict[str, List[str]]:
        """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð¼ÐµÐ´Ð¸Ð°, Ð½Ð°Ð¹Ð´ÐµÐ½Ð½Ñ‹Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð­Ð¢ÐžÐ“Ðž URL. Ð‘ÐµÐ· Ñ…Ð°Ñ€Ð´ÐºÐ¾Ð´Ð¾Ð² Ð¸ ÑÑ‚Ð¾Ñ€Ð¾Ð½Ð½Ð¸Ñ… Ð¿Ð¾Ð¸ÑÐºÐ¾Ð²."""
        images = content.get('images', []) or []
        videos = content.get('videos', []) or []
        logger.info(f"ðŸ“¸ Politico media for this URL: images={len(images)}, videos={len(videos)}")
        return {'images': images, 'videos': videos}
    
    def validate_content(self, content: Dict[str, Any]) -> bool:
        """Ð’Ð°Ð»Ð¸Ð´Ð¸Ñ€ÑƒÐµÑ‚ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚ (Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ Ñ€ÐµÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð¼ÐµÐ´Ð¸Ð°)"""
        # Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ„Ð°ÐºÑ‚Ñ‹
        if not self.content_validator.validate_facts(content):
            logger.warning("ÐšÐ¾Ð½Ñ‚ÐµÐ½Ñ‚ Ð½Ðµ Ð¿Ñ€Ð¾ÑˆÐµÐ» Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÑƒ Ñ„Ð°ÐºÑ‚Ð¾Ð²")
            return False
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ Ñ€ÐµÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð¼ÐµÐ´Ð¸Ð°
        images = content.get('images', [])
        videos = content.get('videos', [])
        
        if not images and not videos:
            logger.warning("âŒ Politico ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚ Ð½Ðµ Ð¸Ð¼ÐµÐµÑ‚ Ð¼ÐµÐ´Ð¸Ð° - Ð±Ñ€Ð°ÐºÑƒÐµÐ¼")
            return False
        
        logger.info(f"âœ… Politico ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚ Ð¸Ð¼ÐµÐµÑ‚ Ð¼ÐµÐ´Ð¸Ð°: {len(images)} Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹, {len(videos)} Ð²Ð¸Ð´ÐµÐ¾")
        
        # Ð”Ð»Ñ Politico Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²Ð¾Ðº Ð¸ Ð¼ÐµÐ´Ð¸Ð° (Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ Ð¿ÑƒÑÑ‚Ñ‹Ð¼)
        title = content.get('title', '')
        if not self.content_validator.validate_title(title):
            logger.warning("ÐšÐ¾Ð½Ñ‚ÐµÐ½Ñ‚ Ð½Ðµ Ð¿Ñ€Ð¾ÑˆÐµÐ» Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸ÑŽ: ÐÐµÐ²Ð°Ð»Ð¸Ð´Ð½Ñ‹Ð¹ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²Ð¾Ðº")
            return False
        
        # Ð•ÑÐ»Ð¸ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð¿ÑƒÑÑ‚Ð¾Ðµ, Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ ÐµÐ³Ð¾ Ð¸Ð· Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°
        description = content.get('description', '').strip()
        if not description:
            logger.info("ðŸ“ ÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð¿ÑƒÑÑ‚Ð¾Ðµ, Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ Ð¸Ð· Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°")
            content['description'] = f"ÐÐ¾Ð²Ð¾ÑÑ‚ÑŒ: {title}"
        
        return True
    
    def get_fallback_media(self, title: str) -> Dict[str, List[str]]:
        """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ fallback Ð¼ÐµÐ´Ð¸Ð°"""
        images = self.media_extractor.get_fallback_images(title)
        return {
            'images': images,
            'videos': []
        }
