#!/usr/bin/env python3
"""
NBC News Engine
–ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π —Å nbcnews.com
"""

import logging
import re
from typing import Dict, List, Any, Optional
from urllib.parse import urlparse, urljoin
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException

from ..base.source_engine import SourceEngine

logger = logging.getLogger(__name__)

class NBCNewsEngine(SourceEngine):
    """–î–≤–∏–∂–æ–∫ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ NBC News"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
    
    def _get_supported_domains(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –¥–æ–º–µ–Ω—ã"""
        return [
            "nbcnews.com",
            "www.nbcnews.com"
        ]
    
    def _get_source_name(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        return "NBC News"
    
    def can_handle(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –º–æ–∂–µ—Ç –ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å URL"""
        return any(domain in url.lower() for domain in self.supported_domains)
    
    def get_engine_info(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–≤–∏–∂–∫–µ"""
        return {
            'name': self.source_name,
            'supported_domains': self.supported_domains,
            'version': '1.0.0'
        }
    
    def parse_url(self, url: str, driver=None) -> Dict[str, Any]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ URL NBC News
        
        Args:
            url: URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
            driver: Selenium WebDriver (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–æ–≤–æ—Å—Ç–∏ –∏–ª–∏ None
        """
        try:
            logger.info(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ NBC News URL: {url}")
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è URL
            if not self._is_valid_url(url):
                logger.warning(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π URL: {url}")
                return None
            
            # –ü–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ Selenium –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            if driver:
                return self._parse_with_selenium(url, driver)
            else:
                logger.warning("‚ö†Ô∏è Selenium WebDriver –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                return {
                    'title': 'NBC News Article - WebDriver Required',
                    'description': 'This article requires Selenium WebDriver for proper parsing',
                    'content': 'This article requires Selenium WebDriver for proper parsing. Please ensure WebDriver is available.',
                    'images': [],
                    'videos': [],
                    'published': '',
                    'source': self.source_name,
                    'content_type': 'article'
                }
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ NBC News URL {url}: {e}")
            return {
                'title': 'NBC News Article',
                'description': '',
                'images': [],
                'videos': [],
                'published': '',
                'source': self.source_name,
                'content_type': 'article'
            }
    
    def _parse_with_selenium(self, url: str, driver) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ Selenium"""
        try:
            logger.info(f"üîç Selenium –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            driver.get(url)
            
            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            try:
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "article"))
                )
            except TimeoutException:
                logger.warning("‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç–∞—Ç—å–∏")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            title = self._extract_title(driver)
            content = self._extract_content(driver)
            author = self._extract_author(driver)
            publish_date = self._extract_publish_date(driver)
            images = self._extract_images(driver)
            videos = self._extract_videos(driver)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            if not self._validate_content({'title': title, 'content': content}):
                logger.warning("‚ùå –ö–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –ø—Ä–æ—à–µ–ª –≤–∞–ª–∏–¥–∞—Ü–∏—é")
                return {
                    'title': 'NBC News Article',
                    'description': '',
                    'images': [],
                    'videos': [],
                    'published': '',
                    'source': self.source_name,
                    'content_type': 'article'
                }
            
            result = {
                'title': title,
                'description': content[:500] if content else '',  # –ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∫ –æ–ø–∏—Å–∞–Ω–∏–µ
                'content': content,
                'author': author,
                'published': publish_date,
                'source': self.source_name,
                'url': url,
                'images': images,
                'videos': videos,
                'content_type': 'article'
            }
            
            logger.info(f"üìù NBC News –ø–∞—Ä—Å–∏–Ω–≥: –∑–∞–≥–æ–ª–æ–≤–æ–∫='{title[:50]}...', –∫–æ–Ω—Ç–µ–Ω—Ç={len(content)} —Å–∏–º–≤–æ–ª–æ–≤, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è={len(images)}, –≤–∏–¥–µ–æ={len(videos)}")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ Selenium –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            return {
                'title': 'NBC News Article',
                'description': '',
                'images': [],
                'videos': [],
                'published': '',
                'source': self.source_name,
                'content_type': 'article'
            }
    
    def _extract_title(self, driver) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
        try:
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
            title_selectors = [
                'h1[data-testid="headline"]',
                'h1.article-hero__headline',
                'h1.article-hero__headline__text',
                'h1.headline',
                'h1[class*="headline"]',
                'h1[class*="title"]',
                'h1',
                '.article-hero h1',
                '[data-testid="article-headline"] h1'
            ]
            
            for selector in title_selectors:
                try:
                    element = driver.find_element(By.CSS_SELECTOR, selector)
                    title = element.text.strip()
                    if title and len(title) > 10:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                        logger.info(f"‚úÖ –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞–π–¥–µ–Ω: {title[:50]}...")
                        return title
                except NoSuchElementException:
                    continue
            
            logger.warning("‚ö†Ô∏è –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return "NBC News Article"
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞: {e}")
            return "NBC News Article"
    
    def _extract_content(self, driver) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        try:
            content_parts = []
            
            # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content_selectors = [
                '.article-body',
                '.article-content',
                '[data-testid="article-body"]',
                '.article-body__content',
                '.article-content__body',
                'article .article-body',
                '.article-body p',
                '.article-content p'
            ]
            
            for selector in content_selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    for element in elements:
                        text = element.text.strip()
                        if text and len(text) > 20:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∞–±–∑–∞—Ü–∞
                            content_parts.append(text)
                except NoSuchElementException:
                    continue
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –≤ —Å—Ç–∞—Ç—å–µ
            if not content_parts:
                try:
                    article = driver.find_element(By.TAG_NAME, "article")
                    paragraphs = article.find_elements(By.TAG_NAME, "p")
                    for p in paragraphs:
                        text = p.text.strip()
                        if text and len(text) > 20:
                            content_parts.append(text)
                except NoSuchElementException:
                    pass
            
            content = ' '.join(content_parts)
            
            if content:
                logger.info(f"‚úÖ –ö–æ–Ω—Ç–µ–Ω—Ç –∏–∑–≤–ª–µ—á–µ–Ω: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
                return content
            else:
                logger.warning("‚ö†Ô∏è –ö–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return ""
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}")
            return ""
    
    def _extract_author(self, driver) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–∞"""
        try:
            author_selectors = [
                '[data-testid="byline"]',
                '.byline',
                '.article-byline',
                '.author',
                '[class*="byline"]',
                '[class*="author"]'
            ]
            
            for selector in author_selectors:
                try:
                    element = driver.find_element(By.CSS_SELECTOR, selector)
                    author = element.text.strip()
                    if author and len(author) > 2:
                        logger.info(f"‚úÖ –ê–≤—Ç–æ—Ä –Ω–∞–π–¥–µ–Ω: {author}")
                        return author
                except NoSuchElementException:
                    continue
            
            return "NBC News"
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∞–≤—Ç–æ—Ä–∞: {e}")
            return "NBC News"
    
    def _extract_publish_date(self, driver) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"""
        try:
            date_selectors = [
                '[data-testid="timestamp"]',
                '.timestamp',
                '.article-timestamp',
                'time[datetime]',
                '[class*="timestamp"]',
                '[class*="date"]'
            ]
            
            for selector in date_selectors:
                try:
                    element = driver.find_element(By.CSS_SELECTOR, selector)
                    date_text = element.text.strip()
                    if date_text:
                        logger.info(f"‚úÖ –î–∞—Ç–∞ –Ω–∞–π–¥–µ–Ω–∞: {date_text}")
                        return date_text
                except NoSuchElementException:
                    continue
            
            return ""
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞—Ç—ã: {e}")
            return ""
    
    def _extract_images(self, driver) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        try:
            images = []
            
            # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            img_selectors = [
                '.article-body img',
                '.article-content img',
                '[data-testid="article-body"] img',
                'article img',
                '.article-hero img',
                '.article-body__content img',
                'figure img',
                '.image-container img',
                '.media img'
            ]
            
            for selector in img_selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    for img in elements:
                        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
                        src = img.get_attribute('src')
                        data_src = img.get_attribute('data-src')
                        data_lazy = img.get_attribute('data-lazy')
                        
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                        for img_url in [src, data_src, data_lazy]:
                            if img_url and self._is_valid_image_url(img_url):
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ URL –ø–æ–ª–Ω—ã–π
                                if img_url.startswith('http'):
                                    if img_url not in images:
                                        images.append(img_url)
                                        logger.debug(f"üì∏ –î–æ–±–∞–≤–ª–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {img_url}")
                                else:
                                    # –ü—Ä–æ–±—É–µ–º —Å–¥–µ–ª–∞—Ç—å URL –ø–æ–ª–Ω—ã–º
                                    full_url = urljoin(driver.current_url, img_url)
                                    if self._is_valid_image_url(full_url):
                                        if full_url not in images:
                                            images.append(full_url)
                                            logger.debug(f"üì∏ –î–æ–±–∞–≤–ª–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (–ø–æ–ª–Ω—ã–π URL): {full_url}")
                except NoSuchElementException:
                    continue
            
            logger.info(f"üì∏ –ù–∞–π–¥–µ–Ω–æ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            return images
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {e}")
            return []
    
    def _extract_videos(self, driver) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–∏–¥–µ–æ"""
        try:
            videos = []
            
            # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –≤–∏–¥–µ–æ
            video_selectors = [
                '.article-body video',
                '.article-content video',
                '[data-testid="article-body"] video',
                'article video',
                'iframe[src*="youtube"]',
                'iframe[src*="vimeo"]',
                'iframe[src*="nbcnews"]',
                'video source',
                '.video-container video',
                '.media video'
            ]
            
            for selector in video_selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    for video in elements:
                        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
                        src = video.get_attribute('src')
                        data_src = video.get_attribute('data-src')
                        
                        # –î–ª—è source —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                        if video.tag_name == 'source':
                            src = video.get_attribute('src')
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                        for video_url in [src, data_src]:
                            if video_url and self._is_valid_video_url(video_url):
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ URL –ø–æ–ª–Ω—ã–π
                                if video_url.startswith('http'):
                                    if video_url not in videos:
                                        videos.append(video_url)
                                        logger.debug(f"üé¨ –î–æ–±–∞–≤–ª–µ–Ω–æ –≤–∏–¥–µ–æ: {video_url}")
                                else:
                                    # –ü—Ä–æ–±—É–µ–º —Å–¥–µ–ª–∞—Ç—å URL –ø–æ–ª–Ω—ã–º
                                    full_url = urljoin(driver.current_url, video_url)
                                    if self._is_valid_video_url(full_url):
                                        if full_url not in videos:
                                            videos.append(full_url)
                                            logger.debug(f"üé¨ –î–æ–±–∞–≤–ª–µ–Ω–æ –≤–∏–¥–µ–æ (–ø–æ–ª–Ω—ã–π URL): {full_url}")
                except NoSuchElementException:
                    continue
            
            logger.info(f"üé¨ –ù–∞–π–¥–µ–Ω–æ {len(videos)} –≤–∏–¥–µ–æ")
            return videos
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≤–∏–¥–µ–æ: {e}")
            return []
    
    def _is_valid_image_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        if not url:
            return False
        
        # –ò—Å–∫–ª—é—á–∞–µ–º blob –∏ data URLs
        if url.startswith('blob:') or url.startswith('data:'):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é
        image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp']
        if any(url.lower().endswith(ext) for ext in image_extensions):
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–º–µ–Ω—ã NBC News
        nbc_domains = ['nbcnews.com', 'media.nbcnews.com', 'media1.nbcnews.com', 'media-cldnry.s-nbcnews.com']
        parsed_url = urlparse(url)
        if any(domain in parsed_url.netloc for domain in nbc_domains):
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ URL —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if 'image' in url.lower() or 'photo' in url.lower():
            return True
        
        return False
    
    def _is_valid_video_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ URL –≤–∏–¥–µ–æ"""
        if not url:
            return False
        
        # –ò—Å–∫–ª—é—á–∞–µ–º blob –∏ data URLs
        if url.startswith('blob:') or url.startswith('data:'):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∏–¥–µ–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
        video_extensions = ['.mp4', '.webm', '.ogg', '.mov', '.m3u8', '.ts']
        if any(url.lower().endswith(ext) for ext in video_extensions):
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∏–¥–µ–æ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã
        video_platforms = ['youtube.com', 'youtu.be', 'vimeo.com', 'nbcnews.com', 'media.nbcnews.com']
        parsed_url = urlparse(url)
        if any(platform in parsed_url.netloc for platform in video_platforms):
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ URL —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–∏–¥–µ–æ
        if 'video' in url.lower() or 'stream' in url.lower():
            return True
        
        return False
    
    def _validate_content(self, content: Dict[str, Any]) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        title = content.get('title', '')
        text_content = content.get('content', '')
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        if not title or len(title) < 10:
            logger.warning("‚ùå –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
        if not text_content or len(text_content) < 50:
            logger.warning("‚ùå –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç")
            return False
        
        logger.info(f"‚úÖ NBC News –∫–æ–Ω—Ç–µ–Ω—Ç –≤–∞–ª–∏–¥–µ–Ω: title={bool(title)}, content={len(text_content)} —Å–∏–º–≤–æ–ª–æ–≤")
        return True
    
    def extract_media(self, url: str, content: Dict[str, Any]) -> Dict[str, List[str]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ–¥–∏–∞ —Ñ–∞–π–ª—ã –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        return {
            'images': content.get('images', []),
            'videos': content.get('videos', [])
        }
    
    def validate_content(self, content: Dict[str, Any]) -> bool:
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        return self._validate_content(content)
    
    def _is_valid_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ URL"""
        if not url:
            return False
        
        from urllib.parse import urlparse
        parsed = urlparse(url)
        
        if not parsed.netloc:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–æ–º–µ–Ω –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
        return any(domain in parsed.netloc for domain in self.supported_domains)
