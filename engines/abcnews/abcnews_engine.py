#!/usr/bin/env python3
"""
ABC News Engine
–ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π —Å abcnews.go.com
"""

import logging
import re
from typing import Dict, List, Any
from urllib.parse import urlparse, urljoin, parse_qs
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException

from ..base.source_engine import SourceEngine

logger = logging.getLogger(__name__)

class ABCNewsEngine(SourceEngine):
    """–î–≤–∏–∂–æ–∫ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ ABC News"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
    
    def _get_supported_domains(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –¥–æ–º–µ–Ω—ã"""
        return [
            "abcnews.go.com",
            "www.abcnews.go.com"
        ]
    
    def _get_source_name(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        return "ABC News"
    
    def can_handle(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –º–æ–∂–µ—Ç –ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å URL"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–º–µ–Ω
        if not any(domain in url.lower() for domain in self.supported_domains):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ entryId - –µ—Å–ª–∏ –µ—Å—Ç—å, —ç—Ç–æ —Å—Å—ã–ª–∫–∞ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –∑–∞–ø–∏—Å—å
        parsed = urlparse(url)
        query_params = parse_qs(parsed.query)
        has_entry_id = 'entryId' in query_params or 'entryid' in query_params
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å entryId, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–∂–µ live-updates
        if has_entry_id:
            logger.info(f"‚úÖ ABC News: URL —Å–æ–¥–µ—Ä–∂–∏—Ç entryId, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—É—é –∑–∞–ø–∏—Å—å")
            return True
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–∏–ø—ã URL –±–µ–∑ entryId, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        excluded_patterns = [
            '/live-updates/',  # Live updates –±–µ–∑ entryId - –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –æ–±–Ω–æ–≤–ª—è–µ–º—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        ]
        
        for pattern in excluded_patterns:
            if pattern in url.lower():
                logger.info(f"‚è≠Ô∏è ABC News: URL —Å–æ–¥–µ—Ä–∂–∏—Ç –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω '{pattern}' –±–µ–∑ entryId, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                return False
        
        return True
    
    def get_engine_info(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–≤–∏–∂–∫–µ"""
        return {
            'name': self.source_name,
            'supported_domains': self.supported_domains,
            'version': '1.0.0'
        }
    
    def parse_url(self, url: str, driver=None) -> Dict[str, Any]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ URL ABC News
        
        Args:
            url: URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
            driver: Selenium WebDriver (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–æ–≤–æ—Å—Ç–∏ –∏–ª–∏ None
        """
        try:
            logger.info(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ ABC News URL: {url}")
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è URL
            if not self._is_valid_url(url):
                logger.warning(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π URL: {url}")
                return None
            
            # –ü–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ Selenium –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            if driver:
                return self._parse_with_selenium(url, driver)
            else:
                # –°–æ–∑–¥–∞–µ–º –¥—Ä–∞–π–≤–µ—Ä –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
                return self._parse_with_selenium(url, None)
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ ABC News URL {url}: {e}")
            import traceback
            logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
            return None
    
    def _parse_with_selenium(self, url: str, driver=None) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ Selenium"""
        driver_created = False
        try:
            # –ï—Å–ª–∏ –¥—Ä–∞–π–≤–µ—Ä –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω, —Å–æ–∑–¥–∞–µ–º –µ–≥–æ
            if driver is None:
                from selenium import webdriver
                from selenium.webdriver.chrome.options import Options
                
                chrome_options = Options()
                chrome_options.add_argument('--headless')
                chrome_options.add_argument('--no-sandbox')
                chrome_options.add_argument('--disable-dev-shm-usage')
                chrome_options.add_argument('--disable-gpu')
                chrome_options.add_argument('--window-size=1920,1080')
                chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
                
                driver = webdriver.Chrome(options=chrome_options)
                driver_created = True
            
            logger.info(f"üîç Selenium –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —ç—Ç–æ live-updates?
            is_live_update = '/live-updates/' in url.lower()
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            driver.get(url)
            
            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            import time
            time.sleep(5 if is_live_update else 3)  # –ë–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è live-updates
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            title = self._extract_title(driver)
            content = self._extract_content(driver)
            description = self._extract_description(driver, content)
            author = self._extract_author(driver)
            publish_date = self._extract_publish_date(driver)
            images = self._extract_images(driver, url)
            videos = self._extract_videos(driver)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            if not self._validate_parsed_content({'title': title, 'content': content}):
                logger.warning("‚ùå –ö–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –ø—Ä–æ—à–µ–ª –≤–∞–ª–∏–¥–∞—Ü–∏—é")
                return None
            
            result = {
                'title': title,
                'description': description,
                'content': content,
                'author': author,
                'published': publish_date,
                'source': self.source_name,
                'url': url,
                'images': images,
                'videos': videos,
                'content_type': 'article'
            }
            
            logger.info(f"üìù ABC News –ø–∞—Ä—Å–∏–Ω–≥: –∑–∞–≥–æ–ª–æ–≤–æ–∫='{title[:50]}...', –∫–æ–Ω—Ç–µ–Ω—Ç={len(content)} —Å–∏–º–≤–æ–ª–æ–≤, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è={len(images)}, –≤–∏–¥–µ–æ={len(videos)}")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ Selenium –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            import traceback
            logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
            return None
        finally:
            # –ó–∞–∫—Ä—ã–≤–∞–µ–º –¥—Ä–∞–π–≤–µ—Ä, –µ—Å–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –µ–≥–æ —Å–∞–º–∏
            if driver_created and driver:
                try:
                    driver.quit()
                except Exception:
                    pass
    
    def _extract_title(self, driver) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
        try:
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
            title_selectors = [
                'h1[data-testid="headline"]',
                'h1.Article__Headline',
                'h1.headline',
                'h1[class*="headline"]',
                'h1[class*="title"]',
                'h1[class*="Headline"]',
                # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è live-updates
                '.LiveBlogPost__Headline h1',
                '.LiveBlogPost__Headline h2',
                '.LiveBlogPost h1',
                '.LiveBlogPost h2',
                '[class*="LiveBlog"] h1',
                '[class*="LiveBlog"] h2',
                'h1',
                'h2',
                '.article-header h1',
                '[data-testid="article-headline"] h1',
                'header h1',
                'article h1'
            ]
            
            for selector in title_selectors:
                try:
                    element = driver.find_element(By.CSS_SELECTOR, selector)
                    title = element.text.strip()
                    if title and len(title) > 10:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                        logger.info(f"‚úÖ –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞–π–¥–µ–Ω —á–µ—Ä–µ–∑ '{selector}': {title[:50]}...")
                        return title
                except NoSuchElementException:
                    continue
            
            # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∏–∑ meta —Ç–µ–≥–æ–≤
            try:
                meta_title = driver.find_element(By.CSS_SELECTOR, 'meta[property="og:title"]')
                title = meta_title.get_attribute('content').strip()
                if title and len(title) > 10:
                    logger.info(f"‚úÖ –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞–π–¥–µ–Ω –∏–∑ og:title: {title[:50]}...")
                    return title
            except NoSuchElementException:
                pass
            
            logger.warning("‚ö†Ô∏è –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return "ABC News Article"
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞: {e}")
            return "ABC News Article"
    
    def _extract_description(self, driver, content: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è"""
        try:
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏—è/—Ä–µ–∑—é–º–µ
            desc_selectors = [
                'meta[property="og:description"]',
                'meta[name="description"]',
                'p.Article__Description',
                'p.summary',
                '.article-description',
                '[data-testid="article-description"]'
            ]
            
            for selector in desc_selectors:
                try:
                    if selector.startswith('meta'):
                        element = driver.find_element(By.CSS_SELECTOR, selector)
                        description = element.get_attribute('content').strip()
                    else:
                        element = driver.find_element(By.CSS_SELECTOR, selector)
                        description = element.text.strip()
                    
                    if description and len(description) > 20:
                        logger.info(f"‚úÖ –û–ø–∏—Å–∞–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–æ: {description[:50]}...")
                        return description
                except NoSuchElementException:
                    continue
            
            # –ï—Å–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            if content and len(content) > 50:
                description = content[:500] + '...' if len(content) > 500 else content
                logger.info(f"‚úÖ –û–ø–∏—Å–∞–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {description[:50]}...")
                return description
            
            logger.warning("‚ö†Ô∏è –û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            return ""
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ–ø–∏—Å–∞–Ω–∏—è: {e}")
            return ""
    
    def _extract_content(self, driver) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        try:
            content_parts = []
            
            # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content_selectors = [
                '.Article__Content p',
                '.article-body p',
                '.article-content p',
                '[data-testid="article-body"] p',
                '.Article__Body p',
                'article .Article__Content p',
                # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è live-updates
                '.LiveBlogPost__Content p',
                '.LiveBlogPost__Body p',
                '[class*="LiveBlogPost"] p',
                '[class*="LiveBlog"] p',
                'article p',
                '.story-body p'
            ]
            
            for selector in content_selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(elements)} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä '{selector}'")
                        for element in elements:
                            text = element.text.strip()
                            if text and len(text) > 20:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∞–±–∑–∞—Ü–∞
                                # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
                                if not self._is_service_text(text):
                                    content_parts.append(text)
                        break  # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç, –≤—ã—Ö–æ–¥–∏–º
                except NoSuchElementException:
                    continue
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –≤ —Å—Ç–∞—Ç—å–µ
            if not content_parts:
                try:
                    article = driver.find_element(By.TAG_NAME, "article")
                    paragraphs = article.find_elements(By.TAG_NAME, "p")
                    logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(paragraphs)} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –≤ article")
                    for p in paragraphs:
                        text = p.text.strip()
                        if text and len(text) > 20:
                            if not self._is_service_text(text):
                                content_parts.append(text)
                except NoSuchElementException:
                    pass
            
            content = ' '.join(content_parts)
            
            if content:
                logger.info(f"‚úÖ –ö–æ–Ω—Ç–µ–Ω—Ç –∏–∑–≤–ª–µ—á–µ–Ω: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ {len(content_parts)} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤")
                return content
            else:
                logger.warning("‚ö†Ô∏è –ö–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return ""
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}")
            return ""
    
    def _is_service_text(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç —Å–ª—É–∂–µ–±–Ω—ã–º"""
        service_keywords = [
            'subscribe',
            'newsletter',
            'advertisement',
            'click here',
            'read more',
            'related:',
            'follow us',
            'sign up',
            'trending',
            'sponsored',
            'editor\'s note'
        ]
        
        text_lower = text.lower()
        return any(keyword in text_lower for keyword in service_keywords)
    
    def _extract_author(self, driver) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–∞"""
        try:
            author_selectors = [
                '[data-testid="byline"]',
                '.byline',
                '.article-byline',
                '.author',
                '[class*="byline"]',
                '[class*="author"]',
                '.Article__Author',
                'meta[name="author"]'
            ]
            
            for selector in author_selectors:
                try:
                    if selector.startswith('meta'):
                        element = driver.find_element(By.CSS_SELECTOR, selector)
                        author = element.get_attribute('content').strip()
                    else:
                        element = driver.find_element(By.CSS_SELECTOR, selector)
                        author = element.text.strip()
                    
                    if author and len(author) > 2:
                        # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–µ–≥–æ ("By ", "ABC News" –∏ —Ç.–¥.)
                        author = re.sub(r'^By\s+', '', author, flags=re.IGNORECASE)
                        logger.info(f"‚úÖ –ê–≤—Ç–æ—Ä –Ω–∞–π–¥–µ–Ω: {author}")
                        return author
                except NoSuchElementException:
                    continue
            
            return "ABC News"
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∞–≤—Ç–æ—Ä–∞: {e}")
            return "ABC News"
    
    def _extract_publish_date(self, driver) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"""
        try:
            date_selectors = [
                '[data-testid="timestamp"]',
                '.timestamp',
                '.article-timestamp',
                'time[datetime]',
                '[class*="timestamp"]',
                '[class*="date"]',
                '.Article__Date',
                'meta[property="article:published_time"]'
            ]
            
            for selector in date_selectors:
                try:
                    element = driver.find_element(By.CSS_SELECTOR, selector)
                    
                    if selector.startswith('meta'):
                        date_text = element.get_attribute('content').strip()
                    elif selector == 'time[datetime]':
                        date_text = element.get_attribute('datetime').strip()
                    else:
                        date_text = element.text.strip()
                    
                    if date_text:
                        logger.info(f"‚úÖ –î–∞—Ç–∞ –Ω–∞–π–¥–µ–Ω–∞: {date_text}")
                        return date_text
                except NoSuchElementException:
                    continue
            
            return ""
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞—Ç—ã: {e}")
            return ""
    
    def _extract_images(self, driver, url: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        try:
            images = []
            
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –≥–ª–∞–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ meta —Ç–µ–≥–æ–≤
            meta_img_selectors = [
                'meta[property="og:image"]',
                'meta[name="twitter:image"]',
                'meta[property="twitter:image"]'
            ]
            
            for selector in meta_img_selectors:
                try:
                    element = driver.find_element(By.CSS_SELECTOR, selector)
                    img_url = element.get_attribute('content')
                    if img_url and self._is_valid_image_url(img_url):
                        full_url = urljoin(url, img_url)
                        if full_url not in images:
                            images.append(full_url)
                            logger.debug(f"üì∏ –î–æ–±–∞–≤–ª–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ meta: {full_url}")
                except NoSuchElementException:
                    continue
            
            # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ
            img_selectors = [
                '.Article__Content img',
                '.article-body img',
                '.article-content img',
                '[data-testid="article-body"] img',
                'article img',
                '.Article__Hero img',
                'figure img',
                '.image-container img',
                '.media img',
                'picture img'
            ]
            
            for selector in img_selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    for img in elements:
                        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
                        src = img.get_attribute('src')
                        data_src = img.get_attribute('data-src')
                        data_lazy = img.get_attribute('data-lazy')
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                        for img_url in [src, data_src, data_lazy]:
                            if img_url and self._is_valid_image_url(img_url):
                                # –î–µ–ª–∞–µ–º URL –ø–æ–ª–Ω—ã–º
                                full_url = urljoin(url, img_url)
                                if full_url not in images:
                                    images.append(full_url)
                                    logger.debug(f"üì∏ –î–æ–±–∞–≤–ª–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {full_url}")
                except NoSuchElementException:
                    continue
            
            logger.info(f"üì∏ –ù–∞–π–¥–µ–Ω–æ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            return images
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {e}")
            return []
    
    def _extract_videos(self, driver) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–∏–¥–µ–æ"""
        try:
            videos = []
            
            # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –≤–∏–¥–µ–æ
            video_selectors = [
                '.Article__Content video',
                '.article-body video',
                '.article-content video',
                '[data-testid="article-body"] video',
                'article video',
                'iframe[src*="youtube"]',
                'iframe[src*="vimeo"]',
                'iframe[src*="abcnews"]',
                'video source',
                '.video-container video',
                '.media video',
                '.Article__Video video'
            ]
            
            for selector in video_selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    for video in elements:
                        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
                        src = video.get_attribute('src')
                        data_src = video.get_attribute('data-src')
                        
                        # –î–ª—è source —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                        if video.tag_name == 'source':
                            src = video.get_attribute('src')
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                        for video_url in [src, data_src]:
                            if video_url and self._is_valid_video_url(video_url):
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ URL –ø–æ–ª–Ω—ã–π
                                if video_url.startswith('http'):
                                    if video_url not in videos:
                                        videos.append(video_url)
                                        logger.debug(f"üé¨ –î–æ–±–∞–≤–ª–µ–Ω–æ –≤–∏–¥–µ–æ: {video_url}")
                                else:
                                    # –ü—Ä–æ–±—É–µ–º —Å–¥–µ–ª–∞—Ç—å URL –ø–æ–ª–Ω—ã–º
                                    full_url = urljoin(driver.current_url, video_url)
                                    if self._is_valid_video_url(full_url):
                                        if full_url not in videos:
                                            videos.append(full_url)
                                            logger.debug(f"üé¨ –î–æ–±–∞–≤–ª–µ–Ω–æ –≤–∏–¥–µ–æ (–ø–æ–ª–Ω—ã–π URL): {full_url}")
                except NoSuchElementException:
                    continue
            
            logger.info(f"üé¨ –ù–∞–π–¥–µ–Ω–æ {len(videos)} –≤–∏–¥–µ–æ")
            return videos
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≤–∏–¥–µ–æ: {e}")
            return []
    
    def _is_valid_image_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        if not url:
            return False
        
        # –ò—Å–∫–ª—é—á–∞–µ–º blob –∏ data URLs
        if url.startswith('blob:') or url.startswith('data:'):
            return False
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if any(x in url.lower() for x in ['1x1', 'spacer', 'pixel', 'blank']):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é
        image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp']
        if any(url.lower().endswith(ext) for ext in image_extensions):
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–º–µ–Ω—ã ABC News
        abc_domains = ['abcnews.go.com', 's.abcnews.com', 'img.abcnews.com', 'cdn.abcnews.com']
        parsed_url = urlparse(url)
        if any(domain in parsed_url.netloc for domain in abc_domains):
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ URL —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if 'image' in url.lower() or 'photo' in url.lower() or 'img' in url.lower():
            return True
        
        return False
    
    def _is_valid_video_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ URL –≤–∏–¥–µ–æ"""
        if not url:
            return False
        
        # –ò—Å–∫–ª—é—á–∞–µ–º blob –∏ data URLs
        if url.startswith('blob:') or url.startswith('data:'):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∏–¥–µ–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
        video_extensions = ['.mp4', '.webm', '.ogg', '.mov', '.m3u8', '.ts']
        if any(url.lower().endswith(ext) for ext in video_extensions):
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∏–¥–µ–æ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã
        video_platforms = ['youtube.com', 'youtu.be', 'vimeo.com', 'abcnews.go.com']
        parsed_url = urlparse(url)
        if any(platform in parsed_url.netloc for platform in video_platforms):
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ URL —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–∏–¥–µ–æ
        if 'video' in url.lower() or 'stream' in url.lower():
            return True
        
        return False
    
    def _validate_parsed_content(self, content: Dict[str, Any]) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä—Å–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        title = content.get('title', '')
        text_content = content.get('content', '')
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        if not title or len(title) < 10 or title == "ABC News Article":
            logger.warning("‚ùå –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
        if not text_content or len(text_content) < 50:
            logger.warning("‚ùå –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç")
            return False
        
        logger.info(f"‚úÖ ABC News –∫–æ–Ω—Ç–µ–Ω—Ç –≤–∞–ª–∏–¥–µ–Ω: title={bool(title)}, content={len(text_content)} —Å–∏–º–≤–æ–ª–æ–≤")
        return True
    
    def extract_media(self, url: str, content: Dict[str, Any]) -> Dict[str, List[str]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ–¥–∏–∞ —Ñ–∞–π–ª—ã –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        return {
            'images': content.get('images', []),
            'videos': content.get('videos', [])
        }
    
    def validate_content(self, content: Dict[str, Any]) -> bool:
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        # –ë–∞–∑–æ–≤–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
        if not self._validate_parsed_content(content):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–µ–¥–∏–∞
        images = content.get('images', [])
        videos = content.get('videos', [])
        
        if not images and not videos:
            logger.warning("‚ö†Ô∏è ABC News: –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –∏–º–µ–µ—Ç –º–µ–¥–∏–∞, –Ω–æ —ç—Ç–æ –¥–æ–ø—É—Å—Ç–∏–º–æ")
            # –î–ª—è ABC News –º–µ–¥–∏–∞ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã, –Ω–µ –±—Ä–∞–∫—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
        
        return True
    
    def _is_valid_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ URL"""
        if not url:
            return False
        
        parsed = urlparse(url)
        
        if not parsed.netloc:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–æ–º–µ–Ω –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
        if not any(domain in parsed.netloc for domain in self.supported_domains):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ entryId - –µ—Å–ª–∏ –µ—Å—Ç—å, —ç—Ç–æ –≤–∞–ª–∏–¥–Ω—ã–π URL –¥–∞–∂–µ –¥–ª—è live-updates
        query_params = parse_qs(parsed.query)
        has_entry_id = 'entryId' in query_params or 'entryid' in query_params
        
        if has_entry_id:
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ URL –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ—Ç entryId)
        excluded_patterns = [
            '/live-updates/',
        ]
        
        for pattern in excluded_patterns:
            if pattern in url.lower():
                return False
        
        return True

