"""
Wall Street Journal news source engine
"""

from typing import Dict, Any, List
import logging
import time
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from ..base import SourceEngine, MediaExtractor, ContentValidator

logger = logging.getLogger(__name__)


class WSJMediaExtractor(MediaExtractor):
    """–ò–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å –º–µ–¥–∏–∞ –¥–ª—è Wall Street Journal"""
    
    def extract_images(self, url: str, content: Dict[str, Any]) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ WSJ"""
        images = []
        
        if 'images' in content:
            for img_url in content['images']:
                if self.validate_image_url(img_url):
                    images.append(img_url)
        
        return images
    
    def extract_videos(self, url: str, content: Dict[str, Any]) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤–∏–¥–µ–æ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ WSJ"""
        videos = []
        
        if 'videos' in content:
            for vid_url in content['videos']:
                if self.validate_video_url(vid_url):
                    videos.append(vid_url)
        
        return videos


class WSJContentValidator(ContentValidator):
    """–í–∞–ª–∏–¥–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è Wall Street Journal"""
    
    def validate_quality(self, content: Dict[str, Any]) -> bool:
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ WSJ"""
        errors = self.get_validation_errors(content)
        
        if errors:
            logger.warning(f"–ö–æ–Ω—Ç–µ–Ω—Ç WSJ –Ω–µ –ø—Ä–æ—à–µ–ª –≤–∞–ª–∏–¥–∞—Ü–∏—é: {', '.join(errors)}")
            return False
        
        return True


class WSJEngine(SourceEngine):
    """
    –î–≤–∏–∂–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π Wall Street Journal
    """
    
    def __init__(self, config: Dict[str, Any]):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–≤–∏–∂–∫–∞ WSJ"""
        super().__init__(config)
        self.media_extractor = WSJMediaExtractor(config)
        self.content_validator = WSJContentValidator(config)
    
    def _get_source_name(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        return "Wall Street Journal"
    
    def _get_supported_domains(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –¥–æ–º–µ–Ω—ã"""
        return ['wsj.com', 'www.wsj.com']
    
    def can_handle(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –º–æ–∂–µ—Ç –ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å URL"""
        return any(domain in url.lower() for domain in self.supported_domains)
    
    def parse_url(self, url: str, driver=None) -> Dict[str, Any]:
        """
        –ü–∞—Ä—Å–∏—Ç URL –Ω–æ–≤–æ—Å—Ç–∏ Wall Street Journal
        
        Args:
            url: URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
            driver: WebDriver –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–æ–≤–æ—Å—Ç–∏
        """
        logger.info(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ WSJ URL: {url[:100]}...")
        
        try:
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —á–µ—Ä–µ–∑ archive –¥–ª—è –æ–±—Ö–æ–¥–∞ paywall
            logger.info("üì¶ –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—å—é —á–µ—Ä–µ–∑ Archive –¥–ª—è –æ–±—Ö–æ–¥–∞ paywall...")
            archive_result = self._try_archive_is(url, driver)
            
            if archive_result and len(archive_result.get('content', '')) > 2000:
                logger.info(f"‚úÖ Archive —É—Å–ø–µ—à–Ω–æ: {len(archive_result.get('content', ''))} —Å–∏–º–≤–æ–ª–æ–≤")
                return archive_result
            else:
                logger.warning("‚ö†Ô∏è Archive –Ω–µ –¥–∞–ª –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –ø—Ä–æ–±—É–µ–º –ø—Ä—è–º–æ–π –ø–∞—Ä—Å–∏–Ω–≥...")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Selenium –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
            result = self._parse_with_selenium(url, driver)
            
            if result and result.get('title'):
                logger.info(f"‚úÖ Selenium –ø–∞—Ä—Å–∏–Ω–≥ —É—Å–ø–µ—à–µ–Ω: {result['title'][:50]}...")
                logger.info(f"üìÑ Selenium –∫–æ–Ω—Ç–µ–Ω—Ç: {len(result.get('content', ''))} —Å–∏–º–≤–æ–ª–æ–≤")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–µ–¥–∏–∞
                images_count = len(result.get('images', []))
                videos_count = len(result.get('videos', []))
                logger.info(f"üì∏ WSJ media for this URL: images={images_count}, videos={videos_count}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è (WSJ —á–∞—Å—Ç–æ –∑–∞ paywall, –ø–æ—ç—Ç–æ–º—É —Å–Ω–∏–∂–∞–µ–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è)
                has_media = images_count > 0 or videos_count > 0
                content_len = len(result.get('content', ''))
                has_content = content_len > 50  # –°–Ω–∏–∂–µ–Ω–æ –∏–∑-–∑–∞ paywall
                
                # –ï—Å–ª–∏ –µ—Å—Ç—å og:image –∏ —Ö–æ—Ç—å –∫–∞–∫–æ–π-—Ç–æ –∫–æ–Ω—Ç–µ–Ω—Ç - –ø—Ä–∏–Ω–∏–º–∞–µ–º
                if has_media and has_content:
                    logger.info(f"‚úÖ WSJ –∫–æ–Ω—Ç–µ–Ω—Ç –∏–º–µ–µ—Ç –º–µ–¥–∏–∞: {images_count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {videos_count} –≤–∏–¥–µ–æ")
                    return result
                elif has_media and result.get('description'):
                    # –ï—Å–ª–∏ –µ—Å—Ç—å –º–µ–¥–∏–∞ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ, –Ω–æ –º–∞–ª–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
                    logger.warning(f"‚ö†Ô∏è WSJ paywall: –º–∞–ª–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ ({content_len} —Å–∏–º–≤–æ–ª–æ–≤), –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ")
                    result['content'] = result.get('description', '') + ' ' + result.get('content', '')
                    logger.info(f"‚úÖ WSJ: –∏—Å–ø–æ–ª—å–∑—É–µ–º description + content ({len(result['content'])} —Å–∏–º–≤–æ–ª–æ–≤)")
                    return result
                else:
                    logger.warning(f"‚ö†Ô∏è WSJ: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–µ–¥–∏–∞ –∏–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (images={images_count}, content_len={content_len})")
                    return {}
            else:
                logger.warning("‚ö†Ô∏è Selenium –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞")
                return {}
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ WSJ URL: {e}", exc_info=True)
            return {}
    
    def _try_archive_is(self, url: str, driver=None) -> Dict[str, Any]:
        """
        –ü—Ä–æ–±—É–µ—Ç –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —á–µ—Ä–µ–∑ Archive –¥–ª—è –æ–±—Ö–æ–¥–∞ paywall
        """
        from selenium import webdriver
        from selenium.webdriver.common.by import By
        from selenium.webdriver.chrome.options import Options
        import time
        import requests
        
        driver_created = False
        
        try:
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –¥–æ–º–µ–Ω—ã archive (archive.is —á–∞—Å—Ç–æ –±–ª–æ–∫–∏—Ä—É–µ—Ç—Å—è)
            archive_domains = ['archive.ph', 'archive.today', 'archive.is']
            archive_url = None
            
            for domain in archive_domains:
                archive_api_url = f"https://{domain}/newest/{url}"
                try:
                    logger.info(f"üì¶ –ü—Ä–æ–±—É–µ–º {domain}...")
                    response = requests.get(archive_api_url, timeout=10, allow_redirects=True)
                    if response.status_code == 200:
                        archive_url = response.url
                        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ –∫–æ–ø–∏—è –Ω–∞ {domain}: {archive_url[:80]}...")
                        break
                except Exception as e:
                    logger.debug(f"‚ö†Ô∏è {domain} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                    continue
            
            if not archive_url:
                logger.warning("‚ö†Ô∏è –ù–∏ –æ–¥–∏–Ω archive-–¥–æ–º–µ–Ω –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω")
                return {}
            
            # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏–∑ Archive
            if not driver:
                chrome_options = Options()
                chrome_options.add_argument('--headless')
                chrome_options.add_argument('--disable-gpu')
                chrome_options.add_argument('--no-sandbox')
                chrome_options.add_argument('--disable-dev-shm-usage')
                driver = webdriver.Chrome(options=chrome_options)
                driver_created = True
            
            driver.get(archive_url)
            time.sleep(3)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
            from bs4 import BeautifulSoup
            html = driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title = ""
            for selector in ['h1', 'meta[property="og:title"]']:
                try:
                    if 'meta' in selector:
                        elem = soup.find('meta', property='og:title')
                        if elem:
                            title = elem.get('content', '')
                    else:
                        elem = soup.find('h1')
                        if elem:
                            title = elem.get_text().strip()
                    if title:
                        break
                except:
                    pass
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
            description = ""
            try:
                meta_desc = soup.find('meta', property='og:description')
                if meta_desc:
                    description = meta_desc.get('content', '')
            except:
                pass
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
            paragraphs = []
            
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º JSON-LD (—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)
            try:
                json_ld_scripts = soup.find_all('script', type='application/ld+json')
                for script in json_ld_scripts:
                    try:
                        import json
                        data = json.loads(script.string)
                        # –ò—â–µ–º articleBody
                        if isinstance(data, dict) and 'articleBody' in data:
                            article_body = data['articleBody']
                            if article_body and len(article_body) > 500:
                                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω articleBody –≤ JSON-LD: {len(article_body)} —Å–∏–º–≤–æ–ª–æ–≤")
                                paragraphs = [article_body]
                                break
                    except:
                        pass
            except:
                pass
            
            # –ï—Å–ª–∏ JSON-LD –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, –ø–∞—Ä—Å–∏–º HTML
            if not paragraphs:
                article = soup.find('article')
                if article:
                    ps = article.find_all('p')
                else:
                    ps = soup.find_all('p')
                
                for p in ps:
                    text = p.get_text().strip()
                    if text and len(text) > 30:
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
                        if any(skip in text for skip in ['Please use the sharing tools', 'Copyright Policy', 'gift article service']):
                            continue
                        if text not in paragraphs:
                            paragraphs.append(text)
            
            content = ' '.join(paragraphs)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            images = []
            try:
                og_image = soup.find('meta', property='og:image')
                if og_image:
                    img_url = og_image.get('content', '')
                    if img_url:
                        images.append(img_url)
            except:
                pass
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É
            from datetime import datetime
            published = datetime.now().isoformat()
            
            result = {
                'title': title,
                'description': description,
                'content': content,
                'published': published,
                'images': images,
                'videos': [],
                'source': 'Wall Street Journal'
            }
            
            logger.info(f"üì¶ Archive –ø–∞—Ä—Å–∏–Ω–≥: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤, {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Archive: {e}")
            return {}
        finally:
            if driver_created and driver:
                try:
                    driver.quit()
                except:
                    pass
    
    def _parse_with_selenium(self, url: str, driver=None) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É WSJ —Å –ø–æ–º–æ—â—å—é Selenium"""
        driver_created = False
        
        try:
            if not driver:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º undetected-chromedriver –¥–ª—è –æ–±—Ö–æ–¥–∞ Cloudflare
                try:
                    import undetected_chromedriver as uc
                    logger.info("üîì –ò—Å–ø–æ–ª—å–∑—É–µ–º undetected-chromedriver –¥–ª—è –æ–±—Ö–æ–¥–∞ Cloudflare...")
                    
                    options = uc.ChromeOptions()
                    options.headless = True
                    options.add_argument('--disable-gpu')
                    options.add_argument('--no-sandbox')
                    options.add_argument('--disable-dev-shm-usage')
                    
                    driver = uc.Chrome(options=options, version_main=None)
                    driver_created = True
                    logger.info("‚úÖ undetected-chromedriver –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å undetected-chromedriver: {e}")
                    logger.info("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π Selenium (–º–æ–∂–µ—Ç –Ω–µ —Å—Ä–∞–±–æ—Ç–∞—Ç—å –∏–∑-–∑–∞ Cloudflare)")
                    from selenium import webdriver
                    from selenium.webdriver.chrome.options import Options
                    
                    chrome_options = Options()
                    chrome_options.add_argument('--headless')
                    chrome_options.add_argument('--disable-gpu')
                    chrome_options.add_argument('--no-sandbox')
                    chrome_options.add_argument('--disable-dev-shm-usage')
                    chrome_options.add_argument('--user-agent=Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)')
                    driver = webdriver.Chrome(options=chrome_options)
                    driver_created = True
            
            logger.info(f"üîç Selenium –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞...")
            
            # –ü—Ä–æ–±—É–µ–º –¥–æ–±–∞–≤–∏—Ç—å referer (–º–æ–∂–µ—Ç –Ω–µ —Å—Ä–∞–±–æ—Ç–∞—Ç—å —Å undetected-chromedriver)
            try:
                driver.execute_cdp_cmd('Network.enable', {})
                driver.execute_cdp_cmd('Network.setExtraHTTPHeaders', {
                    'headers': {
                        'Referer': 'https://www.google.com/',
                    }
                })
            except:
                logger.debug("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å extra headers (–≤–æ–∑–º–æ–∂–Ω–æ undetected-chromedriver)")
            
            driver.get(url)
            
            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (WSJ —Ç—Ä–µ–±—É–µ—Ç –≤—Ä–µ–º–µ–Ω–∏)
            time.sleep(8)
            
            # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ lazy-load –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight / 2);")
            time.sleep(2)
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(1)
            
            html = driver.page_source
            logger.info(f"üìÑ HTML –¥–ª–∏–Ω–∞: {len(html)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title = self._extract_title(driver)
            if not title:
                logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∑–∞–≥–æ–ª–æ–≤–æ–∫")
                return {}
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
            description = self._extract_description(driver)
            
            # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ JSON-LD (—á–∞—Å—Ç–æ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ paywall)
            content = self._extract_content_from_jsonld(driver)
            
            # –ï—Å–ª–∏ JSON-LD –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∏–ª–∏ —Ç–µ–∫—Å—Ç–∞ –º–∞–ª–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
            if not content or len(content) < 500:
                logger.info("‚ö†Ô∏è JSON-LD –Ω–µ –¥–∞–ª –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥")
                content = self._extract_content(driver)
            else:
                logger.info(f"‚úÖ JSON-LD –¥–∞–ª {len(content)} —Å–∏–º–≤–æ–ª–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            published = self._extract_publish_date(driver)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            images = self._extract_images(driver)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–∏–¥–µ–æ
            videos = self._extract_videos(driver)
            
            result = {
                'title': title,
                'description': description,
                'content': content,
                'published': published,
                'images': images,
                'videos': videos,
                'source': 'Wall Street Journal'
            }
            
            logger.info(f"üìù Selenium –ø–∞—Ä—Å–∏–Ω–≥: –∑–∞–≥–æ–ª–æ–≤–æ–∫='{title[:50]}...', –æ–ø–∏—Å–∞–Ω–∏–µ='{description[:50] if description else ''}...', —Å—Ç–∞—Ç—å—è={len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ Selenium –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}", exc_info=True)
            return {}
        finally:
            if driver_created and driver:
                driver.quit()
    
    def _extract_title(self, driver) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—å–∏"""
        try:
            selectors = [
                'h1[class*="headline"]',
                'h1[class*="Headline"]',
                'h1[data-testid="headline"]',
                'h1.wsj-article-headline',
                'h1',
                'meta[property="og:title"]'
            ]
            
            for selector in selectors:
                try:
                    if 'meta' in selector:
                        element = driver.find_element(By.CSS_SELECTOR, selector)
                        title = element.get_attribute('content')
                    else:
                        element = driver.find_element(By.CSS_SELECTOR, selector)
                        title = element.text.strip()
                    
                    if title:
                        logger.info(f"‚úÖ –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞–π–¥–µ–Ω —á–µ—Ä–µ–∑ '{selector}': {title[:50]}...")
                        return title
                except:
                    continue
            
            logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∑–∞–≥–æ–ª–æ–≤–æ–∫")
            return ""
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞: {e}")
            return ""
    
    def _extract_description(self, driver) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏"""
        try:
            selectors = [
                'p[class*="dek"]',
                'p[class*="summary"]',
                'div[class*="article-summary"] p',
                'meta[property="og:description"]',
                'meta[name="description"]'
            ]
            
            for selector in selectors:
                try:
                    if 'meta' in selector:
                        element = driver.find_element(By.CSS_SELECTOR, selector)
                        description = element.get_attribute('content')
                    else:
                        element = driver.find_element(By.CSS_SELECTOR, selector)
                        description = element.text.strip()
                    
                    if description:
                        logger.info(f"‚úÖ –û–ø–∏—Å–∞–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–æ —á–µ—Ä–µ–∑ '{selector}'")
                        return description
                except:
                    continue
            
            logger.warning("‚ö†Ô∏è –û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            return ""
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ–ø–∏—Å–∞–Ω–∏—è: {e}")
            return ""
    
    def _extract_content_from_jsonld(self, driver) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ JSON-LD (—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)"""
        try:
            from bs4 import BeautifulSoup
            import json
            
            html = driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            # –ò—â–µ–º –≤—Å–µ script —Ç–µ–≥–∏ —Å type="application/ld+json"
            json_ld_scripts = soup.find_all('script', type='application/ld+json')
            
            for script in json_ld_scripts:
                try:
                    if not script.string:
                        continue
                    
                    data = json.loads(script.string)
                    
                    # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –∫–∞–∫ –æ–¥–∏–Ω–æ—á–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã, —Ç–∞–∫ –∏ –º–∞—Å—Å–∏–≤—ã
                    items = data if isinstance(data, list) else [data]
                    
                    for item in items:
                        if not isinstance(item, dict):
                            continue
                        
                        # –ò—â–µ–º articleBody
                        if 'articleBody' in item:
                            article_body = item['articleBody']
                            if article_body and isinstance(article_body, str) and len(article_body) > 500:
                                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω articleBody –≤ JSON-LD: {len(article_body)} —Å–∏–º–≤–æ–ª–æ–≤")
                                return article_body
                        
                        # –ò–Ω–æ–≥–¥–∞ –∫–æ–Ω—Ç–µ–Ω—Ç –≤ description
                        if 'description' in item and len(str(item['description'])) > 500:
                            desc = item['description']
                            if isinstance(desc, str):
                                logger.info(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º description –∏–∑ JSON-LD: {len(desc)} —Å–∏–º–≤–æ–ª–æ–≤")
                                return desc
                        
                except json.JSONDecodeError:
                    continue
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON-LD: {e}")
                    continue
            
            logger.debug("JSON-LD –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç articleBody")
            return ""
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è JSON-LD: {e}")
            return ""
    
    def _extract_content(self, driver) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏"""
        try:
            paragraphs = []
            used_selector = None
            
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è WSJ
            selectors = [
                'div[class*="article-content"] p',
                'div[class*="ArticleBody"] p',
                'article p',
                'div[class*="article-body"] p',
                'div[class*="story-body"] p',
                'p[class*="article"]',
            ]
            
            for selector in selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        temp_paragraphs = []
                        for element in elements:
                            text = element.text.strip()
                            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã: –º–∏–Ω–∏–º—É–º 20 —Å–∏–º–≤–æ–ª–æ–≤, –Ω–µ –Ω–∞–≤–∏–≥–∞—Ü–∏—è
                            if text and len(text) > 20 and not text.startswith('Read') and not text.startswith('Related'):
                                if text not in temp_paragraphs:
                                    temp_paragraphs.append(text)
                        
                        if len(temp_paragraphs) >= 3:
                            paragraphs = temp_paragraphs
                            used_selector = selector
                            logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(paragraphs)} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ —á–µ—Ä–µ–∑ '{selector}'")
                            break
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ —Å —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º '{selector}': {e}")
                    continue
            
            # –ï—Å–ª–∏ –º–∞–ª–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –ø—Ä–æ–±—É–µ–º BeautifulSoup –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É
            if len(paragraphs) < 5:
                logger.info("‚ö†Ô∏è –ú–∞–ª–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ —á–µ—Ä–µ–∑ Selenium, –ø—Ä–æ–±—É–µ–º BeautifulSoup...")
                try:
                    from bs4 import BeautifulSoup
                    html = driver.page_source
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # –ò—â–µ–º article body —á–µ—Ä–µ–∑ BeautifulSoup
                    article_selectors = [
                        {'name': 'div', 'class_': lambda x: x and 'article-content' in str(x).lower()},
                        {'name': 'div', 'class_': lambda x: x and 'ArticleBody' in str(x)},
                        {'name': 'article'},
                    ]
                    
                    for sel in article_selectors:
                        if 'class_' in sel:
                            article_body = soup.find(sel['name'], class_=sel['class_'])
                        else:
                            article_body = soup.find(sel['name'])
                        
                        if article_body:
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
                            ps = article_body.find_all('p')
                            temp_paragraphs = []
                            for p in ps:
                                text = p.get_text().strip()
                                if text and len(text) > 20:
                                    if text not in temp_paragraphs:
                                        temp_paragraphs.append(text)
                            
                            if len(temp_paragraphs) > len(paragraphs):
                                paragraphs = temp_paragraphs
                                logger.info(f"‚úÖ BeautifulSoup: –Ω–∞–π–¥–µ–Ω–æ {len(paragraphs)} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤")
                                break
                except Exception as e:
                    logger.debug(f"BeautifulSoup fallback –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")
            
            # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –º–∞–ª–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ—Å—å body text
            if not paragraphs or len(paragraphs) < 3:
                logger.warning("‚ö†Ô∏è –ú–∞–ª–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç body")
                try:
                    body = driver.find_element(By.TAG_NAME, 'body')
                    full_text = body.text.strip()
                    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –ø–æ –ø–µ—Ä–µ–Ω–æ—Å–∞–º —Å—Ç—Ä–æ–∫
                    paragraphs = [p.strip() for p in full_text.split('\n') if len(p.strip()) > 50]
                    logger.info(f"‚ö†Ô∏è Fallback: –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(paragraphs)} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –∏–∑ body")
                except:
                    pass
            
            content = ' '.join(paragraphs)
            logger.info(f"üìÑ –°–æ–±—Ä–∞–Ω–æ {len(paragraphs)} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤, –æ–±—â–∞—è –¥–ª–∏–Ω–∞: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            if used_selector:
                logger.info(f"üìÑ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω —Å–µ–ª–µ–∫—Ç–æ—Ä: {used_selector}")
            
            return content
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}")
            return ""
    
    def _extract_publish_date(self, driver) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"""
        try:
            selectors = [
                'time',
                'meta[property="article:published_time"]',
                'meta[name="article.published"]',
                'span[class*="timestamp"]',
                'div[class*="timestamp"]'
            ]
            
            for selector in selectors:
                try:
                    element = driver.find_element(By.CSS_SELECTOR, selector)
                    if 'meta' in selector:
                        date = element.get_attribute('content')
                    elif selector == 'time':
                        date = element.get_attribute('datetime') or element.text
                    else:
                        date = element.text
                    
                    if date:
                        logger.info(f"‚úÖ –î–∞—Ç–∞ –Ω–∞–π–¥–µ–Ω–∞ —á–µ—Ä–µ–∑ '{selector}': {date}")
                        return date
                except:
                    continue
            
            logger.warning("‚ö†Ô∏è –î–∞—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é")
            from datetime import datetime
            return datetime.now().isoformat()
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞—Ç—ã: {e}")
            from datetime import datetime
            return datetime.now().isoformat()
    
    def _extract_images(self, driver) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ —Å—Ç–∞—Ç—å–∏"""
        try:
            images = []
            
            # –í–ê–ñ–ù–û: –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å og:image –∫–∞–∫ fallback –¥–ª—è paywall
            try:
                og_image = driver.find_element(By.CSS_SELECTOR, 'meta[property="og:image"]')
                og_image_url = og_image.get_attribute('content')
                if og_image_url:
                    logger.info(f"üì∏ –ù–∞–π–¥–µ–Ω–æ og:image: {og_image_url[:100]}...")
                    images.append(og_image_url)
            except:
                pass
            
            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ twitter:image
            try:
                twitter_image = driver.find_element(By.CSS_SELECTOR, 'meta[name="twitter:image"]')
                twitter_image_url = twitter_image.get_attribute('content')
                if twitter_image_url and twitter_image_url not in images:
                    logger.info(f"üì∏ –ù–∞–π–¥–µ–Ω–æ twitter:image: {twitter_image_url[:100]}...")
                    images.append(twitter_image_url)
            except:
                pass
            
            # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ article body
            selectors = [
                'img',  # –°–Ω–∞—á–∞–ª–∞ –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                'div[class*="article-content"] img',
                'article img',
                'figure img',
                'picture img',
                'main img'
            ]
            
            for selector in selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    for element in elements:
                        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
                        src = (element.get_attribute('src') or 
                               element.get_attribute('data-src') or 
                               element.get_attribute('data-lazy-src') or
                               element.get_attribute('srcset'))
                        
                        if src:
                            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π URL –∏–∑ srcset –µ—Å–ª–∏ —ç—Ç–æ srcset
                            if 'srcset' in str(src) or ',' in str(src):
                                src = src.split(',')[0].split(' ')[0]
                            
                            # –§–∏–ª—å—Ç—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                            src_lower = src.lower()
                            
                            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏–∫–æ–Ω–∫–∏ –∏ –º–∞–ª–µ–Ω—å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                            if any(skip in src_lower for skip in ['icon', 'logo', 'avatar', 'placeholder']):
                                continue
                            
                            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                            try:
                                width = element.get_attribute('width')
                                height = element.get_attribute('height')
                                if width and height:
                                    if int(width) < 200 or int(height) < 200:
                                        continue
                            except:
                                pass
                            
                            # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                            if src not in images:
                                images.append(src)
                    
                    if len(images) > 1:  # –£ –Ω–∞—Å —É–∂–µ –µ—Å—Ç—å og:image, –∏—â–µ–º –±–æ–ª—å—à–µ
                        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ '{selector}'")
                        # –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º, —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ —Å —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π '{selector}': {e}")
                    continue
            
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            images = list(dict.fromkeys(images))
            
            logger.info(f"üì∏ –í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(images)}")
            for i, img in enumerate(images[:3], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
                logger.info(f"  üì∏ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {i}: {img[:100]}...")
            
            return images
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {e}")
            return []
    
    def _extract_videos(self, driver) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤–∏–¥–µ–æ –∏–∑ —Å—Ç–∞—Ç—å–∏"""
        try:
            videos = []
            
            selectors = [
                'video source',
                'video',
                'iframe[src*="youtube"]',
                'iframe[src*="vimeo"]'
            ]
            
            for selector in selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    for element in elements:
                        if selector == 'video source':
                            src = element.get_attribute('src')
                        elif selector == 'video':
                            src = element.get_attribute('src')
                        else:  # iframe
                            src = element.get_attribute('src')
                        
                        if src and src not in videos:
                            videos.append(src)
                except:
                    continue
            
            logger.info(f"üé¨ –í—Å–µ–≥–æ –≤–∏–¥–µ–æ: {len(videos)}")
            return videos
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≤–∏–¥–µ–æ: {e}")
            return []
    
    def extract_media(self, url: str, content: Dict[str, Any]) -> Dict[str, List[str]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ–¥–∏–∞ —Ñ–∞–π–ª—ã –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        images = content.get('images', []) or []
        videos = content.get('videos', []) or []
        logger.info(f"üì∏ WSJ media for this URL: images={len(images)}, videos={len(videos)}")
        return {'images': images, 'videos': videos}
    
    def validate_content(self, content: Dict[str, Any]) -> bool:
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –≤–∞–ª–∏–¥–∞—Ç–æ—Ä
        if not self.content_validator.validate_quality(content):
            logger.warning("–ö–æ–Ω—Ç–µ–Ω—Ç WSJ –Ω–µ –ø—Ä–æ—à–µ–ª –≤–∞–ª–∏–¥–∞—Ü–∏—é –∫–∞—á–µ—Å—Ç–≤–∞")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–µ–¥–∏–∞
        images = content.get('images', [])
        videos = content.get('videos', [])
        
        has_media = len(images) > 0 or len(videos) > 0
        if not has_media:
            logger.warning("–ö–æ–Ω—Ç–µ–Ω—Ç WSJ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –º–µ–¥–∏–∞")
            return False
        
        logger.info(f"‚úÖ –ö–æ–Ω—Ç–µ–Ω—Ç WSJ –≤–∞–ª–∏–¥–µ–Ω: {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {len(videos)} –≤–∏–¥–µ–æ")
        return True

