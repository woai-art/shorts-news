#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–æ–≤—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è Politico —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º DuckDuckGo
–û–±—Ö–æ–¥–∏—Ç –±–ª–æ–∫–∏—Ä–æ–≤–∫—É Politico —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫ DuckDuckGo
"""

import requests
from bs4 import BeautifulSoup
import time
import logging
from urllib.parse import urljoin, urlparse
import re

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PoliticoDuckDuckGoParser:
    """–ü–∞—Ä—Å–µ—Ä Politico —á–µ—Ä–µ–∑ DuckDuckGo –ø–æ–∏—Å–∫"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })
    
    def search_politico_article(self, url: str):
        """–ü–æ–∏—Å–∫ —Å—Ç–∞—Ç—å–∏ Politico —á–µ—Ä–µ–∑ DuckDuckGo"""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ URL –¥–ª—è –ø–æ–∏—Å–∫–∞
            parsed_url = urlparse(url)
            path_parts = parsed_url.path.split('/')
            
            # –ò—â–µ–º –≥–æ–¥, –º–µ—Å—è—Ü, –¥–µ–Ω—å –∏ slug
            if len(path_parts) >= 4:
                year = path_parts[2] if path_parts[2].isdigit() else None
                month = path_parts[3] if path_parts[3].isdigit() else None
                slug = path_parts[-1] if path_parts[-1] else None
                
                # –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
                search_queries = []
                
                if slug:
                    # –£–±–∏—Ä–∞–µ–º ID –∏–∑ –∫–æ–Ω—Ü–∞ slug
                    clean_slug = re.sub(r'-\d+$', '', slug)
                    search_queries.append(f'site:politico.com "{clean_slug}"')
                
                # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—â–∏–π –ø–æ–∏—Å–∫ –ø–æ –¥–æ–º–µ–Ω—É
                search_queries.append(f'site:politico.com news')
                
                logger.info(f"üîç –ü–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã: {search_queries}")
                
                # –ü—Ä–æ–±—É–µ–º –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å
                for query in search_queries:
                    result = self._search_duckduckgo(query)
                    if result:
                        return result
                
                return None
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return None
    
    def _search_duckduckgo(self, query: str):
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ DuckDuckGo"""
        try:
            # DuckDuckGo –ø–æ–∏—Å–∫
            search_url = "https://duckduckgo.com/html/"
            params = {
                'q': query,
                'kl': 'us-en'
            }
            
            logger.info(f"üîç –ü–æ–∏—Å–∫ DuckDuckGo: {query}")
            response = self.session.get(search_url, params=params, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ò—â–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
            results = soup.find_all('a', class_='result__a')
            
            for result in results[:5]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                href = result.get('href')
                if href and 'politico.com' in href:
                    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏
                    if href.startswith('//'):
                        href = 'https:' + href
                    elif href.startswith('/'):
                        href = 'https://www.politico.com' + href
                    
                    logger.info(f"üì∞ –ù–∞–π–¥–µ–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {href}")
                    
                    # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç
                    article_data = self._fetch_article_content(href)
                    if article_data:
                        return article_data
            
            logger.warning("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return None
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ DuckDuckGo: {e}")
            return None
    
    def _fetch_article_content(self, url: str):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç–∞—Ç—å–∏"""
        try:
            logger.info(f"üìñ –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞—Ç—å—é: {url}")
            
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ User-Agent
            user_agents = [
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0'
            ]
            
            for i, user_agent in enumerate(user_agents):
                try:
                    self.session.headers.update({'User-Agent': user_agent})
                    response = self.session.get(url, timeout=15)
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
                        title = self._extract_title(soup)
                        description = self._extract_description(soup)
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ CAPTCHA
                        if any(indicator in description.lower() for indicator in [
                            "–ø—Ä–æ–≤–µ—Ä—è–µ–º, —á–µ–ª–æ–≤–µ–∫ –ª–∏ –≤—ã", "please verify you are human",
                            "checking your browser", "captcha"
                        ]):
                            logger.warning(f"üö´ CAPTCHA –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Å User-Agent {i+1}")
                            continue
                        
                        if title != "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞" and description != "–ë–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è":
                            logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç —Å User-Agent {i+1}")
                            return {
                                'success': True,
                                'url': url,
                                'title': title,
                                'description': description,
                                'source': 'Politico (DuckDuckGo)',
                                'published': self._extract_date(soup),
                                'images': self._extract_images(soup, url),
                                'content_type': 'news_article',
                                'parsed_with': 'duckduckgo'
                            }
                    
                    time.sleep(2)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
                    
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ —Å User-Agent {i+1}: {e}")
                    continue
            
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —Å–æ –≤—Å–µ–º–∏ User-Agent")
            return None
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}")
            return None
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
        # Open Graph title
        og_title = soup.find('meta', property='og:title')
        if og_title and og_title.get('content'):
            return og_title['content'].strip()
        
        # Twitter title
        twitter_title = soup.find('meta', attrs={'name': 'twitter:title'})
        if twitter_title and twitter_title.get('content'):
            return twitter_title['content'].strip()
        
        # HTML title
        title_tag = soup.find('title')
        if title_tag and title_tag.text.strip():
            return title_tag.text.strip()
        
        # H1 –∑–∞–≥–æ–ª–æ–≤–æ–∫
        h1 = soup.find('h1')
        if h1 and h1.text.strip():
            return h1.text.strip()
        
        return "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
    
    def _extract_description(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è"""
        # Open Graph description
        og_desc = soup.find('meta', property='og:description')
        if og_desc and og_desc.get('content'):
            return og_desc['content'].strip()
        
        # Twitter description
        twitter_desc = soup.find('meta', attrs={'name': 'twitter:description'})
        if twitter_desc and twitter_desc.get('content'):
            return twitter_desc['content'].strip()
        
        # Meta description
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            return meta_desc['content'].strip()
        
        # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–∞—Ç—å–∏
        article_content = soup.find('article') or soup.find('div', class_='story')
        if article_content:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
            paragraphs = article_content.find_all('p')
            text_parts = []
            for p in paragraphs[:5]:  # –ü–µ—Ä–≤—ã–µ 5 –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
                text = p.get_text(strip=True)
                if text and len(text) > 20:
                    text_parts.append(text)
            
            if text_parts:
                return ' '.join(text_parts)
        
        return "–ë–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è"
    
    def _extract_date(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"""
        # Open Graph published_time
        og_date = soup.find('meta', property='article:published_time')
        if og_date and og_date.get('content'):
            return og_date['content']
        
        # Twitter date
        twitter_date = soup.find('meta', name='twitter:data1')
        if twitter_date and twitter_date.get('content'):
            return twitter_date['content']
        
        # –ò—â–µ–º –¥–∞—Ç—É –≤ —Ç–µ–∫—Å—Ç–µ
        date_patterns = [
            r'\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}',
            r'\d{4}-\d{2}-\d{2}',
            r'[A-Za-z]+ \d{1,2}, \d{4}'
        ]
        
        page_text = soup.get_text()
        for pattern in date_patterns:
            match = re.search(pattern, page_text)
            if match:
                return match.group()
        
        from datetime import datetime
        return datetime.now().isoformat()
    
    def _extract_images(self, soup: BeautifulSoup, base_url: str) -> list:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        images = []
        
        # Open Graph image
        og_image = soup.find('meta', property='og:image')
        if og_image and og_image.get('content'):
            images.append(urljoin(base_url, og_image['content']))
        
        # Twitter image
        twitter_image = soup.find('meta', attrs={'name': 'twitter:image'})
        if twitter_image and twitter_image.get('content'):
            images.append(urljoin(base_url, twitter_image['content']))
        
        return images[:3]  # –ú–∞–∫—Å–∏–º—É–º 3 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è


def test_politico_parser():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–∞"""
    parser = PoliticoDuckDuckGoParser()
    
    # –¢–µ—Å—Ç–æ–≤–∞—è —Å—Å—ã–ª–∫–∞
    test_url = "https://www.politico.com/news/2025/09/16/cruz-says-first-amendment-absolutely-protects-hate-speech-in-wake-of-charlie-kirk-killing-00566448"
    
    print(f"üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–∞—Ä—Å–µ—Ä Politico —Å DuckDuckGo")
    print(f"üì∞ URL: {test_url}")
    print("=" * 80)
    
    result = parser.search_politico_article(test_url)
    
    if result:
        print("‚úÖ –£–°–ü–ï–•!")
        print(f"Title: {result['title']}")
        print(f"Description length: {len(result['description'])}")
        print(f"Description preview: {result['description'][:300]}...")
        print(f"Source: {result['source']}")
        print(f"Parsed with: {result['parsed_with']}")
        print(f"Images: {len(result['images'])}")
    else:
        print("‚ùå –ù–ï –£–î–ê–õ–û–°–¨")
    
    return result


if __name__ == "__main__":
    test_politico_parser()
