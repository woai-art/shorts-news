#!/usr/bin/env python3
"""
–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –ø–æ–ª–Ω—ã—Ö –ø–∞–∫–µ—Ç–æ–≤ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø–∞—Ä—Å–∏–Ω–≥, LLM –æ–±—Ä–∞–±–æ—Ç–∫—É –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–¥–µ–æ
"""

import os
import json
import logging
from datetime import datetime
from typing import Dict, Optional, Any, List
from urllib.parse import urlparse

logger = logging.getLogger(__name__)

class NewsPackageProcessor:
    """–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ–ª–Ω—ã—Ö –ø–∞–∫–µ—Ç–æ–≤ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, web_parser, llm_provider, config: Dict):
        self.web_parser = web_parser
        self.llm_provider = llm_provider
        self.config = config
        
    def process_news_url(self, url: str) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ URL –Ω–æ–≤–æ—Å—Ç–∏ –≤ –ø–æ–ª–Ω—ã–π –ø–∞–∫–µ—Ç –¥–∞–Ω–Ω—ã—Ö"""
        try:
            # –®–∞–≥ 1: –ü–∞—Ä—Å–∏–Ω–≥ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã
            logger.info(f"üåê –ü–∞—Ä—Å–∏–Ω–≥ URL: {url}")
            parsed_data = self.web_parser.parse_url(url)
            
            if not parsed_data or not parsed_data.get('success'):
                logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å URL: {url}")
                return self._create_error_package(url, "Parsing failed")
            
            # –®–∞–≥ 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            base_package = self._extract_base_package(parsed_data, url)
            
            # –®–∞–≥ 3: LLM –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ SEO
            logger.info(f"ü§ñ LLM –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
            llm_package = self.llm_provider.generate_complete_news_package(parsed_data)
            
            # –®–∞–≥ 4: –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø–∞–∫–µ—Ç
            final_package = self._merge_packages(base_package, llm_package)
            
            logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω –ø–æ–ª–Ω—ã–π –ø–∞–∫–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏: {final_package['content']['title'][:50]}...")
            return final_package
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏ {url}: {e}")
            return self._create_error_package(url, str(e))
    
    def _extract_base_package(self, parsed_data: Dict, url: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ –ø–∞–∫–µ—Ç–∞ –∏–∑ —Å–ø–∞—Ä—Å–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        source_name = parsed_data.get('source', self._extract_domain_name(url))
        source_logo = self._get_source_logo_path(source_name)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        published_date, published_time = self._parse_publication_date(
            parsed_data.get('published', '')
        )
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ–¥–∏–∞ –¥–∞–Ω–Ω—ã—Ö
        media_data = self._extract_media_data(parsed_data)
        
        return {
            "source": {
                "name": source_name,
                "logo_url": source_logo,
                "author": parsed_data.get('author', ''),
                "url": url
            },
            "publication": {
                "date": published_date,
                "time": published_time,
                "timestamp": parsed_data.get('published', datetime.now().isoformat())
            },
            "media": media_data,
            "raw_content": {
                "original_title": parsed_data.get('title', ''),
                "original_description": parsed_data.get('description', ''),
                "images": parsed_data.get('images', [])
            }
        }
    
    def _extract_media_data(self, parsed_data: Dict) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ–¥–∏–∞ –¥–∞–Ω–Ω—ã—Ö"""
        images = parsed_data.get('images', [])
        
        primary_image = None
        if images:
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–µ
            primary_image = images[0] if isinstance(images[0], str) else images[0].get('url', '')
        
        return {
            "primary_image": primary_image,
            "video_url": parsed_data.get('video_url', ''),
            "thumbnail": primary_image,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ –∂–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–∞–∫ thumbnail
            "alt_text": parsed_data.get('title', '')
        }
    
    def _parse_publication_date(self, published_str: str) -> tuple:
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"""
        try:
            if not published_str:
                now = datetime.now()
                return now.strftime('%d.%m.%Y'), now.strftime('%H:%M')
            
            # –ü–æ–ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
            if 'T' in published_str:
                dt = datetime.fromisoformat(published_str.replace('Z', '+00:00'))
            else:
                dt = datetime.strptime(published_str, '%Y-%m-%d %H:%M:%S')
            
            return dt.strftime('%d.%m.%Y'), dt.strftime('%H:%M')
            
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É '{published_str}': {e}")
            now = datetime.now()
            return now.strftime('%d.%m.%Y'), now.strftime('%H:%M')
    
    def _extract_domain_name(self, url: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–∏ –¥–æ–º–µ–Ω–∞ –∏–∑ URL"""
        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            
            # –£–±–∏—Ä–∞–µ–º www. –µ—Å–ª–∏ –µ—Å—Ç—å
            if domain.startswith('www.'):
                domain = domain[4:]
                
            # –ò–∑–≤–µ—Å—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            domain_mapping = {
                'cnn.com': 'CNN',
                'bbc.com': 'BBC',
                'reuters.com': 'Reuters',
                'nytimes.com': 'The New York Times',
                'washingtonpost.com': 'The Washington Post',
                'foxnews.com': 'Fox News',
                'nbcnews.com': 'NBC News',
                'abcnews.go.com': 'ABC News',
                'cbsnews.com': 'CBS News'
            }
            
            return domain_mapping.get(domain, domain.split('.')[0].upper())
            
        except Exception:
            return 'News Source'
    
    def _get_source_logo_path(self, source_name: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—É—Ç–∏ –∫ –ª–æ–≥–æ—Ç–∏–ø—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        logo_mapping = {
            'CNN': '../media/CNN.jpg',
            'BBC': '../media/BBC.jpg',
            'Reuters': '../media/Reuters.jpg',
            'The New York Times': '../media/NYTimes.png',
            'The Washington Post': '../media/WashingtonPost.jpg',
            'Fox News': '../media/FoxNews.png',
            'NBC News': '../media/NBC.jpg',
            'ABC News': '../media/ABC.jpg',
            'CBS News': '../media/CBS.jpg'
        }
        
        return logo_mapping.get(source_name, '../media/default_news.jpg')
    
    def _merge_packages(self, base_package: Dict, llm_package: Dict) -> Dict[str, Any]:
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ –ø–∞–∫–µ—Ç–∞ —Å LLM –ø–∞–∫–µ—Ç–æ–º"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è
        llm_package['metadata']['processed_at'] = datetime.now().isoformat()
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–∞–∫–µ—Ç—ã
        final_package = {
            **base_package,
            **llm_package
        }
        
        return final_package
    
    def _create_error_package(self, url: str, error_msg: str) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–∫–µ—Ç–∞ –¥–ª—è —Å–ª—É—á–∞—è –æ—à–∏–±–∫–∏"""
        now = datetime.now()
        
        return {
            "source": {
                "name": "Unknown Source",
                "logo_url": "../media/default_news.jpg",
                "author": "",
                "url": url
            },
            "publication": {
                "date": now.strftime('%d.%m.%Y'),
                "time": now.strftime('%H:%M'),
                "timestamp": now.isoformat()
            },
            "media": {
                "primary_image": "../resources/default_backgrounds/news_default.jpg",
                "video_url": "",
                "thumbnail": "",
                "alt_text": "News Error"
            },
            "content": {
                "title": "Error Processing News",
                "summary": f"Failed to process news from {url}: {error_msg}",
                "key_points": [f"Error: {error_msg}"],
                "quotes": []
            },
            "seo": {
                "youtube_title": "News Processing Error",
                "youtube_description": f"Error processing news content. #error #news",
                "tags": ["error", "news"],
                "hashtags": ["#error", "#news"],
                "category": "News & Politics"
            },
            "metadata": {
                "language": "en",
                "processed_at": now.isoformat(),
                "confidence_score": 0.0,
                "error": error_msg
            }
        }
    
    def validate_package(self, package: Dict) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞–∫–µ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö"""
        required_sections = ['source', 'publication', 'content', 'seo']
        
        for section in required_sections:
            if section not in package:
                logger.error(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å–µ–∫—Ü–∏—è {section} –≤ –ø–∞–∫–µ—Ç–µ")
                return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –≤ content
        content = package.get('content', {})
        if not content.get('title') or not content.get('summary'):
            logger.error("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è title –∏–ª–∏ summary")
            return False
        
        logger.info("‚úÖ –ü–∞–∫–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ—à–µ–ª –≤–∞–ª–∏–¥–∞—Ü–∏—é")
        return True
