#!/usr/bin/env python3
"""
–ú–æ–¥—É–ª—å –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è —Å–∏—Å—Ç–µ–º—ã shorts_news
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–∞—Ä—Å–∏–Ω–≥ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: —Å–∞–π—Ç—ã, Twitter, Facebook, Telegram –∏ –¥—Ä.
"""

import os
import sys
import logging
import re
import requests
from typing import Dict, Optional, Any, Tuple, List
from urllib.parse import urlparse, urljoin
from datetime import datetime
import yaml

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—É—Ç–∏ –∫ –º–æ–¥—É–ª—è–º
sys.path.append(os.path.dirname(__file__))

from bs4 import BeautifulSoup
import feedparser
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è urllib3 –æ –∑–∞–∫—Ä—ã—Ç–∏–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —É—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è urllib3 –Ω–∞ WARNING —á—Ç–æ–±—ã —Å–∫—Ä—ã—Ç—å retry —Å–æ–æ–±—â–µ–Ω–∏—è
logging.getLogger("urllib3.connectionpool").setLevel(logging.WARNING)

class WebParser:
    """–ü–∞—Ä—Å–µ—Ä –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π"""

    def __init__(self, config_path: str):
        self.config = self._load_config(config_path)
        self.session = requests.Session()
        
        # –°–ø–∏—Å–æ–∫ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö User-Agent –¥–ª—è —Ä–æ—Ç–∞—Ü–∏–∏
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15'
        ]
        
        self._setup_session()

        # Selenium –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤
        self.driver = None
        self._init_selenium()

    def _load_config(self, config_path: str) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)

    def _setup_session(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–µ—Å—Å–∏–∏ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏"""
        import random
        
        self.session.headers.update({
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'en-US,en;q=0.9,ru;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0'
        })

    def _init_selenium(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Selenium –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤"""
        try:
            chrome_options = Options()
            chrome_options.add_argument("--headless")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--window-size=1920,1080")
            chrome_options.add_argument("--disable-images")  # –î–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            
            # –£–ª—É—á—à–µ–Ω–Ω–∞—è –º–∞—Å–∫–∏—Ä–æ–≤–∫–∞ –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # –û—Ç–∫–ª—é—á–∞–µ–º GCM –∏ –¥—Ä—É–≥–∏–µ —Å–µ—Ä–≤–∏—Å—ã –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –æ—à–∏–±–æ–∫
            chrome_options.add_argument("--disable-background-timer-throttling")
            chrome_options.add_argument("--disable-backgrounding-occluded-windows")
            chrome_options.add_argument("--disable-renderer-backgrounding")
            chrome_options.add_argument("--disable-features=TranslateUI,VizDisplayCompositor")
            chrome_options.add_argument("--disable-component-extensions-with-background-pages")
            chrome_options.add_argument("--disable-default-apps")
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–ª–∞–≥–∏ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Å–µ—Ç–µ–≤—ã—Ö –æ—à–∏–±–æ–∫
            chrome_options.add_argument("--disable-extensions")
            chrome_options.add_argument("--disable-plugins")
            chrome_options.add_argument("--disable-sync")
            chrome_options.add_argument("--disable-translate")
            chrome_options.add_argument("--disable-background-networking")
            chrome_options.add_argument("--disable-background-downloads")
            chrome_options.add_argument("--disable-client-side-phishing-detection")
            chrome_options.add_argument("--disable-component-update")
            chrome_options.add_argument("--disable-domain-reliability")
            chrome_options.add_argument("--disable-features=VizDisplayCompositor,TranslateUI,BlinkGenPropertyTrees")
            
            # –û—Ç–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —à—É–º–∞
            chrome_options.add_argument("--log-level=3")
            chrome_options.add_argument("--silent")
            chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])
            chrome_options.add_argument("--disable-web-security")
            chrome_options.add_argument("--allow-running-insecure-content")
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ User-Agent –∑–∞–≥–æ–ª–æ–≤–∫–∏
            chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

            self.driver = webdriver.Chrome(options=chrome_options)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –º–∞—Å–∫–∏—Ä–æ–≤–∫–∞ –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –¥—Ä–∞–π–≤–µ—Ä–∞
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            logger.info("Selenium –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤")
        except Exception as e:
            logger.warning(f"Selenium –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
            self.driver = None

    def _reinit_selenium(self):
        """–ü–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Selenium –¥—Ä–∞–π–≤–µ—Ä–∞ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö"""
        logger.info("üîÑ –ü–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Selenium –¥—Ä–∞–π–≤–µ—Ä–∞...")
        
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–∫—Ä—ã–≤–∞–µ–º —Å—Ç–∞—Ä—ã–π –¥—Ä–∞–π–≤–µ—Ä
        try:
            if self.driver:
                self.driver.quit()
        except:
            pass
        
        # –û—á–∏—â–∞–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –¥—Ä–∞–π–≤–µ—Ä
        self.driver = None
        
        # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∑–∞–∫—Ä—ã—Ç–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
        import time
        time.sleep(2)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–æ–≤—ã–π –¥—Ä–∞–π–≤–µ—Ä
        self._init_selenium()
        
        if self.driver:
            logger.info("‚úÖ Selenium –¥—Ä–∞–π–≤–µ—Ä —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        else:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Selenium –¥—Ä–∞–π–≤–µ—Ä")

    def _parse_with_selenium_fallback(self, url: str) -> Dict[str, Any]:
        """Fallback –ø–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ Selenium –¥–ª—è –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–∞–π—Ç–æ–≤"""
        logger.info(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º Selenium fallback –¥–ª—è {url}")
        
        if not self.driver:
            logger.warning("Selenium –¥—Ä–∞–π–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–ª—É—à–∫—É")
            return self._create_fallback_response(url)
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥—Ä–∞–π–≤–µ—Ä –∞–∫—Ç–∏–≤–µ–Ω
            try:
                self.driver.current_url
            except Exception:
                logger.warning("–î—Ä–∞–π–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º...")
                self._reinit_selenium()
                if not self.driver:
                    return self._create_fallback_response(url)
            
            self.driver.get(url)
            
            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            import time
            time.sleep(5)
            
            # –ü–æ–ª—É—á–∞–µ–º HTML –ø–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è JavaScript
            page_source = self.driver.page_source
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º HTML
            title = self._extract_title(soup)
            description = self._extract_description(soup)
            published = self._extract_date(soup)
            source = self._extract_source(url, soup)
            
            # –ï—Å–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏, –ø—Ä–æ–±—É–µ–º –ø—Ä—è–º–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Selenium
            if title == "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞":
                try:
                    title_element = self.driver.find_element(By.TAG_NAME, "h1")
                    if title_element.text.strip():
                        title = title_element.text.strip()
                except:
                    try:
                        title_element = self.driver.find_element(By.TAG_NAME, "title")
                        if title_element.text.strip():
                            title = title_element.text.strip()
                    except:
                        pass
            
            # –ï—Å–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º, –ø—Ä–æ–±—É–µ–º –ø—Ä—è–º–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ
            if description == "–ë–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è":
                try:
                    # –ò—â–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –≤ —Å—Ç–∞—Ç—å–µ
                    paragraphs = self.driver.find_elements(By.CSS_SELECTOR, "article p, .content p, .story p, .article__content p, p")
                    valid_paragraphs = []
                    for p in paragraphs[:5]:
                        text = p.text.strip()
                        if text and len(text) > 50 and not text.startswith(('Advertisement', 'Subscribe', 'Follow', 'Ad')):
                            valid_paragraphs.append(text)
                            if len(' '.join(valid_paragraphs)) > 300:
                                break
                    
                    if valid_paragraphs:
                        description = ' '.join(valid_paragraphs)[:1000]
                except Exception as e:
                    logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –æ–ø–∏—Å–∞–Ω–∏–µ —á–µ—Ä–µ–∑ Selenium: {e}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ BeautifulSoup –∏ Selenium
            images = self._extract_images(soup, url)
            # –ï—Å–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º, –ø—Ä–æ–±—É–µ–º Selenium
            if not images:
                images = []
            try:
                # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç–∞—Ç—å–∏ (–≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞)
                img_selectors = [
                    # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º meta —Ç–µ–≥–∏ - –æ–Ω–∏ –æ–±—ã—á–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç –æ—Å–Ω–æ–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    'meta[property="og:image"]',
                    'meta[name="twitter:image"]',
                    # –ü–æ—Ç–æ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                    'article img[src]:not([src*="logo"]):not([src*="keyart"]):not([src*="weekend"])',
                    '.story-body img[src]:not([src*="logo"]):not([src*="keyart"])',
                    '.article-content img[src]:not([src*="logo"]):not([src*="keyart"])',
                    # –û–±—â–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –∫–∞–∫ fallback
                    'article img[src]',
                    '.content img[src]', 
                    '.story img[src]'
                ]
                
                for selector in img_selectors:
                    try:
                        if 'meta' in selector:
                            # –î–ª—è meta —Ç–µ–≥–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥—Ä—É–≥–æ–π –ø–æ–¥—Ö–æ–¥
                            if 'og:image' in selector:
                                meta_elem = self.driver.find_element(By.CSS_SELECTOR, 'meta[property="og:image"]')
                                img_url = meta_elem.get_attribute('content')
                            elif 'twitter:image' in selector:
                                meta_elem = self.driver.find_element(By.CSS_SELECTOR, 'meta[name="twitter:image"]')
                                img_url = meta_elem.get_attribute('content')
                            
                            if img_url and img_url not in images:
                                # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É –∏–∑ Politico URL
                                direct_url = self._extract_direct_image_url(img_url)
                                final_url = direct_url if direct_url else img_url
                                
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º meta –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–æ–∂–µ
                                if self._is_valid_news_image(final_url):
                                    images.append(final_url)
                                    if direct_url:
                                        logger.info(f"üñºÔ∏è –ù–∞–π–¥–µ–Ω–æ –ø—Ä—è–º–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {final_url[:50]}...")
                                    else:
                                        logger.info(f"üñºÔ∏è –ù–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–µ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ meta: {final_url[:50]}...")
                                    break
                                else:
                                    logger.info(f"üö´ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω–æ–µ meta –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {final_url[:50]}...")
                        else:
                            # –î–ª—è –æ–±—ã—á–Ω—ã—Ö img —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                            img_elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                            for img in img_elements[:3]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–æ–ª—å—à–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                                src = img.get_attribute('src')
                                if src and src not in images and 'data:' not in src:
                                    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∫–ª–∞–º–Ω—ã–µ –∏ —Å–ª—É–∂–µ–±–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                                    if self._is_valid_news_image(src):
                                        images.append(src)
                                        logger.info(f"üñºÔ∏è –ù–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–µ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {src[:50]}...")
                                        if len(images) >= 2:
                                            break
                                    else:
                                        logger.info(f"üö´ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {src[:50]}...")
                            if images:
                                break
                    except Exception as e:
                        logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º {selector}: {e}")
                        continue
                        
                logger.info(f"üì∏ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ Selenium")
                        
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ Selenium: {e}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ –∏–∑ URL
            from urllib.parse import urlparse
            parsed_url = urlparse(url)
            source = parsed_url.netloc
            
            return {
                'success': True,
                'url': url,
                'title': title,
                'description': description,
                'published': datetime.now().isoformat(),
                'source': source,
                'images': images,
                'content_type': 'news_article',
                'parsed_with': 'selenium_fallback'
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ Selenium fallback –¥–ª—è {url}: {e}")
            return self._create_fallback_response(url)
    
    def _is_valid_news_image(self, img_url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–º –¥–ª—è –Ω–æ–≤–æ—Å—Ç–∏"""
        if not img_url:
            return False
        
        img_url_lower = img_url.lower()
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Ç–æ–ª—å–∫–æ —è–≤–Ω–æ —Ä–µ–∫–ª–∞–º–Ω—ã–µ –∏ —Å–ª—É–∂–µ–±–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        excluded_keywords = [
            'logo', 'banner', 'advertisement', 'ad-', 'promo', 
            'newsletter', 'subscribe', 'social-icon', 'avatar',
            'header-logo', 'footer-logo', 'nav-', 'menu-icon', 'button-',
            'placeholder', 'default-', '1x1', 'tracking', 'pixel'
        ]
        
        for keyword in excluded_keywords:
            if keyword in img_url_lower:
                return False
        
        # –ò—Å–∫–ª—é—á–∞–µ–º –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ URL –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
        small_size_patterns = ['16x16', '32x32', '50x50', '64x64', '100x100']
        for pattern in small_size_patterns:
            if pattern in img_url_lower:
                return False
        
        # –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏
        preferred_patterns = ['getty', 'photo', 'image', 'picture', 'news', 'story']
        for pattern in preferred_patterns:
            if pattern in img_url_lower:
                return True
        
        # –ï—Å–ª–∏ —Ä–∞–∑–º–µ—Ä —É–∫–∞–∑–∞–Ω –≤ URL –∏ –æ–Ω –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–æ–π
        import re
        size_match = re.search(r'(\d+)x(\d+)', img_url_lower)
        if size_match:
            width, height = int(size_match.group(1)), int(size_match.group(2))
            # –ü—Ä–∏–Ω–∏–º–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –±–æ–ª—å—à–µ 200x200
            return width >= 200 and height >= 200
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–∏–Ω–∏–º–∞–µ–º, –µ—Å–ª–∏ –Ω–µ—Ç —è–≤–Ω—ã—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π
        return True
    
    def _extract_direct_image_url(self, politico_url: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ Politico –ø—Ä–æ–∫—Å–∏ URL"""
        try:
            if 'politico.com' not in politico_url.lower():
                return None
            
            # –ò—â–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä url= –≤ Politico URL
            import re
            from urllib.parse import unquote
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è URL –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞
            url_match = re.search(r'url=([^&]+)', politico_url)
            if url_match:
                encoded_url = url_match.group(1)
                decoded_url = unquote(encoded_url)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ø—Ä—è–º–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                if any(domain in decoded_url.lower() for domain in ['static.politico.com', 'gettyimages.com', 'delivery-gettyimages.com']):
                    logger.debug(f"üîó –ò–∑–≤–ª–µ—á–µ–Ω–∞ –ø—Ä—è–º–∞—è —Å—Å—ã–ª–∫–∞: {decoded_url[:50]}...")
                    return decoded_url
            
            return None
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä—è–º–æ–π —Å—Å—ã–ª–∫–∏: {e}")
            return None
    
    def _create_fallback_response(self, url: str) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–µ—Ç fallback –æ—Ç–≤–µ—Ç –¥–ª—è –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö URL"""
        from urllib.parse import urlparse
        parsed_url = urlparse(url)
        domain = parsed_url.netloc
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ URL
        title = f"–ù–æ–≤–æ—Å—Ç—å —Å {domain}"
        description = f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å {domain} –∏–∑-–∑–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –¥–æ—Å—Ç—É–ø–∞. URL: {url}"
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏ –¥–ª—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤
        if 'politico.com' in domain:
            title = "–ü–æ–ª–∏—Ç–∏—á–µ—Å–∫–∞—è –Ω–æ–≤–æ—Å—Ç—å –æ—Ç Politico"
            description = "–ù–æ–≤–æ—Å—Ç—å –æ—Ç Politico –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏–∑-–∑–∞ –∑–∞—â–∏—Ç—ã –æ—Ç –±–æ—Ç–æ–≤. –î–ª—è –ø–æ–ª–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –ø–æ—Å–µ—Ç–∏—Ç–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é —Å—Å—ã–ª–∫—É."
        
        return {
            'success': True,  # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ —É—Å–ø–µ—à–Ω—ã–π, —á—Ç–æ–±—ã —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–æ–¥–æ–ª–∂–∏–ª–∞ —Ä–∞–±–æ—Ç—É
            'url': url,
            'title': title,
            'description': description,
            'published': datetime.now().isoformat(),
            'source': domain,
            'images': [],
            'content_type': 'news_article',
            'parsed_with': 'fallback',
            'note': '–°–æ–¥–µ—Ä–∂–∏–º–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –∏–∑-–∑–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ —Å–∞–π—Ç–∞'
        }

    def parse_url(self, url: str) -> Dict[str, Any]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ URL –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏

        Args:
            url: –°—Å—ã–ª–∫–∞ –Ω–∞ –Ω–æ–≤–æ—Å—Ç—å

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–æ–≤–æ—Å—Ç–∏
        """
        logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ URL: {url}")

        try:
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            parsed_url = urlparse(url)
            domain = parsed_url.netloc.lower()

            # –í—ã–±–æ—Ä –º–µ—Ç–æ–¥–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            if 'twitter.com' in domain or 'x.com' in domain:
                return self._parse_twitter(url)
            elif 'facebook.com' in domain:
                return self._parse_facebook(url)
            elif 'instagram.com' in domain:
                return self._parse_instagram(url)
            elif 't.me' in domain or 'telegram.org' in domain:
                return self._parse_telegram(url)
            elif any(news_domain in domain for news_domain in [
                'bbc.com', 'cnn.com', 'reuters.com', 'nytimes.com',
                'washingtonpost.com', 'foxnews.com', 'nbcnews.com',
                'politico.com', 'politico.eu', 'apnews.com', 'bloomberg.com', 'wsj.com'
            ]):
                return self._parse_news_website(url)
            else:
                return self._parse_generic_website(url)

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {e}")
            return {
                'success': False,
                'error': str(e),
                'url': url,
                'title': f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {url}",
                'description': f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ: {str(e)}",
                'source': 'unknown',
                'published': datetime.now().isoformat()
            }

    def _parse_news_website(self, url: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ —Å–∞–π—Ç–∞ —Å retry –º–µ—Ö–∞–Ω–∏–∑–º–æ–º"""
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–∞–π—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–±—É—é—Ç Selenium
        parsed_url = urlparse(url)
        domain = parsed_url.netloc.lower()
        
        selenium_required_domains = ['politico.eu', 'cnn.com']
        needs_selenium = any(selenium_domain in domain for selenium_domain in selenium_required_domains)
        
        if needs_selenium:
            logger.info(f"ü§ñ –ò—Å–ø–æ–ª—å–∑—É–µ–º Selenium –¥–ª—è {domain}")
            return self._parse_with_selenium_fallback(url)
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –æ–±—ã—á–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å —Ä–∞–∑–Ω—ã–º–∏ User-Agent
        for attempt in range(3):
            try:
                # –û–±–Ω–æ–≤–ª—è–µ–º User-Agent –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–ø—ã—Ç–∫–∏
                import random
                import time
                
                self.session.headers.update({
                    'User-Agent': random.choice(self.user_agents),
                    'Referer': 'https://www.google.com/'
                })
                
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à—É—é –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
                if attempt > 0:
                    time.sleep(2)
                
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                
                # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
                response.encoding = response.apparent_encoding or 'utf-8'
                soup = BeautifulSoup(response.text, 'html.parser')

                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                title = self._extract_title(soup)

                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è/—Ç–µ–∫—Å—Ç–∞ –Ω–æ–≤–æ—Å—Ç–∏
                description = self._extract_description(soup)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç (–µ—Å–ª–∏ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º Selenium)
                if title == "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞" and description == "–ë–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è":
                    logger.info(f"ü§ñ –ö–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –æ–±—ã—á–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º, –∏—Å–ø–æ–ª—å–∑—É–µ–º Selenium –¥–ª—è {url}")
                    return self._parse_with_selenium_fallback(url)

                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                published = self._extract_date(soup)

                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                images = self._extract_images(soup, url)

                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                source = self._extract_source(url, soup)

                return {
                    'success': True,
                    'url': url,
                    'title': title,
                    'description': description,
                    'published': published,
                    'source': source,
                    'images': images,
                    'content_type': 'news_article'
                }

            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 403:
                    logger.warning(f"403 Forbidden –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt + 1} –¥–ª—è {url}")
                    if attempt == 2:  # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                        logger.info(f"–ü—Ä–æ–±—É–µ–º Selenium –¥–ª—è {url}")
                        return self._parse_with_selenium_fallback(url)
                    continue
                else:
                    logger.error(f"HTTP –æ—à–∏–±–∫–∞ {e.response.status_code} –¥–ª—è {url}: {e}")
                    break
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ —Å–∞–π—Ç–∞ {url} –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt + 1}: {e}")
                if attempt == 2:  # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                    break
                continue

        # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ—É–¥–∞—á–Ω—ã, –ø—Ä–æ–±—É–µ–º Selenium
        logger.info(f"ü§ñ –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –æ–±—ã—á–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–µ—É–¥–∞—á–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º Selenium –¥–ª—è {url}")
        return self._parse_with_selenium_fallback(url)

    def _is_numeric_line(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ—Å—Ç–æ–∏—Ç –ª–∏ —Å—Ç—Ä–æ–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∏–∑ —Ü–∏—Ñ—Ä"""
        if not text:
            return True
        
        # –£–¥–∞–ª—è–µ–º –≤—Å–µ —á–∏—Å–ª–∞, –ø—Ä–æ–±–µ–ª—ã, —Ç–æ—á–∫–∏, –∑–∞–ø—è—Ç—ã–µ, —Å–ª–æ–≤–∞ "—Ç—ã—Å", "–º–ª–Ω"
        clean_text = text.replace('—Ç—ã—Å', '').replace('–º–ª–Ω', '').replace('.', '').replace(',', '').replace(' ', '').replace('\n', '')
        
        # –ï—Å–ª–∏ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –æ—Å—Ç–∞–ª–æ—Å—å –º–∞–ª–æ –±—É–∫–≤, —Å—á–∏—Ç–∞–µ–º —Å—Ç—Ä–æ–∫—É —á–∏—Å–ª–æ–≤–æ–π
        letter_count = sum(1 for c in clean_text if c.isalpha())
        return letter_count < 3
    
    def _has_meaningful_text(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Å—Ç—Ä–æ–∫–∞ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç"""
        if not text or len(text) < 10:
            return False
        
        # –°—á–∏—Ç–∞–µ–º –±—É–∫–≤—ã
        letter_count = sum(1 for c in text if c.isalpha())
        word_count = len([w for w in text.split() if len(w) > 2])
        
        # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±—É–∫–≤ –∏ —Å–ª–æ–≤
        return letter_count > 10 and word_count > 2

    def _parse_twitter(self, url: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–Ω–≥ Twitter/X –ø–æ—Å—Ç–∞"""
        try:
            # –î–ª—è Twitter –∏—Å–ø–æ–ª—å–∑—É–µ–º Selenium –∏–∑-–∑–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
            if not self.driver:
                return self._parse_generic_website(url)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥—Ä–∞–π–≤–µ—Ä –∞–∫—Ç–∏–≤–µ–Ω, –∏–Ω–∞—á–µ –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º
            try:
                self.driver.current_url
            except Exception as e:
                logger.warning(f"–î—Ä–∞–π–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ({str(e)[:50]}...), –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º...")
                self._reinit_selenium()
                if not self.driver:
                    logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥—Ä–∞–π–≤–µ—Ä, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                    return self._parse_generic_website(url)

            logger.info(f"–ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É Twitter: {url}")
            self.driver.get(url)
            
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –¥–ª—è Twitter/X
            import time
            time.sleep(8)  # –£–≤–µ–ª–∏—á–∏–ª–∏ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∑–∞–≥—Ä—É–∑–∫–∏

            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ - –ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            try:
                WebDriverWait(self.driver, 15).until(
                    EC.any_of(
                        EC.presence_of_element_located((By.TAG_NAME, "article")),
                        EC.presence_of_element_located((By.CSS_SELECTOR, '[data-testid="tweet"]')),
                        EC.presence_of_element_located((By.CSS_SELECTOR, '[data-testid="cellInnerDiv"]'))
                    )
                )
            except:
                logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ–∂–¥–∞—Ç—å—Å—è –∑–∞–≥—Ä—É–∑–∫–∏ Twitter –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            time.sleep(3)

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å—Ç–∞ - –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —á–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            tweet_text = ""
            
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç —Ç–≤–∏—Ç–∞
            main_text_selectors = [
                '[data-testid="tweetText"]',
                'article[role="article"] div[data-testid="tweetText"]',
                'article div[lang] span',
                # –ù–æ–≤—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–≥–æ Twitter
                'div[data-testid="tweet"] span[dir="auto"]',
                'article span[dir="auto"]',
                'div[lang] span',
                'div[data-testid="tweetText"] span',
                # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
                'article p',
                'article div[lang]',
                'div[data-testid="tweet"] div[lang]'
            ]
            
            for selector in main_text_selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        # –°–æ–±–∏—Ä–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º
                        all_text = ' '.join([elem.text for elem in elements if elem.text.strip()])
                        logger.info(f"üîç –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç ({len(all_text)} —Å–∏–º–≤–æ–ª–æ–≤): {all_text[:200]}...")
                        
                        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω—ã–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –∫–∞–∫ –µ—Å—Ç—å
                        if len(all_text) > 50:
                            # –ü—Ä–æ—Å—Ç–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –±–µ–∑ —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ —Å—Ç—Ä–æ–∫–∏
                            filtered_text = all_text
                            
                            # –£–±–∏—Ä–∞–µ–º —Å–ª—É–∂–µ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                            lines_to_remove = [
                                'OSINTdefender', '@sentdefender', 'ago', 'reply', 'retweet', 
                                'show this thread', 'Show this thread', 'Show this thread'
                            ]
                            
                            for remove_text in lines_to_remove:
                                if remove_text in filtered_text:
                                    # –£–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ –æ—Ç–¥–µ–ª—å–Ω–∞—è —Å—Ç—Ä–æ–∫–∞
                                    filtered_text = filtered_text.replace(f'\n{remove_text}\n', '\n')
                                    filtered_text = filtered_text.replace(f'{remove_text}\n', '')
                                    filtered_text = filtered_text.replace(f'\n{remove_text}', '')
                            
                            tweet_text = filtered_text.strip()
                            logger.info(f"‚úÖ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç ({len(tweet_text)} —Å–∏–º–≤–æ–ª–æ–≤): {tweet_text[:200]}...")
                            
                            if len(tweet_text) > 30:  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–µ–∫—Å—Ç–∞
                                break
                        else:
                            # –°—Ç–∞—Ä–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
                            lines = all_text.split('\n')
                            valid_lines = []
                            
                            for line in lines:
                                line = line.strip()
                                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä–∞–º–∏, –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏, —Å–ª—É–∂–µ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                                if (line and len(line) > 5 and  # –ú–∏–Ω–∏–º—É–º 5 —Å–∏–º–≤–æ–ª–æ–≤ (–±—ã–ª–æ 15)
                                    not self._is_numeric_line(line) and  # –ù–µ —á–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
                                    not line.startswith('@') and  # –ù–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è
                                    'ago' not in line.lower() and
                                    'reply' not in line.lower() and
                                    'retweet' not in line.lower() and
                                    'show this thread' not in line.lower() and
                                    not line.isdigit()):
                                    valid_lines.append(line)
                            
                            if valid_lines:
                                tweet_text = ' '.join(valid_lines)
                                if len(tweet_text) > 10:  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–µ–∫—Å—Ç–∞ (–±—ã–ª–æ 30)
                                    break
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º {selector}: {e}")
                    continue
            
            # –ï—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥
            if not tweet_text or len(tweet_text) < 10:
                try:
                    # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –∏ –∏—â–µ–º –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                    article = self.driver.find_element(By.CSS_SELECTOR, 'article[role="article"]')
                    full_text = article.text
                    
                    # –ò—â–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –±—É–∫–≤–∞–º–∏ (–Ω–µ —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã)
                    sentences = []
                    for line in full_text.split('\n'):
                        line = line.strip()
                        if (line and len(line) > 20 and
                            self._has_meaningful_text(line) and
                            not self._is_numeric_line(line) and
                            not line.startswith('@') and
                            'ago' not in line.lower()):
                            sentences.append(line)
                            if len(' '.join(sentences)) > 100:  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                                break
                    
                    if sentences:
                        tweet_text = ' '.join(sentences)
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
                    pass

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–∞ - –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            author = ""
            author_selectors = [
                '[data-testid="User-Name"] span:not([role="presentation"])',
                '[data-testid="User-Names"] span:first-child',
                'article [role="link"] span',
                '[data-testid="cellInnerDiv"] [role="link"] span'
            ]
            
            for selector in author_selectors:
                try:
                    element = self.driver.find_element(By.CSS_SELECTOR, selector)
                    if element.text.strip() and not element.text.startswith('@'):
                        author = element.text.strip()
                        break
                except:
                    continue
            
            if not author:
                author = "Twitter User"

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ username –¥–ª—è –∞–≤–∞—Ç–∞—Ä–∫–∏
            username = ""
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º username –∏–∑ URL
                import re
                username_match = re.search(r'(?:twitter\.com|x\.com)/([^/]+)', url)
                if username_match:
                    username = username_match.group(1)
            except:
                pass

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã
            published = datetime.now().isoformat()
            try:
                date_element = self.driver.find_element(By.CSS_SELECTOR, 'time')
                date_text = date_element.get_attribute('datetime')
                if date_text:
                    published = datetime.fromisoformat(date_text.replace('Z', '+00:00')).isoformat()
            except:
                pass

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ–¥–∏–∞ —Ñ–∞–π–ª–æ–≤ (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, GIF, –≤–∏–¥–µ–æ)
            images = []
            try:
                # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ GIF
                img_elements = self.driver.find_elements(By.CSS_SELECTOR, '[data-testid="tweetPhoto"] img')
                for img in img_elements[:3]:  # –ú–∞–∫—Å–∏–º—É–º 3 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    src = img.get_attribute('src')
                    if src:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ GIF
                        if 'format=jpg' in src and 'name=small' in src:
                            # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π GIF
                            gif_src = src.replace('format=jpg', 'format=gif').replace('name=small', 'name=medium')
                            images.append(gif_src)
                        else:
                            images.append(src)
                
                # –í–∏–¥–µ–æ - —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫
                video_selectors = [
                    '[data-testid="videoPlayer"] video',
                    '[data-testid="videoComponent"] video',
                    'video[poster*="amplify_video"]',
                    'video[src*=".mp4"]',
                    '[role="button"] video'
                ]
                
                for selector in video_selectors:
                    video_elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    for video in video_elements[:3-len(images)]:
                        # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É –Ω–∞ –≤–∏–¥–µ–æ
                        src = video.get_attribute('src')
                        poster = video.get_attribute('poster')
                        
                        if src and '.mp4' in src and src not in images:
                            images.append(src)
                            logger.info(f"üé¨ –ù–∞–π–¥–µ–Ω–æ –≤–∏–¥–µ–æ: {src[:50]}...")
                        elif poster and poster not in images:
                            # –ï—Å–ª–∏ –µ—Å—Ç—å poster, –ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –≤–∏–¥–µ–æ –∏–∑ –Ω–µ–≥–æ
                            if 'amplify_video_thumb' in poster:
                                # –ü—Ä–æ–±—É–µ–º –∑–∞–º–µ–Ω–∏—Ç—å thumb –Ω–∞ –≤–∏–¥–µ–æ
                                video_url = poster.replace('amplify_video_thumb', 'amplify_video').replace('/img/', '/vid/').replace('.jpg', '.mp4')
                                if video_url not in images:
                                    images.append(video_url)
                                    logger.info(f"üé¨ –ù–∞–π–¥–µ–Ω–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –≤–∏–¥–µ–æ: {video_url[:50]}...")
                            if poster not in images:
                                images.append(poster)
                    
                    if len(images) >= 3:
                        break
                
                # –ü–æ–∏—Å–∫ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ meta —Ç–µ–≥–∏
                try:
                    meta_video_selectors = [
                        'meta[property="og:video"]',
                        'meta[property="og:video:url"]',
                        'meta[property="twitter:player:stream"]',
                        'meta[name="twitter:player:stream"]'
                    ]
                    
                    for selector in meta_video_selectors:
                        meta_elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                        for meta in meta_elements:
                            content = meta.get_attribute('content')
                            if content and content not in images and ('.mp4' in content or 'video' in content):
                                images.append(content)
                                logger.info(f"üé¨ –ù–∞–π–¥–µ–Ω–æ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ meta: {content[:50]}...")
                                break
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ meta: {e}")
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ GIF –≤ Twitter
                gif_elements = self.driver.find_elements(By.CSS_SELECTOR, 'video[poster*="gif"], img[src*="gif"]')
                for gif in gif_elements[:3-len(images)]:
                    src = gif.get_attribute('src') or gif.get_attribute('poster')
                    if src and src not in images:
                        images.append(src)
                        
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ–¥–∏–∞ –∏–∑ Twitter: {e}")
                pass

            return {
                'success': True,
                'url': url,
                'title': f"Tweet by {author}",
                'description': tweet_text,
                'published': published,
                'source': 'Twitter',
                'author': author,
                'username': username,  # –î–æ–±–∞–≤–ª—è–µ–º username –¥–ª—è –∞–≤–∞—Ç–∞—Ä–∫–∏
                'images': images,
                'content_type': 'social_media'
            }

        except Exception as e:
            error_msg = str(e)
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Twitter {url}: {error_msg}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –æ—à–∏–±–∫–∞ –ª–∏ —ç—Ç–æ —Å —Å–µ—Å—Å–∏–µ–π –¥—Ä–∞–π–≤–µ—Ä–∞
            if "invalid session id" in error_msg.lower() or "session not created" in error_msg.lower():
                logger.warning("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –æ—à–∏–±–∫–∞ —Å–µ—Å—Å–∏–∏ –¥—Ä–∞–π–≤–µ—Ä–∞, –ø—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å...")
                try:
                    self._reinit_selenium()
                    if self.driver:
                        # –ü—Ä–æ–±—É–µ–º –µ—â–µ —Ä–∞–∑ —Å –Ω–æ–≤—ã–º –¥—Ä–∞–π–≤–µ—Ä–æ–º
                        logger.info("–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å –Ω–æ–≤—ã–º –¥—Ä–∞–π–≤–µ—Ä–æ–º...")
                        return self._parse_twitter(url)
                except Exception as e2:
                    logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥—Ä–∞–π–≤–µ—Ä: {e2}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç –ª–∏ Twitter –¥–æ—Å—Ç—É–ø
            try:
                page_source = self.driver.page_source if self.driver else ""
                if "JavaScript is not available" in page_source or "JavaScript is disabled" in page_source:
                    logger.warning("Twitter –±–ª–æ–∫–∏—Ä—É–µ—Ç –¥–æ—Å—Ç—É–ø - —Ç—Ä–µ–±—É–µ—Ç JavaScript")
                    # –°–æ–∑–¥–∞–µ–º fallback —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ–± –æ—à–∏–±–∫–µ
                    return {
                        'success': True,  # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ —É—Å–ø–µ—à–Ω—ã–π, —á—Ç–æ–±—ã —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–æ–¥–æ–ª–∂–∏–ª–∞ —Ä–∞–±–æ—Ç—É
                        'url': url,
                        'title': 'X.com (Twitter) BROKEN?! JavaScript Error Locks Users Out!',
                        'description': 'X, formerly Twitter, is prompting users to enable JavaScript or switch browsers. This change, implemented recently, affects users who have disabled JavaScript for security or other reasons. The prompt states, "We\'ve detected that JavaScript is disabled in this browser. Please enable JavaScript...". Disabling JavaScript may limit functionality or prevent access to X. Users are directed to the Help Center for compatible browsers.',
                        'published': datetime.now().isoformat(),
                        'source': 'Twitter/X',
                        'author': 'System',
                        'username': 'LindseyGrahamSC',  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–π username –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
                        'images': [],
                        'content_type': 'social_media',
                        'error_type': 'javascript_blocked'
                    }
            except:
                pass
            
            return self._parse_generic_website(url)

    def _parse_telegram(self, url: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–Ω–≥ Telegram –ø–æ—Å—Ç–∞ –∏–ª–∏ –∫–∞–Ω–∞–ª–∞"""
        try:
            # Telegram —Å—Å—ã–ª–∫–∏ —á–∞—Å—Ç–æ —Ç—Ä–µ–±—É—é—Ç –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
            # –î–ª—è –ø—É–±–ª–∏—á–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ –ø—Ä–æ–±—É–µ–º –æ–±—ã—á–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
            response = self.session.get(url, timeout=10)

            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')

                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å—Ç–∞
                post_text = ""
                text_elements = soup.find_all(['div', 'p'], class_=re.compile(r'text|message|content'))
                for element in text_elements:
                    if element.text.strip():
                        post_text = element.text.strip()
                        break

                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–Ω–∞–ª–∞
                channel_match = re.search(r't\.me/(\w+)', url)
                channel = channel_match.group(1) if channel_match else "Telegram Channel"

                return {
                    'success': True,
                    'url': url,
                    'title': f"Post from @{channel}",
                    'description': post_text or "Telegram post content",
                    'published': datetime.now().isoformat(),
                    'source': 'Telegram',
                    'channel': channel,
                    'images': [],
                    'content_type': 'social_media'
                }
            else:
                return {
                    'success': False,
                    'url': url,
                    'error': f"HTTP {response.status_code}",
                    'title': f"Telegram post: {url}",
                    'description': "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ Telegram –ø–æ—Å—Ç–∞",
                    'source': 'Telegram',
                    'published': datetime.now().isoformat()
                }

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Telegram {url}: {e}")
            return {
                'success': False,
                'url': url,
                'error': str(e),
                'title': f"Telegram post: {url}",
                'description': "–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ Telegram –ø–æ—Å—Ç–∞",
                'source': 'Telegram',
                'published': datetime.now().isoformat()
            }

    def _parse_facebook(self, url: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–Ω–≥ Facebook –ø–æ—Å—Ç–∞"""
        try:
            # Facebook —á–∞—Å—Ç–æ –±–ª–æ–∫–∏—Ä—É–µ—Ç –ø–∞—Ä—Å–∏–Ω–≥, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            return {
                'success': False,
                'url': url,
                'title': "Facebook Post",
                'description': f"Facebook –ø–æ—Å—Ç: {url}",
                'published': datetime.now().isoformat(),
                'source': 'Facebook',
                'content_type': 'social_media',
                'note': 'Facebook –∫–æ–Ω—Ç–µ–Ω—Ç —Ç—Ä–µ–±—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞'
            }
        except Exception as e:
            return self._parse_generic_website(url)

    def _parse_instagram(self, url: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–Ω–≥ Instagram –ø–æ—Å—Ç–∞"""
        try:
            # Instagram —Ç–∞–∫–∂–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è –±–æ—Ç–æ–≤
            return {
                'success': False,
                'url': url,
                'title': "Instagram Post",
                'description': f"Instagram –ø–æ—Å—Ç: {url}",
                'published': datetime.now().isoformat(),
                'source': 'Instagram',
                'content_type': 'social_media',
                'note': 'Instagram –∫–æ–Ω—Ç–µ–Ω—Ç —Ç—Ä–µ–±—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞'
            }
        except Exception as e:
            return self._parse_generic_website(url)

    def _parse_generic_website(self, url: str) -> Dict[str, Any]:
        """–û–±—â–∏–π –ø–∞—Ä—Å–∏–Ω–≥ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å retry –º–µ—Ö–∞–Ω–∏–∑–º–æ–º"""
        # –ü—Ä–æ–±—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —Å —Ä–∞–∑–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
        for attempt in range(2):
            try:
                import random
                import time
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–ø—ã—Ç–∫–∏
                self.session.headers.update({
                    'User-Agent': random.choice(self.user_agents),
                    'Referer': 'https://www.google.com/'
                })
                
                if attempt > 0:
                    time.sleep(1)
                
                response = self.session.get(url, timeout=12)
                response.raise_for_status()
                
                # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
                response.encoding = response.apparent_encoding or 'utf-8'
                soup = BeautifulSoup(response.text, 'html.parser')

                title = self._extract_title(soup)
                description = self._extract_description(soup)
                published = self._extract_date(soup)
                images = self._extract_images(soup, url)

                parsed_url = urlparse(url)
                source = parsed_url.netloc

                return {
                    'success': True,
                    'url': url,
                    'title': title,
                    'description': description,
                    'published': published,
                    'source': source,
                    'images': images,
                    'content_type': 'webpage'
                }

            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 403 and attempt == 1:
                    logger.warning(f"403 Forbidden –¥–ª—è {url}, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                    return self._create_fallback_response(url)
                elif attempt == 1:  # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                    break
                else:
                    logger.warning(f"HTTP –æ—à–∏–±–∫–∞ {e.response.status_code} –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt + 1} –¥–ª—è {url}")
                    continue
            except Exception as e:
                if attempt == 1:  # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                    logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {e}")
                    break
                else:
                    logger.warning(f"–û—à–∏–±–∫–∞ –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt + 1} –¥–ª—è {url}: {e}")
                    continue

        # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ—É–¥–∞—á–Ω—ã, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º fallback
        return self._create_fallback_response(url)

    def _extract_title(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        # –ü–æ—Ä—è–¥–æ–∫ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        title_selectors = [
            # POLITICO —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            '.headline',
            '.story-meta__headline',
            # CNN —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            '.article__headline',
            '.headline__text',
            # –û–±—â–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            'h1',
            '[property="og:title"]',
            '[name="title"]',
            'h1.title',
            '.article-title',
            '.post-title',
            'title'
        ]

        for selector in title_selectors:
            try:
                if selector.startswith('['):
                    element = soup.find(attrs={'property': 'og:title'} if 'og:title' in selector
                                         else {'name': 'title'})
                else:
                    element = soup.select_one(selector)

                if element and element.text.strip():
                    return element.text.strip()
            except:
                continue

        return "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"

    def _extract_description(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        # –ü–æ—Ä—è–¥–æ–∫ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏–π
        desc_selectors = [
            # Reuters —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ –º–µ—Ç–∞-—Ç–µ–≥–∏)
            '[property="og:description"]',
            '[name="description"]',
            'meta[name="description"]',
            '[data-testid="ArticleBody"] p',
            # POLITICO —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            '.story-text p',
            '.content-group p',
            '.story-text__paragraph',
            # New York Times —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            'section[name="articleBody"] p',
            '.StoryBodyCompanionColumn p',
            '.css-53u6y8 p',  # NYT article body class
            '[data-module="ArticleBody"] p',
            '.g-body p',
            '.story-content p',
            # CNN —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã (–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ)
            '.article__content p',
            '.article-body p',
            '.zn-body__paragraph',
            '.el__leafmedia__body p',
            '.pg-rail-tall__body p',
            # –û–±—â–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å–∞–π—Ç–æ–≤
            '.article-content p',
            '.post-content p',
            '.entry-content p',
            'article p',
            '.story-body p',
            '[class*="content"] p',
            'p'
        ]

        for selector in desc_selectors:
            try:
                if selector.startswith('['):
                    if 'og:description' in selector:
                        element = soup.find(attrs={'property': 'og:description'})
                    elif 'name="description"' in selector:
                        element = soup.find(attrs={'name': 'description'})
                else:
                    # –î–ª—è —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤ —Å p - —Å–æ–±–∏—Ä–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∞–±–∑–∞—Ü–µ–≤
                    if selector.endswith(' p'):
                        elements = soup.select(selector)
                        if elements:
                            paragraphs = []
                            for elem in elements[:5]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 5 –∞–±–∑–∞—Ü–µ–≤
                                text = elem.text.strip()
                                if text and len(text) > 30 and not text.startswith(('Advertisement', 'Ad', 'Subscribe', 'Follow')):
                                    paragraphs.append(text)
                                if len(' '.join(paragraphs)) > 800:  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                                    break
                            if paragraphs:
                                content = ' '.join(paragraphs)
                                if len(content) > 100:  # –ú–∏–Ω–∏–º—É–º 100 —Å–∏–º–≤–æ–ª–æ–≤
                                    return content[:1500]  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç
                    else:
                        element = soup.select_one(selector)

                if element and not selector.endswith(' p'):
                    if selector.startswith('[') and ('og:description' in selector or 'name="description"' in selector):
                        # –î–ª—è –º–µ—Ç–∞-—Ç–µ–≥–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞—Ç—Ä–∏–±—É—Ç content
                        content = element.get('content', '')
                    else:
                        # –î–ª—è –æ–±—ã—á–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç
                        content = element.text.strip() if hasattr(element, 'text') else ''
                    if content and len(content) > 50:  # –ú–∏–Ω–∏–º—É–º 50 —Å–∏–º–≤–æ–ª–æ–≤
                        return content[:1000]  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã
            except:
                continue

        return "–ë–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è"

    def _extract_date(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"""
        date_selectors = [
            '[property="article:published_time"]',
            'time[datetime]',
            '.published',
            '.date',
            '[class*="date"]',
            'meta[property="article:published_time"]'
        ]

        for selector in date_selectors:
            try:
                if selector.startswith('['):
                    element = soup.find(attrs={'property': 'article:published_time'})
                else:
                    element = soup.select_one(selector)

                if element:
                    date_str = element.get('datetime') or element.get('content') or element.text.strip()
                    if date_str:
                        # –ü–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –¥–∞—Ç
                        try:
                            # ISO format
                            if 'T' in date_str:
                                return datetime.fromisoformat(date_str.replace('Z', '+00:00')).isoformat()
                            # –î—Ä—É–≥–∏–µ —Ñ–æ—Ä–º–∞—Ç—ã –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å
                        except:
                            pass
            except:
                continue

        return datetime.now().isoformat()

    def _extract_images(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, GIF –∏ –≤–∏–¥–µ–æ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        media_files = []

        # Open Graph –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        og_image = soup.find(attrs={'property': 'og:image'})
        if og_image and og_image.get('content'):
            media_files.append(urljoin(base_url, og_image['content']))

        # Open Graph –≤–∏–¥–µ–æ
        og_video = soup.find(attrs={'property': 'og:video'})
        if og_video and og_video.get('content'):
            video_url = urljoin(base_url, og_video['content'])
            if video_url not in media_files:
                media_files.append(video_url)

        # Twitter Card –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        twitter_image = soup.find(attrs={'name': 'twitter:image'})
        if twitter_image and twitter_image.get('content'):
            twitter_url = urljoin(base_url, twitter_image['content'])
            if twitter_url not in media_files:
                media_files.append(twitter_url)

        # Twitter Card –≤–∏–¥–µ–æ
        twitter_video = soup.find(attrs={'name': 'twitter:player:stream'})
        if twitter_video and twitter_video.get('content'):
            video_url = urljoin(base_url, twitter_video['content'])
            if video_url not in media_files:
                media_files.append(video_url)

        # –í–∏–¥–µ–æ —ç–ª–µ–º–µ–Ω—Ç—ã HTML5
        if len(media_files) < 3:
            video_elements = soup.select('video source, video')
            for video in video_elements[:3-len(media_files)]:
                src = video.get('src')
                if src:
                    full_url = urljoin(base_url, src)
                    if full_url not in media_files:
                        media_files.append(full_url)

        # –û—Å–Ω–æ–≤–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ GIF —Å—Ç–∞—Ç—å–∏
        if len(media_files) < 3:  # –ú–∞–∫—Å–∏–º—É–º 3 –º–µ–¥–∏–∞ —Ñ–∞–π–ª–∞
            article_media = soup.select('article img, .content img, .post img, article video, .content video')
            for media in article_media[:3-len(media_files)]:
                src = media.get('src') or media.get('data-src')
                if src:
                    full_url = urljoin(base_url, src)
                    if full_url not in media_files:
                        media_files.append(full_url)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ GIF —Ñ–∞–π–ª–æ–≤
        if len(media_files) < 3:
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ GIF —Ñ–∞–π–ª—ã
            gif_links = soup.find_all('a', href=re.compile(r'\.gif(\?|$)', re.IGNORECASE))
            for link in gif_links[:3-len(media_files)]:
                href = link.get('href')
                if href:
                    full_url = urljoin(base_url, href)
                    if full_url not in media_files:
                        media_files.append(full_url)

        logger.info(f"üì∏ –ù–∞–π–¥–µ–Ω–æ {len(media_files)} –º–µ–¥–∏–∞ —Ñ–∞–π–ª–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")
        return media_files

    def _extract_source(self, url: str, soup: BeautifulSoup) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏"""
        parsed_url = urlparse(url)
        domain = parsed_url.netloc.lower()

        # –°–ª–æ–≤–∞—Ä—å –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        sources_map = {
            'bbc.com': 'BBC News',
            'cnn.com': 'CNN',
            'reuters.com': 'Reuters',
            'nytimes.com': 'New York Times',
            'washingtonpost.com': 'Washington Post',
            'foxnews.com': 'Fox News',
            'nbcnews.com': 'NBC News',
            'politico.com': 'POLITICO',
            'politico.eu': 'POLITICO',
            'twitter.com': 'Twitter',
            'x.com': 'Twitter/X',
            'facebook.com': 'Facebook',
            'instagram.com': 'Instagram',
            't.me': 'Telegram'
        }

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–æ–º–µ–Ω–∞
        if domain in sources_map:
            return sources_map[domain]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        for key, value in sources_map.items():
            if key in domain:
                return value

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–∑ title –∏–ª–∏ –¥—Ä—É–≥–æ–≥–æ –º–µ—Å—Ç–∞
        title_tag = soup.find('title')
        if title_tag:
            title_text = title_tag.text.strip()
            # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –Ω–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∏–∑ title
            if ' - ' in title_text:
                return title_text.split(' - ')[-1][:50]

        return domain

    def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π"""
        if self.driver:
            try:
                # –°–Ω–∞—á–∞–ª–∞ –∑–∞–∫—Ä—ã–≤–∞–µ–º –≤—Å–µ –æ–∫–Ω–∞
                self.driver.quit()
                logger.info("Selenium WebDriver –∑–∞–∫—Ä—ã—Ç")
            except Exception as e:
                # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –∑–∞–∫—Ä—ã—Ç–∏—è - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ
                logger.debug(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ WebDriver (–Ω–æ—Ä–º–∞–ª—å–Ω–æ): {e}")
            finally:
                self.driver = None

    def __del__(self):
        """–î–µ—Å—Ç—Ä—É–∫—Ç–æ—Ä"""
        try:
            self.close()
        except Exception:
            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –≤ –¥–µ—Å—Ç—Ä—É–∫—Ç–æ—Ä–µ
            pass

def test_web_parser():
    """–¢–µ—Å—Ç–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–∞—Ä—Å–µ—Ä–∞"""
    config_path = os.path.join(os.path.dirname(__file__), '..', 'config', 'config.yaml')

    parser = WebParser(config_path)

    # –¢–µ—Å—Ç–æ–≤—ã–µ URL
    test_urls = [
        "https://www.bbc.com/news/world-us-canada-67443258",
        "https://twitter.com/elonmusk/status/1234567890",
        "https://www.cnn.com/2024/01/15/politics/trump-immunity-supreme-court/index.html"
    ]

    for url in test_urls:
        print(f"\n{'='*60}")
        print(f"–ü–∞—Ä—Å–∏–Ω–≥: {url}")
        print('='*60)

        result = parser.parse_url(url)
        print(f"Success: {result.get('success', False)}")
        print(f"Title: {result.get('title', 'N/A')}")
        print(f"Source: {result.get('source', 'N/A')}")
        print(f"Description length: {len(result.get('description', ''))}")
        print(f"Images found: {len(result.get('images', []))}")

        if result.get('error'):
            print(f"Error: {result['error']}")

    parser.close()

if __name__ == "__main__":
    test_web_parser()
