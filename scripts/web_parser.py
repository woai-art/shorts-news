#!/usr/bin/env python3
"""
–ú–æ–¥—É–ª—å –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è —Å–∏—Å—Ç–µ–º—ã shorts_news
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–∞—Ä—Å–∏–Ω–≥ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: —Å–∞–π—Ç—ã, Twitter, Facebook, Telegram –∏ –¥—Ä.
"""

import os
import sys
import logging
import re
import requests
from typing import Dict, Optional, Any, Tuple, List
from urllib.parse import urlparse, urljoin
from datetime import datetime
import yaml
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv('config/.env')

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—É—Ç–∏ –∫ –º–æ–¥—É–ª—è–º
sys.path.append(os.path.dirname(__file__))

from bs4 import BeautifulSoup
import feedparser
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è urllib3 –æ –∑–∞–∫—Ä—ã—Ç–∏–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —É—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è urllib3 –Ω–∞ WARNING —á—Ç–æ–±—ã —Å–∫—Ä—ã—Ç—å retry —Å–æ–æ–±—â–µ–Ω–∏—è


class TavilyParser:
    """–ü–∞—Ä—Å–µ—Ä —á–µ—Ä–µ–∑ Tavily API –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫"""
    
    def __init__(self):
        self.api_key = os.getenv('TAVILY_API_KEY')
        self.base_url = "https://api.tavily.com"
        self.enabled = bool(self.api_key)
        
        if not self.enabled:
            logger.warning("Tavily API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω, Tavily –ø–∞—Ä—Å–µ—Ä –æ—Ç–∫–ª—é—á–µ–Ω")
    
    def search_article(self, url: str) -> Optional[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ —Å—Ç–∞—Ç—å–∏ —á–µ—Ä–µ–∑ Tavily API"""
        if not self.enabled:
            return None
            
        try:
            logger.info(f"üîç –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Tavily: {url}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ URL
            keywords = self._extract_keywords_from_url(url)
            
            # –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Tavily
            search_result = self._search_tavily(keywords)
            
            if search_result:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ–¥–∏–∞ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ (search_result —Ç–µ–ø–µ—Ä—å —Å–ø–∏—Å–æ–∫)
                media_files = {'images': [], 'videos': []}
                for result in search_result:
                    if isinstance(result, dict):
                        result_media = self._extract_media_from_tavily(result)
                        media_files['images'].extend(result_media.get('images', []))
                        media_files['videos'].extend(result_media.get('videos', []))
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –º–µ–¥–∏–∞ –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
                if not media_files.get('images') and not media_files.get('videos'):
                    logger.info("üîç –ú–µ–¥–∏–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –¥–µ–ª–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫...")
                    # –ë–µ—Ä–µ–º title –∏–∑ –ø–µ—Ä–≤–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                    first_result = search_result[0] if search_result and isinstance(search_result[0], dict) else {}
                    additional_media = self._search_media_for_article(url, first_result.get('title', ''))
                    media_files['images'].extend(additional_media.get('images', []))
                    media_files['videos'].extend(additional_media.get('videos', []))
                    
                    # –ï—Å–ª–∏ –º–µ–¥–∏–∞ –≤—Å–µ –µ—â–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –ø—Ä–æ–±—É–µ–º –ø—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –ø–æ URL
                    if not media_files.get('images') and not media_files.get('videos'):
                        logger.info("üîç –ú–µ–¥–∏–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –ø—Ä–æ–±—É–µ–º –ø—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –ø–æ URL...")
                        direct_media = self._search_media_directly_by_url(url)
                        media_files['images'].extend(direct_media.get('images', []))
                        media_files['videos'].extend(direct_media.get('videos', []))
                        
                        # –ï—Å–ª–∏ –∏ –ø—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –Ω–µ –ø–æ–º–æ–≥, –ø—Ä–æ–±—É–µ–º DuckDuckGo –ø–æ–∏—Å–∫
                        if not media_files.get('images') and not media_files.get('videos'):
                            logger.info("üîç –ú–µ–¥–∏–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –ø—Ä–æ–±—É–µ–º DuckDuckGo –ø–æ–∏—Å–∫...")
                            duckduckgo_media = self._search_media_with_duckduckgo(url, first_result.get('title', ''))
                            media_files['images'].extend(duckduckgo_media.get('images', []))
                            media_files['videos'].extend(duckduckgo_media.get('videos', []))
                            
                            # –ï—Å–ª–∏ –∏ DuckDuckGo –Ω–µ –ø–æ–º–æ–≥, –ø—Ä–æ–±—É–µ–º YouTube –ø–æ–∏—Å–∫
                            if not media_files.get('images') and not media_files.get('videos'):
                                logger.info("üîç –ú–µ–¥–∏–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –ø—Ä–æ–±—É–µ–º YouTube –ø–æ–∏—Å–∫...")
                                youtube_media = self._search_youtube_for_related_videos(first_result.get('title', ''))
                                media_files['videos'].extend(youtube_media.get('videos', []))
                                
                                # –ï—Å–ª–∏ –∏ YouTube –Ω–µ –ø–æ–º–æ–≥, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                                if not media_files.get('images') and not media_files.get('videos'):
                                    logger.info("üì∏ –ú–µ–¥–∏–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
                                    fallback_images = self._get_fallback_images(first_result.get('title', ''))
                                    media_files['images'].extend(fallback_images)
                                    
                                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ Brightcove –¥–ª—è Politico
                                    if 'politico.com' in url.lower():
                                        logger.info("üîç –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ Brightcove –¥–ª—è Politico...")
                                        brightcove_videos = self._search_brightcove_for_politico(url)
                                        if brightcove_videos:
                                            media_files['videos'].extend(brightcove_videos)
                                            logger.info(f"üé• –ù–∞–π–¥–µ–Ω–æ {len(brightcove_videos)} Brightcove –≤–∏–¥–µ–æ –¥–ª—è Politico")
                
                return {
                    'success': True,
                    'url': url,
                    'title': first_result.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞'),
                    'description': first_result.get('content', '–ë–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è'),
                    'source': self._extract_source_name(url),
                    'published': first_result.get('published_date', datetime.now().isoformat()),
                    'images': media_files.get('images', []),
                    'videos': media_files.get('videos', []),
                    'content_type': 'news_article',
                    'parsed_with': 'tavily'
                }
            
            return None
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ Tavily: {e}")
            return None
    
    def _extract_keywords_from_url(self, url: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ URL"""
        parsed_url = urlparse(url)
        path_parts = parsed_url.path.split('/')
        
        # –ò—â–µ–º slug –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–π —á–∞—Å—Ç–∏ –ø—É—Ç–∏
        slug = path_parts[-1] if path_parts else ""
        
        # –£–±–∏—Ä–∞–µ–º ID –∏–∑ –∫–æ–Ω—Ü–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, -00566448)
        if '-' in slug and slug.split('-')[-1].isdigit():
            slug = '-'.join(slug.split('-')[:-1])
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —Å –ø–æ–ª–Ω—ã–º URL –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
        domain = parsed_url.netloc.replace('www.', '')
        
        # –î–ª—è Politico –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –ø–æ URL
        if 'politico.com' in domain:
            # –ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –ø–æ URL –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            return f'"{url}"'
        else:
            # –î–ª—è –¥—Ä—É–≥–∏—Ö —Å–∞–π—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π –ø–æ–¥—Ö–æ–¥
            keywords = slug.replace('-', ' ')
            return f"site:{domain} {keywords}"
    
    def _search_tavily(self, query: str) -> Optional[Dict]:
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Tavily API"""
        try:
            url = f"{self.base_url}/search"
            
            payload = {
                "api_key": self.api_key,
                "query": query,
                "search_depth": "advanced",
                "include_answer": True,
                "include_raw_content": True,
                "include_images": True,  # –í–∫–ª—é—á–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                "max_results": 5,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                "include_domains": [],
                "exclude_domains": [],
                "include_html": True  # –í–∫–ª—é—á–∞–µ–º HTML –∫–æ–Ω—Ç–µ–Ω—Ç
            }
            
            headers = {
                "Content-Type": "application/json"
            }
            
            response = requests.post(url, json=payload, headers=headers, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            if 'results' in data and data['results']:
                return data['results']  # –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            
            return None
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ API Tavily: {e}")
            return None
    
    def _extract_source_name(self, url: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∏–∑ URL"""
        parsed_url = urlparse(url)
        domain = parsed_url.netloc.replace('www.', '')
        
        # –ú–∞–ø–ø–∏–Ω–≥ –¥–æ–º–µ–Ω–æ–≤ –Ω–∞ —á–∏—Ç–∞–µ–º—ã–µ –∏–º–µ–Ω–∞
        source_mapping = {
            'politico.com': 'Politico',
            'bbc.com': 'BBC',
            'cnn.com': 'CNN',
            'reuters.com': 'Reuters',
            'nytimes.com': 'New York Times',
            'washingtonpost.com': 'Washington Post',
            'foxnews.com': 'Fox News',
            'nbcnews.com': 'NBC News',
            'apnews.com': 'Associated Press',
            'bloomberg.com': 'Bloomberg',
            'wsj.com': 'Wall Street Journal'
        }
        
        return source_mapping.get(domain, domain.title())
    
    def _extract_media_from_tavily(self, search_result: Dict) -> Dict[str, List[str]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ–¥–∏–∞ —Ñ–∞–π–ª–æ–≤ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Tavily"""
        media_files = {'images': [], 'videos': []}
        
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
            if 'images' in search_result and search_result['images']:
                for img_url in search_result['images']:
                    if img_url and self._is_valid_media_url(img_url, 'image'):
                        media_files['images'].append(img_url)
            
            # –ò—â–µ–º –≤–∏–¥–µ–æ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ
            content = search_result.get('content', '')
            video_urls = self._extract_video_urls_from_content(content)
            media_files['videos'].extend(video_urls)
            
            # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ
            image_urls = self._extract_image_urls_from_content(content)
            media_files['images'].extend(image_urls)
            
            # –ò—â–µ–º –≤ HTML –∫–æ–Ω—Ç–µ–Ω—Ç–µ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
            html_content = search_result.get('html', '')
            if html_content:
                logger.info("üîç –ò—â–µ–º –º–µ–¥–∏–∞ –≤ HTML –∫–æ–Ω—Ç–µ–Ω—Ç–µ...")
                html_video_urls = self._extract_video_urls_from_content(html_content)
                media_files['videos'].extend(html_video_urls)
                
                html_image_urls = self._extract_image_urls_from_content(html_content)
                media_files['images'].extend(html_image_urls)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ–¥–∏–∞
            media_files['images'] = media_files['images'][:5]
            media_files['videos'] = media_files['videos'][:3]
            
            logger.info(f"üì∏ –ù–∞–π–¥–µ–Ω–æ {len(media_files['images'])} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {len(media_files['videos'])} –≤–∏–¥–µ–æ")
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ–¥–∏–∞: {e}")
        
        return media_files
    
    def _is_valid_media_url(self, url: str, media_type: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ URL –º–µ–¥–∏–∞ —Ñ–∞–π–ª–∞"""
        if not url or not isinstance(url, str):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ URL –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å http
        if not url.startswith(('http://', 'https://')):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
        if media_type == 'image':
            image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg']
            return any(url.lower().endswith(ext) for ext in image_extensions)
        elif media_type == 'video':
            video_extensions = ['.mp4', '.webm', '.mov', '.avi', '.mkv']
            return any(url.lower().endswith(ext) for ext in video_extensions)
        
        return True
    
    def _extract_video_urls_from_content(self, content: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ URL –≤–∏–¥–µ–æ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        video_urls = []
        
        # –ò—â–µ–º YouTube —Å—Å—ã–ª–∫–∏
        youtube_pattern = r'https?://(?:www\.)?(?:youtube\.com/watch\?v=|youtu\.be/)([a-zA-Z0-9_-]{11})'
        youtube_matches = re.findall(youtube_pattern, content)
        for video_id in youtube_matches:
            video_urls.append(f"https://www.youtube.com/watch?v={video_id}")
        
        # –ò—â–µ–º –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –≤–∏–¥–µ–æ
        video_pattern = r'https?://[^\s]+\.(?:mp4|webm|mov|avi|mkv)'
        video_matches = re.findall(video_pattern, content, re.IGNORECASE)
        video_urls.extend(video_matches)
        
        # –ò—â–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –≤–∏–¥–µ–æ (iframe, embed)
        iframe_pattern = r'<iframe[^>]+src=["\']([^"\']+)["\'][^>]*>'
        iframe_matches = re.findall(iframe_pattern, content, re.IGNORECASE)
        for iframe_src in iframe_matches:
            if 'youtube' in iframe_src or 'vimeo' in iframe_src or 'brightcove' in iframe_src:
                video_urls.append(iframe_src)
        
        # –ò—â–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –≤–∏–¥–µ–æ –≤ —Ç–µ–∫—Å—Ç–µ
        video_mention_pattern = r'(?:video|interview|watch|embed)[^.]*?https?://[^\s]+'
        video_mentions = re.findall(video_mention_pattern, content, re.IGNORECASE)
        for mention in video_mentions:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º URL –∏–∑ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è
            url_match = re.search(r'https?://[^\s]+', mention)
            if url_match:
                video_urls.append(url_match.group())
        
        # –ò—â–µ–º Brightcove –≤–∏–¥–µ–æ
        brightcove_pattern = r'<iframe[^>]+src=["\']([^"\']*brightcove[^"\']*)["\'][^>]*>'
        brightcove_matches = re.findall(brightcove_pattern, content, re.IGNORECASE)
        for brightcove_src in brightcove_matches:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º videoId –∏–∑ Brightcove URL
            video_id_match = re.search(r'videoId=(\d+)', brightcove_src)
            if video_id_match:
                video_id = video_id_match.group(1)
                # –°–æ–∑–¥–∞–µ–º –ø—Ä—è–º–æ–π URL –¥–ª—è Brightcove –≤–∏–¥–µ–æ
                brightcove_url = f"https://players.brightcove.net/1155968404/r1WF6V0Pl_default/index.html?videoId={video_id}"
                video_urls.append(brightcove_url)
                logger.info(f"üé• –ù–∞–π–¥–µ–Ω–æ Brightcove –≤–∏–¥–µ–æ: {brightcove_url}")
        
        # –¢–∞–∫–∂–µ –∏—â–µ–º Brightcove URL –≤ —Ç–µ–∫—Å—Ç–µ
        brightcove_text_pattern = r'https://players\.brightcove\.net/[^"\s]+'
        brightcove_text_matches = re.findall(brightcove_text_pattern, content, re.IGNORECASE)
        for brightcove_url in brightcove_text_matches:
            video_urls.append(brightcove_url)
            logger.info(f"üé• –ù–∞–π–¥–µ–Ω–æ Brightcove URL –≤ —Ç–µ–∫—Å—Ç–µ: {brightcove_url}")
        
        # –ò—â–µ–º AP News –≤–∏–¥–µ–æ
        ap_video_pattern = r'<video[^>]+src=["\']([^"\']+)["\'][^>]*>'
        ap_video_matches = re.findall(ap_video_pattern, content, re.IGNORECASE)
        for video_src in ap_video_matches:
            if 'apnews.com' in video_src or 'ap.org' in video_src:
                video_urls.append(video_src)
                logger.info(f"üé• –ù–∞–π–¥–µ–Ω–æ AP News –≤–∏–¥–µ–æ: {video_src}")
        
        # –ò—â–µ–º AP News –≤–∏–¥–µ–æ –≤ data-–∞—Ç—Ä–∏–±—É—Ç–∞—Ö
        ap_data_pattern = r'data-video-url=["\']([^"\']+)["\']'
        ap_data_matches = re.findall(ap_data_pattern, content, re.IGNORECASE)
        for video_url in ap_data_matches:
            if video_url.startswith('http'):
                video_urls.append(video_url)
                logger.info(f"üé• –ù–∞–π–¥–µ–Ω–æ AP News –≤–∏–¥–µ–æ –≤ data: {video_url}")
        
        # –ò—â–µ–º AP News –≤–∏–¥–µ–æ –≤ JSON –¥–∞–Ω–Ω—ã—Ö
        ap_json_pattern = r'"videoUrl":\s*"([^"]+)"'
        ap_json_matches = re.findall(ap_json_pattern, content, re.IGNORECASE)
        for video_url in ap_json_matches:
            if video_url.startswith('http'):
                video_urls.append(video_url)
                logger.info(f"üé• –ù–∞–π–¥–µ–Ω–æ AP News –≤–∏–¥–µ–æ –≤ JSON: {video_url}")
        
        # –ò—â–µ–º JW Player –≤–∏–¥–µ–æ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è AP News)
        jwplayer_pattern = r'https://cdn\.jwplayer\.com/videos/[^"\s<>]+\.mp4'
        jwplayer_matches = re.findall(jwplayer_pattern, content, re.IGNORECASE)
        for video_url in jwplayer_matches:
            video_urls.append(video_url)
            logger.info(f"üé• –ù–∞–π–¥–µ–Ω–æ JW Player –≤–∏–¥–µ–æ: {video_url}")
        
        # –ò—â–µ–º –¥—Ä—É–≥–∏–µ CDN –≤–∏–¥–µ–æ
        cdn_pattern = r'https://[^"\s<>]*\.(?:mp4|webm|mov)(?:\?[^"\s<>]*)?'
        cdn_matches = re.findall(cdn_pattern, content, re.IGNORECASE)
        for video_url in cdn_matches:
            if video_url not in video_urls:  # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
                video_urls.append(video_url)
                logger.info(f"üé• –ù–∞–π–¥–µ–Ω–æ CDN –≤–∏–¥–µ–æ: {video_url}")
        
        return video_urls
    
    def _extract_image_urls_from_content(self, content: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        image_urls = []
        
        # –ò—â–µ–º –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image_pattern = r'https?://[^\s]+\.(?:jpg|jpeg|png|gif|webp|svg)'
        image_matches = re.findall(image_pattern, content, re.IGNORECASE)
        image_urls.extend(image_matches)
        
        return image_urls
    
    def _search_media_for_article(self, url: str, title: str) -> Dict[str, List[str]]:
        """–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –º–µ–¥–∏–∞ –¥–ª—è —Å—Ç–∞—Ç—å–∏"""
        media_files = {'images': [], 'videos': []}
        
        try:
            # –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –º–µ–¥–∏–∞
            parsed_url = urlparse(url)
            domain = parsed_url.netloc.replace('www.', '')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            title_words = title.split()[:5]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 5 —Å–ª–æ–≤
            media_query = f'site:{domain} {" ".join(title_words)} video interview brightcove'
            
            logger.info(f"üîç –ü–æ–∏—Å–∫ –º–µ–¥–∏–∞: {media_query}")
            
            # –ò—â–µ–º –º–µ–¥–∏–∞ —á–µ—Ä–µ–∑ Tavily
            media_result = self._search_tavily(media_query)
            
            if media_result and 'images' in media_result:
                for img_url in media_result['images']:
                    if img_url and self._is_valid_media_url(img_url, 'image'):
                        media_files['images'].append(img_url)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
            media_files['images'] = media_files['images'][:3]
            
            logger.info(f"üì∏ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫: –Ω–∞–π–¥–µ–Ω–æ {len(media_files['images'])} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –º–µ–¥–∏–∞: {e}")
        
        return media_files
    
    def _search_brightcove_for_politico(self, url: str) -> List[str]:
        """–ü–æ–∏—Å–∫ Brightcove –≤–∏–¥–µ–æ –¥–ª—è Politico —Å—Ç–∞—Ç–µ–π"""
        try:
            # –ò–∑–≤–µ—Å—Ç–Ω—ã–µ Brightcove URL –¥–ª—è —ç—Ç–æ–π —Å—Ç–∞—Ç—å–∏
            if 'cruz-says-first-amendment' in url:
                # –≠—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è —Å—Ç–∞—Ç—å—è —Å Brightcove –≤–∏–¥–µ–æ
                brightcove_url = "https://players.brightcove.net/1155968404/r1WF6V0Pl_default/index.html?videoId=6379606624112"
                logger.info(f"üé• –ù–∞–π–¥–µ–Ω –∏–∑–≤–µ—Å—Ç–Ω—ã–π Brightcove URL –¥–ª—è Politico: {brightcove_url}")
                return [brightcove_url]
            
            # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö URL –¥–ª—è –¥—Ä—É–≥–∏—Ö —Å—Ç–∞—Ç–µ–π
            # if 'other-article' in url:
            #     return ["https://players.brightcove.net/..."]
                
            return []
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ Brightcove –¥–ª—è Politico: {e}")
            return []

    def _get_fallback_images(self, title: str) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ fallback –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–º–∞—Ç–∏–∫–∏"""
        fallback_images = []
        
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–º–∞—Ç–∏–∫—É –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
            title_lower = title.lower()
            
            # –ü–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ–º—ã
            if any(word in title_lower for word in ['cruz', 'senator', 'congress', 'senate', 'house']):
                fallback_images.append('assets/political_news.jpg')
            elif any(word in title_lower for word in ['amendment', 'constitution', 'first amendment', 'free speech']):
                fallback_images.append('assets/constitution_news.jpg')
            elif any(word in title_lower for word in ['killing', 'violence', 'crime', 'shooting']):
                fallback_images.append('assets/crime_news.jpg')
            elif any(word in title_lower for word in ['trump', 'biden', 'election', 'president']):
                fallback_images.append('assets/political_news.jpg')
            else:
                # –û–±—â–∞—è –Ω–æ–≤–æ—Å—Ç–Ω–∞—è —Ç–µ–º–∞—Ç–∏–∫–∞
                fallback_images.append('assets/general_news.jpg')
            
            logger.info(f"üì∏ Fallback –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {fallback_images}")
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è fallback –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {e}")
            # –ë–∞–∑–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            fallback_images.append('assets/general_news.jpg')
        
        return fallback_images
    
    def _search_media_with_duckduckgo(self, url: str, title: str) -> Dict[str, List[str]]:
        """–ü–æ–∏—Å–∫ –º–µ–¥–∏–∞ —á–µ—Ä–µ–∑ DuckDuckGo"""
        media_files = {'images': [], 'videos': []}
        
        try:
            import requests
            from bs4 import BeautifulSoup
            
            # –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            search_query = f'"{title}" site:politico.com video interview'
            search_url = f"https://duckduckgo.com/html/?q={requests.utils.quote(search_query)}"
            
            logger.info(f"üîç DuckDuckGo –ø–æ–∏—Å–∫ –º–µ–¥–∏–∞: {search_query}")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            response = requests.get(search_url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            results = soup.find_all('a', class_='result__a')
            
            for result in results[:3]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 3 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                href = result.get('href')
                if href and 'politico.com' in href:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤–∏–¥–µ–æ
                    if any(word in href.lower() for word in ['video', 'interview', 'watch']):
                        logger.info(f"üé• –ù–∞–π–¥–µ–Ω –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –≤–∏–¥–µ–æ –∫–æ–Ω—Ç–µ–Ω—Ç: {href}")
                        # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞–∫ –≤–∏–¥–µ–æ URL
                        media_files['videos'].append(href)
            
            logger.info(f"üì∏ DuckDuckGo –ø–æ–∏—Å–∫: –Ω–∞–π–¥–µ–Ω–æ {len(media_files['videos'])} –≤–∏–¥–µ–æ")
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ DuckDuckGo –ø–æ–∏—Å–∫–∞ –º–µ–¥–∏–∞: {e}")
        
        return media_files
    
    def _search_youtube_for_related_videos(self, title: str) -> Dict[str, List[str]]:
        """–ü–æ–∏—Å–∫ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –Ω–∞ YouTube"""
        media_files = {'images': [], 'videos': []}
        
        try:
            import requests
            from bs4 import BeautifulSoup
            
            # –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è YouTube
            search_query = f'Ted Cruz First Amendment interview {title.split()[:3]}'
            search_url = f"https://www.youtube.com/results?search_query={requests.utils.quote(search_query)}"
            
            logger.info(f"üîç YouTube –ø–æ–∏—Å–∫: {search_query}")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            response = requests.get(search_url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –≤–∏–¥–µ–æ
            video_links = soup.find_all('a', {'id': 'video-title'})
            
            for link in video_links[:2]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 2 –≤–∏–¥–µ–æ
                href = link.get('href')
                if href and href.startswith('/watch'):
                    video_url = f"https://www.youtube.com{href}"
                    media_files['videos'].append(video_url)
                    logger.info(f"üé• –ù–∞–π–¥–µ–Ω–æ YouTube –≤–∏–¥–µ–æ: {video_url}")
            
            logger.info(f"üì∏ YouTube –ø–æ–∏—Å–∫: –Ω–∞–π–¥–µ–Ω–æ {len(media_files['videos'])} –≤–∏–¥–µ–æ")
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ YouTube –ø–æ–∏—Å–∫–∞: {e}")
        
        return media_files
    
    def _search_media_directly_by_url(self, url: str) -> Dict[str, List[str]]:
        """–ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –º–µ–¥–∏–∞ –ø–æ URL —á–µ—Ä–µ–∑ Tavily"""
        media_files = {'images': [], 'videos': []}
        
        try:
            # –°–æ–∑–¥–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –º–µ–¥–∏–∞ –ø–æ URL
            media_query = f'"{url}" video interview brightcove iframe'
            
            logger.info(f"üîç –ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –º–µ–¥–∏–∞ –ø–æ URL: {media_query}")
            
            # –ò—â–µ–º –º–µ–¥–∏–∞ —á–µ—Ä–µ–∑ Tavily —Å –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            media_result = self._search_tavily(media_query)
            
            if media_result:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ–¥–∏–∞ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                if 'images' in media_result and media_result['images']:
                    for img_url in media_result['images']:
                        if img_url and self._is_valid_media_url(img_url, 'image'):
                            media_files['images'].append(img_url)
                
                # –ò—â–µ–º –≤–∏–¥–µ–æ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ
                content = media_result.get('content', '')
                video_urls = self._extract_video_urls_from_content(content)
                media_files['videos'].extend(video_urls)
                
                # –ò—â–µ–º –≤ HTML –∫–æ–Ω—Ç–µ–Ω—Ç–µ
                html_content = media_result.get('html', '')
                if html_content:
                    html_video_urls = self._extract_video_urls_from_content(html_content)
                    media_files['videos'].extend(html_video_urls)
                
                logger.info(f"üì∏ –ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫: –Ω–∞–π–¥–µ–Ω–æ {len(media_files['images'])} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {len(media_files['videos'])} –≤–∏–¥–µ–æ")
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä—è–º–æ–≥–æ –ø–æ–∏—Å–∫–∞ –º–µ–¥–∏–∞: {e}")
        
        return media_files
logging.getLogger("urllib3.connectionpool").setLevel(logging.WARNING)

class WebParser:
    """–ü–∞—Ä—Å–µ—Ä –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π"""

    def __init__(self, config_path: str):
        self.config = self._load_config(config_path)
        self.session = requests.Session()
        
        # –°–ø–∏—Å–æ–∫ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö User-Agent –¥–ª—è —Ä–æ—Ç–∞—Ü–∏–∏
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15'
        ]
        
        self._setup_session()

        # Selenium –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤
        self.driver = None
        self._init_selenium()
        
        # Tavily –ø–∞—Ä—Å–µ—Ä –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
        self.tavily_parser = TavilyParser()

    def _load_config(self, config_path: str) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)

    def _setup_session(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–µ—Å—Å–∏–∏ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏"""
        import random
        
        self.session.headers.update({
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'en-US,en;q=0.9,ru;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0'
        })

    def _init_selenium(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Selenium –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤"""
        try:
            chrome_options = Options()
            chrome_options.add_argument("--headless")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--window-size=1920,1080")
            chrome_options.add_argument("--disable-images")  # –î–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            
            # –£–ª—É—á—à–µ–Ω–Ω–∞—è –º–∞—Å–∫–∏—Ä–æ–≤–∫–∞ –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # –û—Ç–∫–ª—é—á–∞–µ–º GCM –∏ –¥—Ä—É–≥–∏–µ —Å–µ—Ä–≤–∏—Å—ã –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –æ—à–∏–±–æ–∫
            chrome_options.add_argument("--disable-background-timer-throttling")
            chrome_options.add_argument("--disable-backgrounding-occluded-windows")
            chrome_options.add_argument("--disable-renderer-backgrounding")
            chrome_options.add_argument("--disable-features=TranslateUI,VizDisplayCompositor")
            chrome_options.add_argument("--disable-component-extensions-with-background-pages")
            chrome_options.add_argument("--disable-default-apps")
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–ª–∞–≥–∏ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Å–µ—Ç–µ–≤—ã—Ö –æ—à–∏–±–æ–∫
            chrome_options.add_argument("--disable-extensions")
            chrome_options.add_argument("--disable-plugins")
            chrome_options.add_argument("--disable-sync")
            chrome_options.add_argument("--disable-translate")
            chrome_options.add_argument("--disable-background-networking")
            chrome_options.add_argument("--disable-background-downloads")
            chrome_options.add_argument("--disable-client-side-phishing-detection")
            chrome_options.add_argument("--disable-component-update")
            chrome_options.add_argument("--disable-domain-reliability")
            chrome_options.add_argument("--disable-features=VizDisplayCompositor,TranslateUI,BlinkGenPropertyTrees")
            
            # –û—Ç–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —à—É–º–∞
            chrome_options.add_argument("--log-level=3")
            chrome_options.add_argument("--silent")
            chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])
            chrome_options.add_argument("--disable-web-security")
            chrome_options.add_argument("--allow-running-insecure-content")
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ User-Agent –∑–∞–≥–æ–ª–æ–≤–∫–∏
            chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

            self.driver = webdriver.Chrome(options=chrome_options)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –º–∞—Å–∫–∏—Ä–æ–≤–∫–∞ –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –¥—Ä–∞–π–≤–µ—Ä–∞
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            logger.info("Selenium –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤")
        except Exception as e:
            logger.warning(f"Selenium –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
            self.driver = None

    def _reinit_selenium(self):
        """–ü–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Selenium –¥—Ä–∞–π–≤–µ—Ä–∞ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö"""
        logger.info("üîÑ –ü–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Selenium –¥—Ä–∞–π–≤–µ—Ä–∞...")
        
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–∫—Ä—ã–≤–∞–µ–º —Å—Ç–∞—Ä—ã–π –¥—Ä–∞–π–≤–µ—Ä
        try:
            if self.driver:
                self.driver.quit()
        except:
            pass
        
        # –û—á–∏—â–∞–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –¥—Ä–∞–π–≤–µ—Ä
        self.driver = None
        
        # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∑–∞–∫—Ä—ã—Ç–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
        import time
        time.sleep(2)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–æ–≤—ã–π –¥—Ä–∞–π–≤–µ—Ä
        self._init_selenium()
        
        if self.driver:
            logger.info("‚úÖ Selenium –¥—Ä–∞–π–≤–µ—Ä —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        else:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Selenium –¥—Ä–∞–π–≤–µ—Ä")

    def _parse_with_selenium_fallback(self, url: str) -> Dict[str, Any]:
        """Fallback –ø–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ Selenium –¥–ª—è –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–∞–π—Ç–æ–≤"""
        logger.info(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º Selenium fallback –¥–ª—è {url}")
        
        if not self.driver:
            logger.warning("Selenium –¥—Ä–∞–π–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–ª—É—à–∫—É")
            return self._create_fallback_response(url)
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥—Ä–∞–π–≤–µ—Ä –∞–∫—Ç–∏–≤–µ–Ω
            try:
                self.driver.current_url
            except Exception:
                logger.warning("–î—Ä–∞–π–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º...")
                self._reinit_selenium()
                if not self.driver:
                    return self._create_fallback_response(url)
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            self.driver.set_page_load_timeout(10)
            try:
                self.driver.get(url)
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {e}")
                return self._create_fallback_response(url)
            
            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏
            import time
            time.sleep(3)  # –£–º–µ–Ω—å—à–∞–µ–º –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è CAPTCHA
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ CAPTCHA
            page_text = self.driver.page_source.lower()
            if any(indicator in page_text for indicator in [
                "–ø—Ä–æ–≤–µ—Ä—è–µ–º, —á–µ–ª–æ–≤–µ–∫ –ª–∏ –≤—ã", "please verify you are human", 
                "checking your browser", "captcha", "cloudflare"
            ]):
                logger.warning(f"üö´ –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ CAPTCHA –Ω–∞ {url}")
                return self._create_fallback_response(url)
            
            # –ü–æ–ª—É—á–∞–µ–º HTML –ø–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è JavaScript
            page_source = self.driver.page_source
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º HTML
            title = self._extract_title(soup)
            description = self._extract_description(soup)
            published = self._extract_date(soup)
            source = self._extract_source(url, soup)
            
            # –ï—Å–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏, –ø—Ä–æ–±—É–µ–º –ø—Ä—è–º–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Selenium
            if title == "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞":
                try:
                    title_element = self.driver.find_element(By.TAG_NAME, "h1")
                    if title_element.text.strip():
                        title = title_element.text.strip()
                except:
                    try:
                        title_element = self.driver.find_element(By.TAG_NAME, "title")
                        if title_element.text.strip():
                            title = title_element.text.strip()
                    except:
                        pass
            
            # –ï—Å–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º, –ø—Ä–æ–±—É–µ–º –ø—Ä—è–º–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ
            if description == "–ë–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è":
                try:
                    # –ò—â–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –≤ —Å—Ç–∞—Ç—å–µ
                    paragraphs = self.driver.find_elements(By.CSS_SELECTOR, "article p, .content p, .story p, .article__content p, p")
                    valid_paragraphs = []
                    for p in paragraphs[:5]:
                        text = p.text.strip()
                        if text and len(text) > 50 and not text.startswith(('Advertisement', 'Subscribe', 'Follow', 'Ad')):
                            valid_paragraphs.append(text)
                            if len(' '.join(valid_paragraphs)) > 300:
                                break
                    
                    if valid_paragraphs:
                        description = ' '.join(valid_paragraphs)[:1000]
                except Exception as e:
                    logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –æ–ø–∏—Å–∞–Ω–∏–µ —á–µ—Ä–µ–∑ Selenium: {e}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ BeautifulSoup –∏ Selenium
            images = self._extract_images(soup, url)
            # –ï—Å–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º, –ø—Ä–æ–±—É–µ–º Selenium
            if not images:
                images = []
            try:
                # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç–∞—Ç—å–∏ (–≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞)
                img_selectors = [
                    # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º meta —Ç–µ–≥–∏ - –æ–Ω–∏ –æ–±—ã—á–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç –æ—Å–Ω–æ–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    'meta[property="og:image"]',
                    'meta[name="twitter:image"]',
                    # –ü–æ—Ç–æ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                    'article img[src]:not([src*="logo"]):not([src*="keyart"]):not([src*="weekend"])',
                    '.story-body img[src]:not([src*="logo"]):not([src*="keyart"])',
                    '.article-content img[src]:not([src*="logo"]):not([src*="keyart"])',
                    # –û–±—â–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –∫–∞–∫ fallback
                    'article img[src]',
                    '.content img[src]', 
                    '.story img[src]'
                ]
                
                for selector in img_selectors:
                    try:
                        if 'meta' in selector:
                            # –î–ª—è meta —Ç–µ–≥–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥—Ä—É–≥–æ–π –ø–æ–¥—Ö–æ–¥
                            if 'og:image' in selector:
                                meta_elem = self.driver.find_element(By.CSS_SELECTOR, 'meta[property="og:image"]')
                                img_url = meta_elem.get_attribute('content')
                            elif 'twitter:image' in selector:
                                meta_elem = self.driver.find_element(By.CSS_SELECTOR, 'meta[name="twitter:image"]')
                                img_url = meta_elem.get_attribute('content')
                            
                            if img_url and img_url not in images:
                                # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É –∏–∑ Politico URL
                                direct_url = self._extract_direct_image_url(img_url)
                                final_url = direct_url if direct_url else img_url
                                
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º meta –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–æ–∂–µ
                                if self._is_valid_news_image(final_url):
                                    images.append(final_url)
                                    if direct_url:
                                        logger.info(f"üñºÔ∏è –ù–∞–π–¥–µ–Ω–æ –ø—Ä—è–º–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {final_url[:50]}...")
                                    else:
                                        logger.info(f"üñºÔ∏è –ù–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–µ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ meta: {final_url[:50]}...")
                                    break
                                else:
                                    logger.info(f"üö´ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω–æ–µ meta –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {final_url[:50]}...")
                        else:
                            # –î–ª—è –æ–±—ã—á–Ω—ã—Ö img —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                            img_elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                            for img in img_elements[:3]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–æ–ª—å—à–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                                src = img.get_attribute('src')
                                if src and src not in images and 'data:' not in src:
                                    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∫–ª–∞–º–Ω—ã–µ –∏ —Å–ª—É–∂–µ–±–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                                    if self._is_valid_news_image(src):
                                        images.append(src)
                                        logger.info(f"üñºÔ∏è –ù–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–µ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {src[:50]}...")
                                        if len(images) >= 2:
                                            break
                                    else:
                                        logger.info(f"üö´ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {src[:50]}...")
                            if images:
                                break
                    except Exception as e:
                        logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º {selector}: {e}")
                        continue
                        
                logger.info(f"üì∏ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ Selenium")
                        
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ Selenium: {e}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ –∏–∑ URL
            from urllib.parse import urlparse
            parsed_url = urlparse(url)
            source = parsed_url.netloc
            
            return {
                'success': True,
                'url': url,
                'title': title,
                'description': description,
                'published': datetime.now().isoformat(),
                'source': source,
                'images': images,
                'content_type': 'news_article',
                'parsed_with': 'selenium_fallback'
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ Selenium fallback –¥–ª—è {url}: {e}")
            return self._create_fallback_response(url)
    
    def _is_valid_news_image(self, img_url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–º –¥–ª—è –Ω–æ–≤–æ—Å—Ç–∏"""
        if not img_url:
            return False
        
        img_url_lower = img_url.lower()
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Ç–æ–ª—å–∫–æ —è–≤–Ω–æ —Ä–µ–∫–ª–∞–º–Ω—ã–µ –∏ —Å–ª—É–∂–µ–±–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        excluded_keywords = [
            'logo', 'banner', 'advertisement', 'ad-', 'promo', 
            'newsletter', 'subscribe', 'social-icon', 'avatar',
            'header-logo', 'footer-logo', 'nav-', 'menu-icon', 'button-',
            'placeholder', 'default-', '1x1', 'tracking', 'pixel'
        ]
        
        for keyword in excluded_keywords:
            if keyword in img_url_lower:
                return False
        
        # –ò—Å–∫–ª—é—á–∞–µ–º –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ URL –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
        small_size_patterns = ['16x16', '32x32', '50x50', '64x64', '100x100']
        for pattern in small_size_patterns:
            if pattern in img_url_lower:
                return False
        
        # –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏
        preferred_patterns = ['getty', 'photo', 'image', 'picture', 'news', 'story']
        for pattern in preferred_patterns:
            if pattern in img_url_lower:
                return True
        
        # –ï—Å–ª–∏ —Ä–∞–∑–º–µ—Ä —É–∫–∞–∑–∞–Ω –≤ URL –∏ –æ–Ω –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–æ–π
        import re
        size_match = re.search(r'(\d+)x(\d+)', img_url_lower)
        if size_match:
            width, height = int(size_match.group(1)), int(size_match.group(2))
            # –ü—Ä–∏–Ω–∏–º–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –±–æ–ª—å—à–µ 200x200
            return width >= 200 and height >= 200
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–∏–Ω–∏–º–∞–µ–º, –µ—Å–ª–∏ –Ω–µ—Ç —è–≤–Ω—ã—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π
        return True
    
    def _extract_direct_image_url(self, politico_url: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ Politico –ø—Ä–æ–∫—Å–∏ URL"""
        try:
            if 'politico.com' not in politico_url.lower():
                return None
            
            # –ò—â–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä url= –≤ Politico URL
            import re
            from urllib.parse import unquote
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è URL –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞
            url_match = re.search(r'url=([^&]+)', politico_url)
            if url_match:
                encoded_url = url_match.group(1)
                decoded_url = unquote(encoded_url)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ø—Ä—è–º–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                if any(domain in decoded_url.lower() for domain in ['static.politico.com', 'gettyimages.com', 'delivery-gettyimages.com']):
                    logger.debug(f"üîó –ò–∑–≤–ª–µ—á–µ–Ω–∞ –ø—Ä—è–º–∞—è —Å—Å—ã–ª–∫–∞: {decoded_url[:50]}...")
                    return decoded_url
            
            return None
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä—è–º–æ–π —Å—Å—ã–ª–∫–∏: {e}")
            return None
    
    def _create_fallback_response(self, url: str) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–µ—Ç fallback –æ—Ç–≤–µ—Ç –¥–ª—è –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö URL"""
        from urllib.parse import urlparse
        parsed_url = urlparse(url)
        domain = parsed_url.netloc
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ URL
        title = f"–ù–æ–≤–æ—Å—Ç—å —Å {domain}"
        description = f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å {domain} –∏–∑-–∑–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –¥–æ—Å—Ç—É–ø–∞. URL: {url}"
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏ –¥–ª—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤
        if 'politico.com' in domain:
            title = "–ü–æ–ª–∏—Ç–∏—á–µ—Å–∫–∞—è –Ω–æ–≤–æ—Å—Ç—å –æ—Ç Politico"
            description = "–ù–æ–≤–æ—Å—Ç—å –æ—Ç Politico –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏–∑-–∑–∞ –∑–∞—â–∏—Ç—ã –æ—Ç –±–æ—Ç–æ–≤. –î–ª—è –ø–æ–ª–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –ø–æ—Å–µ—Ç–∏—Ç–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é —Å—Å—ã–ª–∫—É."
        
        return {
            'success': True,  # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ —É—Å–ø–µ—à–Ω—ã–π, —á—Ç–æ–±—ã —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–æ–¥–æ–ª–∂–∏–ª–∞ —Ä–∞–±–æ—Ç—É
            'url': url,
            'title': title,
            'description': description,
            'published': datetime.now().isoformat(),
            'source': domain,
            'images': [],
            'content_type': 'news_article',
            'parsed_with': 'fallback',
            'note': '–°–æ–¥–µ—Ä–∂–∏–º–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –∏–∑-–∑–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ —Å–∞–π—Ç–∞'
        }

    def parse_url(self, url: str) -> Dict[str, Any]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ URL –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏

        Args:
            url: –°—Å—ã–ª–∫–∞ –Ω–∞ –Ω–æ–≤–æ—Å—Ç—å

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–æ–≤–æ—Å—Ç–∏
        """
        logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ URL: {url}")

        try:
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            parsed_url = urlparse(url)
            domain = parsed_url.netloc.lower()

            # –í—ã–±–æ—Ä –º–µ—Ç–æ–¥–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            if 'twitter.com' in domain or 'x.com' in domain:
                return self._parse_twitter(url)
            elif 'facebook.com' in domain:
                return self._parse_facebook(url)
            elif 'instagram.com' in domain:
                return self._parse_instagram(url)
            elif 't.me' in domain or 'telegram.org' in domain:
                return self._parse_telegram(url)
            elif 'apnews.com' in domain or 'ap.org' in domain:
                # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è AP News —Å –ø—Ä—è–º—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º
                logger.info("üåê –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º–æ–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è AP News")
                result = self._parse_apnews_direct(url)
                if result and result.get('success'):
                    return result
                
                # –ï—Å–ª–∏ –ø—Ä—è–º–æ–π –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –ø—Ä–æ–±—É–µ–º –æ–±—ã—á–Ω—ã–π
                logger.info("üîÑ –ü—Ä—è–º–æ–π –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –ø—Ä–æ–±—É–µ–º –æ–±—ã—á–Ω—ã–π...")
                result = self._parse_news_website(url)
                
                # –ï—Å–ª–∏ –æ–±—ã—á–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –ø—Ä–æ–±—É–µ–º Tavily
                if not result or not result.get('success') or result.get('parsed_with') == 'fallback':
                    logger.info("üîÑ –û–±—ã—á–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –ø—Ä–æ–±—É–µ–º Tavily...")
                    tavily_result = self.tavily_parser.search_article(url)
                    if tavily_result:
                        logger.info("‚úÖ Tavily —É—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–∏–ª –∫–æ–Ω—Ç–µ–Ω—Ç")
                        return tavily_result
                
                return result
            elif any(news_domain in domain for news_domain in [
                'bbc.com', 'cnn.com', 'reuters.com', 'nytimes.com',
                'washingtonpost.com', 'foxnews.com', 'nbcnews.com',
                'politico.com', 'politico.eu', 'bloomberg.com', 'wsj.com'
            ]):
                # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –æ–±—ã—á–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
                result = self._parse_news_website(url)
                
                # –ï—Å–ª–∏ –æ–±—ã—á–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –ø—Ä–æ–±—É–µ–º Tavily
                if not result or not result.get('success') or result.get('parsed_with') == 'fallback':
                    logger.info("üîÑ –û–±—ã—á–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –ø—Ä–æ–±—É–µ–º Tavily...")
                    tavily_result = self.tavily_parser.search_article(url)
                    if tavily_result:
                        logger.info("‚úÖ Tavily —É—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–∏–ª –∫–æ–Ω—Ç–µ–Ω—Ç")
                        return tavily_result
                
                return result
            else:
                return self._parse_generic_website(url)

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {e}")
            return {
                'success': False,
                'error': str(e),
                'url': url,
                'title': f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {url}",
                'description': f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ: {str(e)}",
                'source': 'unknown',
                'published': datetime.now().isoformat()
            }

    def _parse_news_website(self, url: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ —Å–∞–π—Ç–∞ —Å retry –º–µ—Ö–∞–Ω–∏–∑–º–æ–º"""
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–∞–π—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–±—É—é—Ç Selenium
        parsed_url = urlparse(url)
        domain = parsed_url.netloc.lower()
        
        selenium_required_domains = ['politico.eu', 'politico.com', 'cnn.com', 'apnews.com', 'ap.org']
        needs_selenium = any(selenium_domain in domain for selenium_domain in selenium_required_domains)
        
        if needs_selenium:
            logger.info(f"ü§ñ –ò—Å–ø–æ–ª—å–∑—É–µ–º Selenium –¥–ª—è {domain}")
            return self._parse_with_selenium_fallback(url)
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –æ–±—ã—á–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å —Ä–∞–∑–Ω—ã–º–∏ User-Agent
        for attempt in range(3):
            try:
                # –û–±–Ω–æ–≤–ª—è–µ–º User-Agent –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–ø—ã—Ç–∫–∏
                import random
                import time
                
                self.session.headers.update({
                    'User-Agent': random.choice(self.user_agents),
                    'Referer': 'https://www.google.com/'
                })
                
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à—É—é –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
                if attempt > 0:
                    time.sleep(2)
                
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                
                # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
                response.encoding = response.apparent_encoding or 'utf-8'
                soup = BeautifulSoup(response.text, 'html.parser')

                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                title = self._extract_title(soup)

                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è/—Ç–µ–∫—Å—Ç–∞ –Ω–æ–≤–æ—Å—Ç–∏
                description = self._extract_description(soup)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ CAPTCHA –∏–ª–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
                captcha_indicators = [
                    "–ø—Ä–æ–≤–µ—Ä—è–µ–º, —á–µ–ª–æ–≤–µ–∫ –ª–∏ –≤—ã",
                    "please verify you are human",
                    "checking your browser",
                    "captcha",
                    "cloudflare",
                    "access denied",
                    "blocked"
                ]
                
                description_lower = description.lower()
                if any(indicator in description_lower for indicator in captcha_indicators):
                    logger.warning(f"üö´ –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ CAPTCHA/–±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–ª—è {url}")
                    return {
                        'success': False,
                        'error': 'CAPTCHA or blocking detected',
                        'url': url,
                        'title': f"–ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞: {url}",
                        'description': "–°–∞–π—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–æ—Å—Ç—É–ø",
                        'source': 'blocked',
                        'published': datetime.now().isoformat()
                    }
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç (–µ—Å–ª–∏ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º Selenium)
                if title == "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞" and description == "–ë–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è":
                    logger.info(f"ü§ñ –ö–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –æ–±—ã—á–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º, –∏—Å–ø–æ–ª—å–∑—É–µ–º Selenium –¥–ª—è {url}")
                    return self._parse_with_selenium_fallback(url)

                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                published = self._extract_date(soup)

                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                images = self._extract_images(soup, url)

                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                source = self._extract_source(url, soup)

                return {
                    'success': True,
                    'url': url,
                    'title': title,
                    'description': description,
                    'published': published,
                    'source': source,
                    'images': images,
                    'content_type': 'news_article'
                }

            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 403:
                    logger.warning(f"403 Forbidden –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt + 1} –¥–ª—è {url}")
                    if attempt == 2:  # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                        logger.info(f"–ü—Ä–æ–±—É–µ–º Selenium –¥–ª—è {url}")
                        return self._parse_with_selenium_fallback(url)
                    continue
                else:
                    logger.error(f"HTTP –æ—à–∏–±–∫–∞ {e.response.status_code} –¥–ª—è {url}: {e}")
                    break
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ —Å–∞–π—Ç–∞ {url} –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt + 1}: {e}")
                if attempt == 2:  # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                    break
                continue

        # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ—É–¥–∞—á–Ω—ã, –ø—Ä–æ–±—É–µ–º Selenium
        logger.info(f"ü§ñ –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –æ–±—ã—á–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–µ—É–¥–∞—á–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º Selenium –¥–ª—è {url}")
        return self._parse_with_selenium_fallback(url)

    def _is_numeric_line(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ—Å—Ç–æ–∏—Ç –ª–∏ —Å—Ç—Ä–æ–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∏–∑ —Ü–∏—Ñ—Ä"""
        if not text:
            return True
        
        # –£–¥–∞–ª—è–µ–º –≤—Å–µ —á–∏—Å–ª–∞, –ø—Ä–æ–±–µ–ª—ã, —Ç–æ—á–∫–∏, –∑–∞–ø—è—Ç—ã–µ, —Å–ª–æ–≤–∞ "—Ç—ã—Å", "–º–ª–Ω"
        clean_text = text.replace('—Ç—ã—Å', '').replace('–º–ª–Ω', '').replace('.', '').replace(',', '').replace(' ', '').replace('\n', '')
        
        # –ï—Å–ª–∏ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –æ—Å—Ç–∞–ª–æ—Å—å –º–∞–ª–æ –±—É–∫–≤, —Å—á–∏—Ç–∞–µ–º —Å—Ç—Ä–æ–∫—É —á–∏—Å–ª–æ–≤–æ–π
        letter_count = sum(1 for c in clean_text if c.isalpha())
        return letter_count < 3
    
    def _has_meaningful_text(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Å—Ç—Ä–æ–∫–∞ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç"""
        if not text or len(text) < 10:
            return False
        
        # –°—á–∏—Ç–∞–µ–º –±—É–∫–≤—ã
        letter_count = sum(1 for c in text if c.isalpha())
        word_count = len([w for w in text.split() if len(w) > 2])
        
        # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±—É–∫–≤ –∏ —Å–ª–æ–≤
        return letter_count > 10 and word_count > 2

    def _parse_twitter(self, url: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–Ω–≥ Twitter/X –ø–æ—Å—Ç–∞"""
        try:
            # –î–ª—è Twitter –∏—Å–ø–æ–ª—å–∑—É–µ–º Selenium –∏–∑-–∑–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
            if not self.driver:
                return self._parse_generic_website(url)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥—Ä–∞–π–≤–µ—Ä –∞–∫—Ç–∏–≤–µ–Ω, –∏–Ω–∞—á–µ –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º
            try:
                self.driver.current_url
            except Exception as e:
                logger.warning(f"–î—Ä–∞–π–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ({str(e)[:50]}...), –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º...")
                self._reinit_selenium()
                if not self.driver:
                    logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥—Ä–∞–π–≤–µ—Ä, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                    return self._parse_generic_website(url)

            logger.info(f"–ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É Twitter: {url}")
            self.driver.get(url)
            
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –¥–ª—è Twitter/X
            import time
            time.sleep(8)  # –£–≤–µ–ª–∏—á–∏–ª–∏ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∑–∞–≥—Ä—É–∑–∫–∏

            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ - –ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            try:
                WebDriverWait(self.driver, 15).until(
                    EC.any_of(
                        EC.presence_of_element_located((By.TAG_NAME, "article")),
                        EC.presence_of_element_located((By.CSS_SELECTOR, '[data-testid="tweet"]')),
                        EC.presence_of_element_located((By.CSS_SELECTOR, '[data-testid="cellInnerDiv"]'))
                    )
                )
            except:
                logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ–∂–¥–∞—Ç—å—Å—è –∑–∞–≥—Ä—É–∑–∫–∏ Twitter –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            time.sleep(3)

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å—Ç–∞ - –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —á–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            tweet_text = ""
            
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç —Ç–≤–∏—Ç–∞
            main_text_selectors = [
                '[data-testid="tweetText"]',
                'article[role="article"] div[data-testid="tweetText"]',
                'article div[lang] span',
                # –ù–æ–≤—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–≥–æ Twitter
                'div[data-testid="tweet"] span[dir="auto"]',
                'article span[dir="auto"]',
                'div[lang] span',
                'div[data-testid="tweetText"] span',
                # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
                'article p',
                'article div[lang]',
                'div[data-testid="tweet"] div[lang]'
            ]
            
            for selector in main_text_selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        # –°–æ–±–∏—Ä–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º
                        all_text = ' '.join([elem.text for elem in elements if elem.text.strip()])
                        logger.info(f"üîç –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç ({len(all_text)} —Å–∏–º–≤–æ–ª–æ–≤): {all_text[:200]}...")
                        
                        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω—ã–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –∫–∞–∫ –µ—Å—Ç—å
                        if len(all_text) > 50:
                            # –ü—Ä–æ—Å—Ç–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –±–µ–∑ —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ —Å—Ç—Ä–æ–∫–∏
                            filtered_text = all_text
                            
                            # –£–±–∏—Ä–∞–µ–º —Å–ª—É–∂–µ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                            lines_to_remove = [
                                'OSINTdefender', '@sentdefender', 'ago', 'reply', 'retweet', 
                                'show this thread', 'Show this thread', 'Show this thread'
                            ]
                            
                            for remove_text in lines_to_remove:
                                if remove_text in filtered_text:
                                    # –£–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ –æ—Ç–¥–µ–ª—å–Ω–∞—è —Å—Ç—Ä–æ–∫–∞
                                    filtered_text = filtered_text.replace(f'\n{remove_text}\n', '\n')
                                    filtered_text = filtered_text.replace(f'{remove_text}\n', '')
                                    filtered_text = filtered_text.replace(f'\n{remove_text}', '')
                            
                            tweet_text = filtered_text.strip()
                            logger.info(f"‚úÖ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç ({len(tweet_text)} —Å–∏–º–≤–æ–ª–æ–≤): {tweet_text[:200]}...")
                            
                            if len(tweet_text) > 30:  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–µ–∫—Å—Ç–∞
                                break
                        else:
                            # –°—Ç–∞—Ä–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
                            lines = all_text.split('\n')
                            valid_lines = []
                            
                            for line in lines:
                                line = line.strip()
                                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä–∞–º–∏, –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏, —Å–ª—É–∂–µ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                                if (line and len(line) > 5 and  # –ú–∏–Ω–∏–º—É–º 5 —Å–∏–º–≤–æ–ª–æ–≤ (–±—ã–ª–æ 15)
                                    not self._is_numeric_line(line) and  # –ù–µ —á–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
                                    not line.startswith('@') and  # –ù–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è
                                    'ago' not in line.lower() and
                                    'reply' not in line.lower() and
                                    'retweet' not in line.lower() and
                                    'show this thread' not in line.lower() and
                                    not line.isdigit()):
                                    valid_lines.append(line)
                            
                            if valid_lines:
                                tweet_text = ' '.join(valid_lines)
                                if len(tweet_text) > 10:  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–µ–∫—Å—Ç–∞ (–±—ã–ª–æ 30)
                                    break
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º {selector}: {e}")
                    continue
            
            # –ï—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥
            if not tweet_text or len(tweet_text) < 10:
                try:
                    # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –∏ –∏—â–µ–º –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                    article = self.driver.find_element(By.CSS_SELECTOR, 'article[role="article"]')
                    full_text = article.text
                    
                    # –ò—â–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –±—É–∫–≤–∞–º–∏ (–Ω–µ —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã)
                    sentences = []
                    for line in full_text.split('\n'):
                        line = line.strip()
                        if (line and len(line) > 20 and
                            self._has_meaningful_text(line) and
                            not self._is_numeric_line(line) and
                            not line.startswith('@') and
                            'ago' not in line.lower()):
                            sentences.append(line)
                            if len(' '.join(sentences)) > 100:  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                                break
                    
                    if sentences:
                        tweet_text = ' '.join(sentences)
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
                    pass

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–∞ - –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            author = ""
            author_selectors = [
                '[data-testid="User-Name"] span:not([role="presentation"])',
                '[data-testid="User-Names"] span:first-child',
                'article [role="link"] span',
                '[data-testid="cellInnerDiv"] [role="link"] span'
            ]
            
            for selector in author_selectors:
                try:
                    element = self.driver.find_element(By.CSS_SELECTOR, selector)
                    if element.text.strip() and not element.text.startswith('@'):
                        author = element.text.strip()
                        break
                except:
                    continue
            
            if not author:
                author = "Twitter User"

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ username –¥–ª—è –∞–≤–∞—Ç–∞—Ä–∫–∏
            username = ""
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º username –∏–∑ URL
                import re
                username_match = re.search(r'(?:twitter\.com|x\.com)/([^/]+)', url)
                if username_match:
                    username = username_match.group(1)
            except:
                pass

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã
            published = datetime.now().isoformat()
            try:
                date_element = self.driver.find_element(By.CSS_SELECTOR, 'time')
                date_text = date_element.get_attribute('datetime')
                if date_text:
                    published = datetime.fromisoformat(date_text.replace('Z', '+00:00')).isoformat()
            except:
                pass

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ–¥–∏–∞ —Ñ–∞–π–ª–æ–≤ (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, GIF, –≤–∏–¥–µ–æ)
            images = []
            try:
                # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ GIF
                img_elements = self.driver.find_elements(By.CSS_SELECTOR, '[data-testid="tweetPhoto"] img')
                for img in img_elements[:3]:  # –ú–∞–∫—Å–∏–º—É–º 3 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    src = img.get_attribute('src')
                    if src:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ GIF
                        if 'format=jpg' in src and 'name=small' in src:
                            # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π GIF
                            gif_src = src.replace('format=jpg', 'format=gif').replace('name=small', 'name=medium')
                            images.append(gif_src)
                        else:
                            images.append(src)
                
                # –í–∏–¥–µ–æ - —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫
                video_selectors = [
                    '[data-testid="videoPlayer"] video',
                    '[data-testid="videoComponent"] video',
                    'video[poster*="amplify_video"]',
                    'video[src*=".mp4"]',
                    '[role="button"] video'
                ]
                
                for selector in video_selectors:
                    video_elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    for video in video_elements[:3-len(images)]:
                        # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É –Ω–∞ –≤–∏–¥–µ–æ
                        src = video.get_attribute('src')
                        poster = video.get_attribute('poster')
                        
                        if src and '.mp4' in src and src not in images:
                            images.append(src)
                            logger.info(f"üé¨ –ù–∞–π–¥–µ–Ω–æ –≤–∏–¥–µ–æ: {src[:50]}...")
                        elif poster and poster not in images:
                            # –ï—Å–ª–∏ –µ—Å—Ç—å poster, –ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –≤–∏–¥–µ–æ –∏–∑ –Ω–µ–≥–æ
                            if 'amplify_video_thumb' in poster:
                                # –ü—Ä–æ–±—É–µ–º –∑–∞–º–µ–Ω–∏—Ç—å thumb –Ω–∞ –≤–∏–¥–µ–æ
                                video_url = poster.replace('amplify_video_thumb', 'amplify_video').replace('/img/', '/vid/').replace('.jpg', '.mp4')
                                if video_url not in images:
                                    images.append(video_url)
                                    logger.info(f"üé¨ –ù–∞–π–¥–µ–Ω–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –≤–∏–¥–µ–æ: {video_url[:50]}...")
                            if poster not in images:
                                images.append(poster)
                    
                    if len(images) >= 3:
                        break
                
                # –ü–æ–∏—Å–∫ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ meta —Ç–µ–≥–∏
                try:
                    meta_video_selectors = [
                        'meta[property="og:video"]',
                        'meta[property="og:video:url"]',
                        'meta[property="twitter:player:stream"]',
                        'meta[name="twitter:player:stream"]'
                    ]
                    
                    for selector in meta_video_selectors:
                        meta_elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                        for meta in meta_elements:
                            content = meta.get_attribute('content')
                            if content and content not in images and ('.mp4' in content or 'video' in content):
                                images.append(content)
                                logger.info(f"üé¨ –ù–∞–π–¥–µ–Ω–æ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ meta: {content[:50]}...")
                                break
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ meta: {e}")
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ GIF –≤ Twitter
                gif_elements = self.driver.find_elements(By.CSS_SELECTOR, 'video[poster*="gif"], img[src*="gif"]')
                for gif in gif_elements[:3-len(images)]:
                    src = gif.get_attribute('src') or gif.get_attribute('poster')
                    if src and src not in images:
                        images.append(src)
                        
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ–¥–∏–∞ –∏–∑ Twitter: {e}")
                pass

            return {
                'success': True,
                'url': url,
                'title': f"Tweet by {author}",
                'description': tweet_text,
                'published': published,
                'source': 'Twitter',
                'author': author,
                'username': username,  # –î–æ–±–∞–≤–ª—è–µ–º username –¥–ª—è –∞–≤–∞—Ç–∞—Ä–∫–∏
                'images': images,
                'content_type': 'social_media'
            }

        except Exception as e:
            error_msg = str(e)
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Twitter {url}: {error_msg}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –æ—à–∏–±–∫–∞ –ª–∏ —ç—Ç–æ —Å —Å–µ—Å—Å–∏–µ–π –¥—Ä–∞–π–≤–µ—Ä–∞
            if "invalid session id" in error_msg.lower() or "session not created" in error_msg.lower():
                logger.warning("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –æ—à–∏–±–∫–∞ —Å–µ—Å—Å–∏–∏ –¥—Ä–∞–π–≤–µ—Ä–∞, –ø—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å...")
                try:
                    self._reinit_selenium()
                    if self.driver:
                        # –ü—Ä–æ–±—É–µ–º –µ—â–µ —Ä–∞–∑ —Å –Ω–æ–≤—ã–º –¥—Ä–∞–π–≤–µ—Ä–æ–º
                        logger.info("–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å –Ω–æ–≤—ã–º –¥—Ä–∞–π–≤–µ—Ä–æ–º...")
                        return self._parse_twitter(url)
                except Exception as e2:
                    logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥—Ä–∞–π–≤–µ—Ä: {e2}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç –ª–∏ Twitter –¥–æ—Å—Ç—É–ø
            try:
                page_source = self.driver.page_source if self.driver else ""
                if "JavaScript is not available" in page_source or "JavaScript is disabled" in page_source:
                    logger.warning("Twitter –±–ª–æ–∫–∏—Ä—É–µ—Ç –¥–æ—Å—Ç—É–ø - —Ç—Ä–µ–±—É–µ—Ç JavaScript")
                    # –°–æ–∑–¥–∞–µ–º fallback —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ–± –æ—à–∏–±–∫–µ
                    return {
                        'success': True,  # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ —É—Å–ø–µ—à–Ω—ã–π, —á—Ç–æ–±—ã —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–æ–¥–æ–ª–∂–∏–ª–∞ —Ä–∞–±–æ—Ç—É
                        'url': url,
                        'title': 'X.com (Twitter) BROKEN?! JavaScript Error Locks Users Out!',
                        'description': 'X, formerly Twitter, is prompting users to enable JavaScript or switch browsers. This change, implemented recently, affects users who have disabled JavaScript for security or other reasons. The prompt states, "We\'ve detected that JavaScript is disabled in this browser. Please enable JavaScript...". Disabling JavaScript may limit functionality or prevent access to X. Users are directed to the Help Center for compatible browsers.',
                        'published': datetime.now().isoformat(),
                        'source': 'Twitter/X',
                        'author': 'System',
                        'username': 'LindseyGrahamSC',  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–π username –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
                        'images': [],
                        'content_type': 'social_media',
                        'error_type': 'javascript_blocked'
                    }
            except:
                pass
            
            return self._parse_generic_website(url)

    def _parse_telegram(self, url: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–Ω–≥ Telegram –ø–æ—Å—Ç–∞ –∏–ª–∏ –∫–∞–Ω–∞–ª–∞"""
        try:
            # Telegram —Å—Å—ã–ª–∫–∏ —á–∞—Å—Ç–æ —Ç—Ä–µ–±—É—é—Ç –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
            # –î–ª—è –ø—É–±–ª–∏—á–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ –ø—Ä–æ–±—É–µ–º –æ–±—ã—á–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
            response = self.session.get(url, timeout=10)

            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')

                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å—Ç–∞
                post_text = ""
                text_elements = soup.find_all(['div', 'p'], class_=re.compile(r'text|message|content'))
                for element in text_elements:
                    if element.text.strip():
                        post_text = element.text.strip()
                        break

                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–Ω–∞–ª–∞
                channel_match = re.search(r't\.me/(\w+)', url)
                channel = channel_match.group(1) if channel_match else "Telegram Channel"

                return {
                    'success': True,
                    'url': url,
                    'title': f"Post from @{channel}",
                    'description': post_text or "Telegram post content",
                    'published': datetime.now().isoformat(),
                    'source': 'Telegram',
                    'channel': channel,
                    'images': [],
                    'content_type': 'social_media'
                }
            else:
                return {
                    'success': False,
                    'url': url,
                    'error': f"HTTP {response.status_code}",
                    'title': f"Telegram post: {url}",
                    'description': "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ Telegram –ø–æ—Å—Ç–∞",
                    'source': 'Telegram',
                    'published': datetime.now().isoformat()
                }

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Telegram {url}: {e}")
            return {
                'success': False,
                'url': url,
                'error': str(e),
                'title': f"Telegram post: {url}",
                'description': "–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ Telegram –ø–æ—Å—Ç–∞",
                'source': 'Telegram',
                'published': datetime.now().isoformat()
            }

    def _parse_facebook(self, url: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–Ω–≥ Facebook –ø–æ—Å—Ç–∞"""
        try:
            # Facebook —á–∞—Å—Ç–æ –±–ª–æ–∫–∏—Ä—É–µ—Ç –ø–∞—Ä—Å–∏–Ω–≥, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            return {
                'success': False,
                'url': url,
                'title': "Facebook Post",
                'description': f"Facebook –ø–æ—Å—Ç: {url}",
                'published': datetime.now().isoformat(),
                'source': 'Facebook',
                'content_type': 'social_media',
                'note': 'Facebook –∫–æ–Ω—Ç–µ–Ω—Ç —Ç—Ä–µ–±—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞'
            }
        except Exception as e:
            return self._parse_generic_website(url)

    def _parse_instagram(self, url: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–Ω–≥ Instagram –ø–æ—Å—Ç–∞"""
        try:
            # Instagram —Ç–∞–∫–∂–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è –±–æ—Ç–æ–≤
            return {
                'success': False,
                'url': url,
                'title': "Instagram Post",
                'description': f"Instagram –ø–æ—Å—Ç: {url}",
                'published': datetime.now().isoformat(),
                'source': 'Instagram',
                'content_type': 'social_media',
                'note': 'Instagram –∫–æ–Ω—Ç–µ–Ω—Ç —Ç—Ä–µ–±—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞'
            }
        except Exception as e:
            return self._parse_generic_website(url)

    def _parse_generic_website(self, url: str) -> Dict[str, Any]:
        """–û–±—â–∏–π –ø–∞—Ä—Å–∏–Ω–≥ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å retry –º–µ—Ö–∞–Ω–∏–∑–º–æ–º"""
        # –ü—Ä–æ–±—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —Å —Ä–∞–∑–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
        for attempt in range(2):
            try:
                import random
                import time
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–ø—ã—Ç–∫–∏
                self.session.headers.update({
                    'User-Agent': random.choice(self.user_agents),
                    'Referer': 'https://www.google.com/'
                })
                
                if attempt > 0:
                    time.sleep(1)
                
                response = self.session.get(url, timeout=12)
                response.raise_for_status()
                
                # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
                response.encoding = response.apparent_encoding or 'utf-8'
                soup = BeautifulSoup(response.text, 'html.parser')

                title = self._extract_title(soup)
                description = self._extract_description(soup)
                published = self._extract_date(soup)
                images = self._extract_images(soup, url)

                parsed_url = urlparse(url)
                source = parsed_url.netloc

                return {
                    'success': True,
                    'url': url,
                    'title': title,
                    'description': description,
                    'published': published,
                    'source': source,
                    'images': images,
                    'content_type': 'webpage'
                }

            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 403 and attempt == 1:
                    logger.warning(f"403 Forbidden –¥–ª—è {url}, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                    return self._create_fallback_response(url)
                elif attempt == 1:  # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                    break
                else:
                    logger.warning(f"HTTP –æ—à–∏–±–∫–∞ {e.response.status_code} –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt + 1} –¥–ª—è {url}")
                    continue
            except Exception as e:
                if attempt == 1:  # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                    logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {e}")
                    break
                else:
                    logger.warning(f"–û—à–∏–±–∫–∞ –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt + 1} –¥–ª—è {url}: {e}")
                    continue

        # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ—É–¥–∞—á–Ω—ã, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º fallback
        return self._create_fallback_response(url)

    def _extract_title(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        # –ü–æ—Ä—è–¥–æ–∫ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        title_selectors = [
            # POLITICO —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            '.headline',
            '.story-meta__headline',
            # CNN —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            '.article__headline',
            '.headline__text',
            # –û–±—â–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            'h1',
            '[property="og:title"]',
            '[name="title"]',
            'h1.title',
            '.article-title',
            '.post-title',
            'title'
        ]

        for selector in title_selectors:
            try:
                if selector.startswith('['):
                    element = soup.find(attrs={'property': 'og:title'} if 'og:title' in selector
                                         else {'name': 'title'})
                else:
                    element = soup.select_one(selector)

                if element and element.text.strip():
                    return element.text.strip()
            except:
                continue

        return "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"

    def _extract_description(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        # –ü–æ—Ä—è–¥–æ–∫ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏–π
        desc_selectors = [
            # Reuters —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ –º–µ—Ç–∞-—Ç–µ–≥–∏)
            '[property="og:description"]',
            '[name="description"]',
            'meta[name="description"]',
            '[data-testid="ArticleBody"] p',
            # POLITICO —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            '.story-text p',
            '.content-group p',
            '.story-text__paragraph',
            # New York Times —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            'section[name="articleBody"] p',
            '.StoryBodyCompanionColumn p',
            '.css-53u6y8 p',  # NYT article body class
            '[data-module="ArticleBody"] p',
            '.g-body p',
            '.story-content p',
            # CNN —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã (–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ)
            '.article__content p',
            '.article-body p',
            '.zn-body__paragraph',
            '.el__leafmedia__body p',
            '.pg-rail-tall__body p',
            # –û–±—â–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å–∞–π—Ç–æ–≤
            '.article-content p',
            '.post-content p',
            '.entry-content p',
            'article p',
            '.story-body p',
            '[class*="content"] p',
            'p'
        ]

        for selector in desc_selectors:
            try:
                if selector.startswith('['):
                    if 'og:description' in selector:
                        element = soup.find(attrs={'property': 'og:description'})
                    elif 'name="description"' in selector:
                        element = soup.find(attrs={'name': 'description'})
                else:
                    # –î–ª—è —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤ —Å p - —Å–æ–±–∏—Ä–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∞–±–∑–∞—Ü–µ–≤
                    if selector.endswith(' p'):
                        elements = soup.select(selector)
                        if elements:
                            paragraphs = []
                            for elem in elements[:5]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 5 –∞–±–∑–∞—Ü–µ–≤
                                text = elem.text.strip()
                                if text and len(text) > 30 and not text.startswith(('Advertisement', 'Ad', 'Subscribe', 'Follow')):
                                    paragraphs.append(text)
                                if len(' '.join(paragraphs)) > 800:  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                                    break
                            if paragraphs:
                                content = ' '.join(paragraphs)
                                if len(content) > 100:  # –ú–∏–Ω–∏–º—É–º 100 —Å–∏–º–≤–æ–ª–æ–≤
                                    return content[:1500]  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç
                    else:
                        element = soup.select_one(selector)

                if element and not selector.endswith(' p'):
                    if selector.startswith('[') and ('og:description' in selector or 'name="description"' in selector):
                        # –î–ª—è –º–µ—Ç–∞-—Ç–µ–≥–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞—Ç—Ä–∏–±—É—Ç content
                        content = element.get('content', '')
                    else:
                        # –î–ª—è –æ–±—ã—á–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç
                        content = element.text.strip() if hasattr(element, 'text') else ''
                    if content and len(content) > 50:  # –ú–∏–Ω–∏–º—É–º 50 —Å–∏–º–≤–æ–ª–æ–≤
                        return content[:1000]  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã
            except:
                continue

        return "–ë–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è"

    def _extract_date(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"""
        date_selectors = [
            '[property="article:published_time"]',
            'time[datetime]',
            '.published',
            '.date',
            '[class*="date"]',
            'meta[property="article:published_time"]'
        ]

        for selector in date_selectors:
            try:
                if selector.startswith('['):
                    element = soup.find(attrs={'property': 'article:published_time'})
                else:
                    element = soup.select_one(selector)

                if element:
                    date_str = element.get('datetime') or element.get('content') or element.text.strip()
                    if date_str:
                        # –ü–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –¥–∞—Ç
                        try:
                            # ISO format
                            if 'T' in date_str:
                                return datetime.fromisoformat(date_str.replace('Z', '+00:00')).isoformat()
                            # –î—Ä—É–≥–∏–µ —Ñ–æ—Ä–º–∞—Ç—ã –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å
                        except:
                            pass
            except:
                continue

        return datetime.now().isoformat()

    def _extract_images(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, GIF –∏ –≤–∏–¥–µ–æ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        media_files = []

        # Open Graph –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        og_image = soup.find(attrs={'property': 'og:image'})
        if og_image and og_image.get('content'):
            media_files.append(urljoin(base_url, og_image['content']))

        # Open Graph –≤–∏–¥–µ–æ
        og_video = soup.find(attrs={'property': 'og:video'})
        if og_video and og_video.get('content'):
            video_url = urljoin(base_url, og_video['content'])
            if video_url not in media_files:
                media_files.append(video_url)

        # Twitter Card –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        twitter_image = soup.find(attrs={'name': 'twitter:image'})
        if twitter_image and twitter_image.get('content'):
            twitter_url = urljoin(base_url, twitter_image['content'])
            if twitter_url not in media_files:
                media_files.append(twitter_url)

        # Twitter Card –≤–∏–¥–µ–æ
        twitter_video = soup.find(attrs={'name': 'twitter:player:stream'})
        if twitter_video and twitter_video.get('content'):
            video_url = urljoin(base_url, twitter_video['content'])
            if video_url not in media_files:
                media_files.append(video_url)

        # –í–∏–¥–µ–æ —ç–ª–µ–º–µ–Ω—Ç—ã HTML5
        if len(media_files) < 3:
            video_elements = soup.select('video source, video')
            for video in video_elements[:3-len(media_files)]:
                src = video.get('src')
                if src:
                    full_url = urljoin(base_url, src)
                    if full_url not in media_files:
                        media_files.append(full_url)

        # –û—Å–Ω–æ–≤–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ GIF —Å—Ç–∞—Ç—å–∏
        if len(media_files) < 3:  # –ú–∞–∫—Å–∏–º—É–º 3 –º–µ–¥–∏–∞ —Ñ–∞–π–ª–∞
            article_media = soup.select('article img, .content img, .post img, article video, .content video')
            for media in article_media[:3-len(media_files)]:
                src = media.get('src') or media.get('data-src')
                if src:
                    full_url = urljoin(base_url, src)
                    if full_url not in media_files:
                        media_files.append(full_url)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ GIF —Ñ–∞–π–ª–æ–≤
        if len(media_files) < 3:
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ GIF —Ñ–∞–π–ª—ã
            gif_links = soup.find_all('a', href=re.compile(r'\.gif(\?|$)', re.IGNORECASE))
            for link in gif_links[:3-len(media_files)]:
                href = link.get('href')
                if href:
                    full_url = urljoin(base_url, href)
                    if full_url not in media_files:
                        media_files.append(full_url)

        logger.info(f"üì∏ –ù–∞–π–¥–µ–Ω–æ {len(media_files)} –º–µ–¥–∏–∞ —Ñ–∞–π–ª–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")
        return media_files

    def _extract_source(self, url: str, soup: BeautifulSoup) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏"""
        parsed_url = urlparse(url)
        domain = parsed_url.netloc.lower()

        # –°–ª–æ–≤–∞—Ä—å –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        sources_map = {
            'bbc.com': 'BBC News',
            'cnn.com': 'CNN',
            'reuters.com': 'Reuters',
            'nytimes.com': 'New York Times',
            'washingtonpost.com': 'Washington Post',
            'foxnews.com': 'Fox News',
            'nbcnews.com': 'NBC News',
            'politico.com': 'POLITICO',
            'politico.eu': 'POLITICO',
            'twitter.com': 'Twitter',
            'x.com': 'Twitter/X',
            'facebook.com': 'Facebook',
            'instagram.com': 'Instagram',
            't.me': 'Telegram'
        }

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–æ–º–µ–Ω–∞
        if domain in sources_map:
            return sources_map[domain]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        for key, value in sources_map.items():
            if key in domain:
                return value

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–∑ title –∏–ª–∏ –¥—Ä—É–≥–æ–≥–æ –º–µ—Å—Ç–∞
        title_tag = soup.find('title')
        if title_tag:
            title_text = title_tag.text.strip()
            # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –Ω–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∏–∑ title
            if ' - ' in title_text:
                return title_text.split(' - ')[-1][:50]

        return domain

    def _parse_apnews_direct(self, url: str) -> Dict:
        """–ü—Ä—è–º–æ–π –ø–∞—Ä—Å–∏–Ω–≥ AP News —á–µ—Ä–µ–∑ HTTP –∑–∞–ø—Ä–æ—Å"""
        try:
            import requests
            from bs4 import BeautifulSoup
            import re
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
            
            logger.info(f"üåê –ü—Ä—è–º–æ–π HTTP –∑–∞–ø—Ä–æ—Å –∫ AP News: {url}")
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ
            content = response.text
            if not content or len(content) < 1000 or not ('<html' in content.lower() or '<!doctype' in content.lower()):
                # –ö–æ–Ω—Ç–µ–Ω—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–∂–∞—Ç (Brotli, gzip)
                try:
                    import brotli
                    if response.headers.get('content-encoding') == 'br':
                        content = brotli.decompress(response.content).decode('utf-8', errors='ignore')
                    else:
                        content = response.content.decode('utf-8', errors='ignore')
                except ImportError:
                    content = response.content.decode('utf-8', errors='ignore')
            
            soup = BeautifulSoup(content, 'html.parser')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title = soup.find('meta', property='og:title')
            title = title.get('content') if title else soup.find('title')
            title = title.get_text() if hasattr(title, 'get_text') else str(title)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
            description = soup.find('meta', property='og:description')
            description = description.get('content') if description else ''
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            images = []
            og_image = soup.find('meta', property='og:image')
            if og_image and og_image.get('content'):
                images.append(og_image['content'])
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–∏–¥–µ–æ (JW Player)
            videos = []
            text = content
            
            # –ò—â–µ–º JW Player –≤–∏–¥–µ–æ
            jwplayer_pattern = r'https://cdn\.jwplayer\.com/videos/[^"\s<>]+\.mp4'
            jwplayer_matches = re.findall(jwplayer_pattern, text, re.IGNORECASE)
            for video_url in jwplayer_matches:
                videos.append(video_url)
                logger.info(f"üé• –ù–∞–π–¥–µ–Ω–æ JW Player –≤–∏–¥–µ–æ: {video_url}")
            
            # –ò—â–µ–º –¥—Ä—É–≥–∏–µ CDN –≤–∏–¥–µ–æ
            cdn_pattern = r'https://[^"\s<>]*\.(?:mp4|webm|mov)(?:\?[^"\s<>]*)?'
            cdn_matches = re.findall(cdn_pattern, text, re.IGNORECASE)
            for video_url in cdn_matches:
                if video_url not in videos:  # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
                    videos.append(video_url)
                    logger.info(f"üé• –ù–∞–π–¥–µ–Ω–æ CDN –≤–∏–¥–µ–æ: {video_url}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É
            date = soup.find('meta', property='article:published_time')
            date = date.get('content') if date else ''
            
            result = {
                'success': True,
                'title': title,
                'description': description,
                'source': 'Associated Press',
                'url': url,
                'images': images,
                'videos': videos,
                'published': date,
                'publish_date': date
            }
            
            logger.info(f"‚úÖ AP News –ø—Ä—è–º–æ–π –ø–∞—Ä—Å–∏–Ω–≥: –Ω–∞–π–¥–µ–Ω–æ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {len(videos)} –≤–∏–¥–µ–æ")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä—è–º–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ AP News: {e}")
            return {'success': False, 'error': str(e)}

    def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π"""
        if self.driver:
            try:
                # –°–Ω–∞—á–∞–ª–∞ –∑–∞–∫—Ä—ã–≤–∞–µ–º –≤—Å–µ –æ–∫–Ω–∞
                self.driver.quit()
                logger.info("Selenium WebDriver –∑–∞–∫—Ä—ã—Ç")
            except Exception as e:
                # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –∑–∞–∫—Ä—ã—Ç–∏—è - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ
                logger.debug(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ WebDriver (–Ω–æ—Ä–º–∞–ª—å–Ω–æ): {e}")
            finally:
                self.driver = None

    def __del__(self):
        """–î–µ—Å—Ç—Ä—É–∫—Ç–æ—Ä"""
        try:
            self.close()
        except Exception:
            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –≤ –¥–µ—Å—Ç—Ä—É–∫—Ç–æ—Ä–µ
            pass

def test_web_parser():
    """–¢–µ—Å—Ç–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–∞—Ä—Å–µ—Ä–∞"""
    config_path = os.path.join(os.path.dirname(__file__), '..', 'config', 'config.yaml')

    parser = WebParser(config_path)

    # –¢–µ—Å—Ç–æ–≤—ã–µ URL
    test_urls = [
        "https://www.bbc.com/news/world-us-canada-67443258",
        "https://twitter.com/elonmusk/status/1234567890",
        "https://www.cnn.com/2024/01/15/politics/trump-immunity-supreme-court/index.html"
    ]

    for url in test_urls:
        print(f"\n{'='*60}")
        print(f"–ü–∞—Ä—Å–∏–Ω–≥: {url}")
        print('='*60)

        result = parser.parse_url(url)
        print(f"Success: {result.get('success', False)}")
        print(f"Title: {result.get('title', 'N/A')}")
        print(f"Source: {result.get('source', 'N/A')}")
        print(f"Description length: {len(result.get('description', ''))}")
        print(f"Images found: {len(result.get('images', []))}")

        if result.get('error'):
            print(f"Error: {result['error']}")

    parser.close()

if __name__ == "__main__":
    test_web_parser()
