#!/usr/bin/env python3
"""
–ú–æ–¥—É–ª—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ LLM –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —à–æ—Ä—Ç—Å–æ–≤
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã: Gemini, OpenAI, Anthropic
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ–≤—ã–π Google AI SDK genai —Å Google Search Grounding
"""

import os
import json
import logging
from typing import Dict, List, Optional, Any
import yaml
from datetime import datetime

from scripts.llm_direct_provider import GeminiDirectProvider
from logger_config import logger
from scripts.llm_base import LLMProvider

# –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–π SDK
try:
    from google import genai
    from google.genai import types
    USE_NEW_SDK = True
    logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–æ–≤—ã–π Google AI SDK (genai)")
except ImportError:
    USE_NEW_SDK = False
    logger.warning("–ù–æ–≤—ã–π SDK google.genai –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–∫—Ç–æ–≤ –±—É–¥–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.")


class GeminiProvider(LLMProvider):
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è Google Gemini"""

    def __init__(self, api_key: str, model: str = "models/gemini-2.0-flash", config: Dict = None):
        self.api_key = api_key
        self.model_name = model
        self.config = config or {}
        self.direct_provider = None
        self.grounding_config = self.config.get('grounding', {})
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º grounding_is_available –¥–ª—è –≤—Å–µ—Ö –ø—É—Ç–µ–π
        self.grounding_is_available = self.grounding_config.get('enable_fact_checking', False)
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º force_direct_api –¥–ª—è –≤—Å–µ—Ö –ø—É—Ç–µ–π
        self.force_direct_api = self.config.get('force_direct_api', False)

        if self.force_direct_api:
            logger.info("‚ö° –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä—è–º–æ–π API –≤—ã–∑–æ–≤ (force_direct_api: true)")
            self.direct_provider = GeminiDirectProvider(api_key=self.api_key, model=self.model_name, config=self.config)
            if self.grounding_is_available:
                logger.info("‚úÖ Google Search Grounding –¥–æ—Å—Ç—É–ø–µ–Ω (—á–µ—Ä–µ–∑ –ø—Ä—è–º–æ–π API)")
            else:
                logger.info("‚úÖ Google Search Grounding –æ—Ç–∫–ª—é—á–µ–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
        else:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º SDK
            self._initialize_sdk(api_key)
            self.model = self.model_name  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            logger.info("‚úÖ Gemini –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å SDK")
            if self.grounding_is_available:
                 logger.info("‚úÖ Google Search Grounding –¥–æ—Å—Ç—É–ø–µ–Ω")
            else:
                logger.info("‚úÖ Google Search Grounding –æ—Ç–∫–ª—é—á–µ–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")

    def _initialize_sdk(self, api_key: str):
        try:
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º Google AI Studio –≤–º–µ—Å—Ç–æ Vertex AI
            import os
            os.environ['GOOGLE_GENAI_USE_VERTEXAI'] = 'False'
            logger.info("üîß –£—Å—Ç–∞–Ω–æ–≤–∏–ª–∏ GOOGLE_GENAI_USE_VERTEXAI=False –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è AI Studio")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∏–µ–Ω—Ç –¥–ª—è Google AI Studio
            self.client = genai.Client(api_key=api_key)
            
            # –ë–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
            self.generation_config = types.GenerateContentConfig(
                temperature=self.config.get('temperature', 0.7),
                max_output_tokens=self.config.get('max_tokens', 2000),
            )
            
            # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å Google Search Grounding
            if self.grounding_is_available:
                self.grounding_config = types.GenerateContentConfig(
                    tools=[types.Tool(google_search=types.GoogleSearch())],
                    temperature=self.config.get('grounding', {}).get('grounding_temperature', 0.3),
                )
                logger.info("‚úÖ Google Search Grounding (–ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–∫—Ç–æ–≤) –¥–æ—Å—Ç—É–ø–µ–Ω —á–µ—Ä–µ–∑ SDK.")
            else:
                self.grounding_config = None
        except Exception as e:
            logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ SDK genai: {e}. –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –ø—Ä—è–º–æ–π API.")
            self.force_direct_api = True
            self._initialize_direct_provider(self.api_key, self.model_name, self.config)

    def _initialize_direct_provider(self, api_key, model, config):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä—è–º–æ–≥–æ API –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
        try:
            self.direct_provider = GeminiDirectProvider(api_key, model, config)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä—è–º–æ–≥–æ API: {e}")
            self.direct_provider = None

    def _verify_facts_with_search(self, news_text: str, context: str = "") -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–∫—Ç–æ–≤ —á–µ—Ä–µ–∑ Google Search Grounding"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤–∫–ª—é—á–µ–Ω–∞ –ª–∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–∫—Ç–æ–≤
        if not self.grounding_is_available:
            logger.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–∫—Ç–æ–≤ –æ—Ç–∫–ª—é—á–µ–Ω–∞ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
            return {
                'fact_check': {
                    'accuracy_score': 1.0,
                    'issues_found': [],
                    'corrections': [],
                    'verification_status': 'skipped'
                },
                'grounding_data': {'chunks': [], 'supports': []},
                'verification_sources': []
            }

        # –ï—Å–ª–∏ –Ω–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∫–∏ grounding, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –ø—Ä–æ–≤–µ—Ä–∫—É
        if not self.force_direct_api and (not USE_NEW_SDK or not self.grounding_config):
            logger.info("Google Search Grounding –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –ø—Ä–æ–≤–µ—Ä–∫—É")
            return self._basic_fact_check(news_text, context)

        prompt = f"""
        –ü—Ä–æ–≤–µ—Ä—å —Ñ–∞–∫—Ç—ã –≤ —ç—Ç–æ–π –Ω–æ–≤–æ—Å—Ç–∏ –∏ —É–±–µ–¥–∏—Å—å –≤ –∏—Ö –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏.
        –û—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –æ–±—Ä–∞—Ç–∏ –Ω–∞:
        - –¢–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ–∏–≥—É—Ä (–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç, —ç–∫—Å–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç –∏ —Ç.–¥.)
        - –ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –¥–∞—Ç –∏ —Å–æ–±—ã—Ç–∏–π
        - –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–π –∏ —Ç–µ—Ä–º–∏–Ω–æ–≤

        –ù–æ–≤–æ—Å—Ç—å: {news_text}

        {f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}" if context else ""}

        –í–µ—Ä–Ω–∏ –∞–Ω–∞–ª–∏–∑ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
        {{
            "fact_check": {{
                "accuracy_score": 0.0-1.0,
                "issues_found": ["—Å–ø–∏—Å–æ–∫ –ø—Ä–æ–±–ª–µ–º"],
                "corrections": ["—Å–ø–∏—Å–æ–∫ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π"],
                "verification_status": "verified|needs_check|incorrect"
            }},
            "current_context": {{
                "key_figures_status": {{"–∏–º—è": "—Ç–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å"}},
                "recent_events": ["–≤–∞–∂–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è"],
                "breaking_news": "–∫—Ä–∏—Ç–∏—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"
            }}
        }}
        """

        try:
            # –ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω –ø—Ä—è–º–æ–π –≤—ã–∑–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –ø—Ä–æ–≤–µ—Ä–∫—É
            if self.force_direct_api:
                logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –ø—Ä–æ–≤–µ—Ä–∫—É —Ñ–∞–∫—Ç–æ–≤ (–ø—Ä—è–º–æ–π API)")
                return self._basic_fact_check(news_text, context)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π API —Å–æ–≥–ª–∞—Å–Ω–æ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config=self.grounding_config
            )

            # –ò–∑–≤–ª–µ–∫–∞–µ–º grounding –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            grounding_data = {
                'chunks': [],
                'supports': [],
                'search_entry_point': None
            }

            if hasattr(response.candidates[0], 'grounding_metadata'):
                metadata = response.candidates[0].grounding_metadata
                if metadata.grounding_chunks:
                    grounding_data['chunks'] = [
                        {
                            'title': chunk.web.title,
                            'uri': chunk.web.uri,
                            'content': getattr(chunk.web, 'snippet', '')
                        } for chunk in metadata.grounding_chunks
                    ]
                if metadata.grounding_supports:
                    grounding_data['supports'] = [
                        {
                            'confidence_scores': support.confidence_scores,
                            'segment': {
                                'start_index': support.segment.start_index,
                                'end_index': support.segment.end_index,
                                'text': support.segment.text
                            } if support.segment else None
                        } for support in metadata.grounding_supports
                    ]

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º parts –∫–∞–∫ —Å–ø–∏—Å–æ–∫
            parts = response.candidates[0].content.parts
            if isinstance(parts, list):
                result_text = parts[0].text
            else:
                result_text = parts.text
            
            # –ü—ã—Ç–∞–µ–º—Å—è –ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ JSON, –µ—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è - —Å–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            try:
                fact_check_data = json.loads(result_text)
                logger.info("‚úÖ –ü–æ–ª—É—á–µ–Ω JSON –æ—Ç–≤–µ—Ç –æ—Ç Google Search Grounding")
            except json.JSONDecodeError:
                logger.info("üìù –ü–æ–ª—É—á–µ–Ω —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –æ—Ç Google Search Grounding, —Å–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É")
                # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –∏ grounding –¥–∞–Ω–Ω—ã—Ö
                accuracy_score = 0.8 if grounding_data['chunks'] else 0.5
                fact_check_data = {
                    'fact_check': {
                        'accuracy_score': accuracy_score,
                        'verification_status': 'verified' if grounding_data['chunks'] else 'needs_check',
                        'corrections': self._extract_corrections_from_text(result_text),
                        'verified_facts': result_text[:500],  # –ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∫ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ —Ñ–∞–∫—Ç—ã
                        'confidence': 'high' if grounding_data['chunks'] else 'medium'
                    }
                }

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä–æ–≥ —Ç–æ—á–Ω–æ—Å—Ç–∏
            accuracy_score = fact_check_data.get('fact_check', {}).get('accuracy_score', 0.5)
            if accuracy_score < self.config.get('grounding', {}).get('fact_check_threshold', 0.7):
                logger.warning(f"–ù–∏–∑–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å —Ñ–∞–∫—Ç–æ–≤: {accuracy_score}")

            return {
                'fact_check': fact_check_data['fact_check'],
                'grounding_data': grounding_data,
                'verification_sources': grounding_data['chunks'],
                'grounding_text': result_text  # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
            }

        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤ —á–µ—Ä–µ–∑ Google Search: {e}")
            return {
                'fact_check': {
                    'accuracy_score': 0.5,
                    'issues_found': ['–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ñ–∞–∫—Ç—ã'],
                    'corrections': [],
                    'verification_status': 'needs_check'
                },
                'grounding_data': {'chunks': [], 'supports': []},
                'verification_sources': []
            }
    
    def _extract_corrections_from_text(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        corrections = []
        
        # –ü—Ä–æ—Å—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π
        correction_patterns = [
            r"–¥–µ–π—Å—Ç–≤—É—é—â–∏–π –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç",
            r"–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç —Å—à–∞.*—Ç—Ä–∞–º–ø",
            r"—Ç—Ä–∞–º–ø.*–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç",
            r"–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ",
            r"–Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ",
            r"—Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏"
        ]
        
        import re
        for pattern in correction_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                corrections.append(f"–ù–∞–π–¥–µ–Ω–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É: {pattern}")
        
        return corrections

    def _basic_fact_check(self, news_text: str, context: str = "") -> Dict[str, Any]:
        """–ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–∫—Ç–æ–≤ –±–µ–∑ Google Search Grounding"""
        prompt = f"""
        –¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø—Ä–æ–≤–µ—Ä–∫–µ —Ñ–∞–∫—Ç–æ–≤. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –Ω–æ–≤–æ—Å—Ç—å –Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å –∏ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å.
        
        –ó–ê–î–ê–ß–ê: –ù–∞–π–¥–∏ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ –≤ —Ç–µ–∫—Å—Ç–µ, –æ—Å–æ–±–µ–Ω–Ω–æ:
        - –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –∏–ª–∏ —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ —Ç–∏—Ç—É–ª—ã –∏ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏
        - –£—Å—Ç–∞—Ä–µ–≤—à—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–∫—É—â–∏—Ö —Å–æ–±—ã—Ç–∏—è—Ö
        - –õ–æ–≥–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è
        - –ù–µ—Ç–æ—á–Ω–æ—Å—Ç–∏ –≤ –¥–∞—Ç–∞—Ö –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞–º–∫–∞—Ö
        
        –ö–û–ù–¢–ï–ö–°–¢: –¢–µ–∫—É—â–∏–π –≥–æ–¥ - 2025. –ü—Ä–æ–≤–µ—Ä—å –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –≤—Å–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.
        
        –¢–ï–ö–°–¢: {news_text}
        
        –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON (–±–µ–∑ markdown, –±–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤):
        {{
            "fact_check": {{
                "accuracy_score": 0.9,
                "issues_found": ["–Ω–∞–π–¥–µ–Ω–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞"],
                "corrections": ["–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"],
                "verification_status": "verified"
            }}
        }}
        """

        try:
            # –ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω –ø—Ä—è–º–æ–π –≤—ã–∑–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
            if self.force_direct_api and self.direct_provider:
                response_text = self.direct_provider._call_gemini_api(prompt, temperature=0.3)
                if not response_text:
                    raise ValueError("–ü—Ä—è–º–æ–π –≤—ã–∑–æ–≤ API –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤.")
                # –£–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ markdown-–æ–±–µ—Ä—Ç–∫–∏
                if response_text.startswith('```json'):
                    response_text = response_text[7:]
                if response_text.endswith('```'):
                    response_text = response_text[:-3]
                result = json.loads(response_text.strip())

            elif USE_NEW_SDK and hasattr(self, 'client') and self.client:
                response = self.client.models.generate_content(
                    model=self.model,
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        temperature=0.3,
                        response_mime_type="application/json"
                    )
                )
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º parts –∫–∞–∫ —Å–ø–∏—Å–æ–∫
                parts = response.candidates[0].content.parts
                if isinstance(parts, list):
                    text = parts[0].text.strip()
                else:
                    text = parts.text.strip()
                result = json.loads(text)
            else:
                # –°—Ç–∞—Ä—ã–π SDK
                response = self.client.generate_content(
                    prompt,
                    generation_config=genai.GenerationConfig(
                        temperature=0.3
                    )
                )
                # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
                text_response = response.text.strip()
                # –£–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ markdown-–æ–±–µ—Ä—Ç–∫–∏
                if text_response.startswith('```json'):
                    text_response = text_response[7:]
                if text_response.endswith('```'):
                    text_response = text_response[:-3]
                result = json.loads(text_response.strip())

            return {
                'fact_check': result['fact_check'],
                'grounding_data': {'chunks': [], 'supports': []},
                'verification_sources': []
            }

        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –±–∞–∑–æ–≤–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤: {e}")
            return {
                'fact_check': {
                    'accuracy_score': 0.5,
                    'issues_found': ['–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É'],
                    'corrections': [],
                    'verification_status': 'uncertain'
                },
                'grounding_data': {'chunks': [], 'supports': []},
                'verification_sources': []
            }

    def summarize_for_video(self, text: str, context: str = "") -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –≤–∏–¥–µ–æ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ñ–∞–∫—Ç–æ–≤"""
        
        # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä—è–º–æ–π API, –¥–µ–ª–µ–≥–∏—Ä—É–µ–º –≤—ã–∑–æ–≤
        if self.force_direct_api and self.direct_provider:
            return self.direct_provider.summarize_for_video(text)
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–∫—Ç—ã
        fact_check = self._verify_facts_with_search(text, context)

        # –£–ª—É—á—à–∞–µ–º –ø—Ä–æ–º–ø—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤
        corrections_text = ""
        if fact_check.get('fact_check', {}).get('issues_found'):
            corrections_text = f"""
            –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø (–æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫–µ —Ñ–∞–∫—Ç–æ–≤):
            {chr(10).join(f"- {issue}" for issue in fact_check['fact_check']['issues_found'])}

            –ü–†–ê–í–ò–õ–¨–ù–´–ï –§–ê–ö–¢–´:
            {chr(10).join(f"- {correction}" for correction in fact_check['fact_check'].get('corrections', []))}
            """
        elif fact_check.get('fact_check', {}).get('corrections'):
            corrections_text = f"""
            –ü–†–ê–í–ò–õ–¨–ù–´–ï –§–ê–ö–¢–´ (–æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫–µ —Ñ–∞–∫—Ç–æ–≤):
            {chr(10).join(f"- {correction}" for correction in fact_check['fact_check']['corrections'])}
            """

        prompt = f"""
        Summarize the following news into a clear, neutral, and informative script for a short video.

        Requirements:
        - Language: English, for an international audience
        - Style: professional, factual, neutral, news-report tone
        - Length: 400‚Äì600 characters (about 4‚Äì6 sentences)
        - Must cover: WHO, WHAT, WHEN, WHERE, WHY, and IMPACT
        - If the news contains a direct quote, include it exactly as written and in quotation marks
        - Focus on the main thesis of the news, avoid speculation or misleading interpretation
        - Provide necessary context and significance in a concise way
        - Write in complete sentences, with smooth narrative flow
        - No abbreviations, no subjective language, no cut-off sentences
        - No emojis, no hashtags, no social media style

        {corrections_text}

        News: {text}

        Final factual short video summary in English:
        """

        try:
            if self.force_direct_api:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º–æ–π API –ø—Ä–æ–≤–∞–π–¥–µ—Ä
                return self.direct_provider.summarize_for_video(text)
            elif USE_NEW_SDK and self.client:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π SDK
                response = self.client.models.generate_content(
                    model=self.model,
                    contents=prompt,
                    config=self.generation_config
                )
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º parts –∫–∞–∫ —Å–ø–∏—Å–æ–∫
                parts = response.candidates[0].content.parts
                if isinstance(parts, list):
                    return parts[0].text.strip()
                else:
                    return parts.text.strip()
            else:
                # Fallback
                return self._fallback_summarize(text)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –≤–∏–¥–µ–æ: {e}")
            return self._fallback_summarize(text)

    def _fallback_summarize(self, text: str) -> str:
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –º–µ—Ç–æ–¥ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"""
        if len(text) <= 150:
            return text
        return text[:147] + "..."

    def generate_seo_package(self, text: str, source_url: str = "") -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è SEO –ø–∞–∫–µ—Ç–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ñ–∞–∫—Ç–æ–≤"""
        
        # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä—è–º–æ–π API, –¥–µ–ª–µ–≥–∏—Ä—É–µ–º –≤—ã–∑–æ–≤
        if self.force_direct_api and self.direct_provider:
            return self.direct_provider.generate_seo_package(text, source_url)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–∫—Ç—ã –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π SEO
        fact_check = self._verify_facts_with_search(text)

        corrections_text = ""
        if fact_check.get('fact_check', {}).get('issues_found'):
            corrections_text = f"""
            –í–ê–ñ–ù–û: –£—á–∏—Ç—ã–≤–∞–π —ç—Ç–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ SEO –∫–æ–Ω—Ç–µ–Ω—Ç–∞:
            {chr(10).join(f"- {issue}" for issue in fact_check['fact_check']['issues_found'])}
            """
        elif fact_check.get('fact_check', {}).get('corrections'):
            corrections_text = f"""
            –í–ê–ñ–ù–û: –£—á–∏—Ç—ã–≤–∞–π —ç—Ç–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ SEO –∫–æ–Ω—Ç–µ–Ω—Ç–∞:
            {chr(10).join(f"- {correction}" for correction in fact_check['fact_check']['corrections'])}
            """

        prompt = f"""
        TASK: Create YouTube Shorts SEO package for international news content.

        OUTPUT FORMAT: JSON only, no explanations
        {{
          "title": "...",
          "description": "...",
          "tags": ["...", "..."]
        }}

        TITLE RULES:
        - Maximum 100 characters
        - Clickbait style but factual
        - Start with action words or shocking facts
        - NO generic words like 'news', 'update', 'breaking'
        - Examples: 'Putin meets Trump at Alaska hotel', 'Bitcoin crashes 40% in single day', 'Ukraine captures Russian general'

        DESCRIPTION RULES:
        - 1-2 SHORT sentences maximum
        - Add context or key details not in title
        - Can be empty if title says everything
        - Include 5 relevant hashtags at the end with # symbol

        TAGS RULES:
        - Exactly 12-15 tags
        - Mix of: specific names, general topics, emotions, locations
        - Single words or short phrases (max 3 words)
        - NO '#' symbols
        - Include: relevant people names, countries, topics, trending keywords
        - Examples: ['putin', 'trump', 'politics', 'russia', 'america', 'meeting', 'diplomacy', 'world news', 'leadership', 'international', 'alaska', 'summit']

        {corrections_text}

        SOURCE TEXT:
        {text}

        JSON:
        """

        try:
            if self.force_direct_api:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º–æ–π API –ø—Ä–æ–≤–∞–π–¥–µ—Ä
                return self.direct_provider.generate_seo_package(text, source_url)
            elif USE_NEW_SDK and self.client:
                # –ù–æ–≤—ã–π SDK
                config = types.GenerateContentConfig(
                    temperature=0.3,
                    response_mime_type="application/json"
                )
                response = self.client.models.generate_content(
                    model=self.model,
                    contents=prompt,
                    config=config
                )
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º parts –∫–∞–∫ —Å–ø–∏—Å–æ–∫
                parts = response.candidates[0].content.parts
                if isinstance(parts, list):
                    text = parts[0].text.strip()
                else:
                    text = parts.text.strip()
                try:
                    result = json.loads(text)
                except json.JSONDecodeError:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–∞—Ä—Å–∏—Ç—å JSON –≤ generate_seo_package: {text[:100]}...")
                    return self._fallback_seo_package(text)
            else:
                # Fallback
                return self._fallback_seo_package(text)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ result - —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å
            if not isinstance(result, dict):
                logger.warning(f"LLM –≤–µ—Ä–Ω—É–ª –Ω–µ —Å–ª–æ–≤–∞—Ä—å –≤ generate_seo_package: {type(result)}")
                return self._fallback_seo_package(text)

            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞
            title = (result.get('title') or '').strip()
            if not title:
                title = (text[:70] + '...') if len(text) > 70 else text

            # –î–æ–±–∞–≤–ª—è–µ–º —Ö–µ—à—Ç–µ–≥–∏ –∫ –∑–∞–≥–æ–ª–æ–≤–∫—É –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
            if '#' not in title and result.get('tags'):
                tags = result.get('tags', [])
                if isinstance(tags, str):
                    tags = [t.strip() for t in tags.split(',') if t.strip()]

                if tags:
                    title_hashtags = [f"#{tag.replace(' ', '')}" for tag in tags[:3]]
                    title_with_hashtags = f"{title} {' '.join(title_hashtags)}"

                    if len(title_with_hashtags) > 90:
                        max_title_len = 90 - len(' '.join(title_hashtags)) - 1
                        title = title[:max_title_len].rstrip() + "..."
                        title = f"{title} {' '.join(title_hashtags)}"
                    else:
                        title = title_with_hashtags

            description = (result.get('description') or '').strip()
            if len(description) > 280:
                description = description[:279].rstrip() + '...'

            tags = result.get('tags') or []
            if isinstance(tags, str):
                tags = [t.strip() for t in tags.split(',') if t.strip()]

            # –ú–∏–Ω–∏–º—É–º 5 —Ç–µ–≥–æ–≤
            base_tags = ['–Ω–æ–≤–∏–Ω–∏', 'news', 'shorts']
            tags = list(dict.fromkeys([*tags, *base_tags]))[:15]
            if len(tags) < 5:
                tags += ['updates', 'video', 'world']
                tags = tags[:15]

            return {
                'title': title,
                'description': description,
                'tags': tags,
                'category': result.get('category', '–ù–æ–≤–æ—Å—Ç–∏'),
                'fact_verification': fact_check['fact_check'],
                'sources_used': fact_check['verification_sources']
            }

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ SEO –ø–∞–∫–µ—Ç–∞ —á–µ—Ä–µ–∑ Gemini: {e}")
            return self._fallback_seo_package(text)

    def categorize_news(self, news_text: str) -> Dict[str, Any]:
        """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–∏"""
        categories = ["politics", "business", "technology", "sports", "entertainment", "health", "science"]

        prompt = f"""
        –û–ø—Ä–µ–¥–µ–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—é –¥–ª—è —ç—Ç–æ–π –Ω–æ–≤–æ—Å—Ç–∏. –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {', '.join(categories)}

        –ù–æ–≤–æ—Å—Ç—å: {news_text}

        –í–µ—Ä–Ω–∏ JSON –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
        {{
            "category": "–æ—Å–Ω–æ–≤–Ω–∞—è_–∫–∞—Ç–µ–≥–æ—Ä–∏—è",
            "confidence": 0.0-1.0,
            "tags": ["—Ç–µ–≥1", "—Ç–µ–≥2", "—Ç–µ–≥3"],
            "language": "—è–∑—ã–∫_–Ω–æ–≤–æ—Å—Ç–∏"
        }}
        """

        try:
            if USE_NEW_SDK:
                response = self.client.models.generate_content(
                    model=self.model,
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        temperature=0.3,
                        response_mime_type="application/json"
                    )
                )
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º parts –∫–∞–∫ —Å–ø–∏—Å–æ–∫
                parts = response.candidates[0].content.parts
                if isinstance(parts, list):
                    text = parts[0].text.strip()
                else:
                    text = parts.text.strip()
                result = json.loads(text)
            else:
                response = self.client.generate_content(
                    prompt,
                    generation_config=genai.GenerationConfig(temperature=0.3)
                )
                text_response = response.text.strip()
                if text_response.startswith('```json'):
                    text_response = text_response[7:]
                if text_response.endswith('```'):
                    text_response = text_response[:-3]
                result = json.loads(text_response.strip())

            return result

        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            return {
                "category": "general",
                "confidence": 0.5,
                "tags": ["news"],
                "language": "unknown"
            }

    def generate_structured_content(self, news_data: Dict) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –∞–Ω–∏–º–∞—Ü–∏–∏ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ñ–∞–∫—Ç–æ–≤"""
        
        # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä—è–º–æ–π API, –¥–µ–ª–µ–≥–∏—Ä—É–µ–º –≤—ã–∑–æ–≤
        if self.force_direct_api and self.direct_provider:
            return self.direct_provider.generate_structured_content(news_data)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–∫—Ç—ã –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        news_text = f"{news_data.get('title', '')}. {news_data.get('description', '')}"
        fact_check = self._verify_facts_with_search(news_text)

        corrections_text = ""
        if fact_check.get('fact_check', {}).get('issues_found'):
            corrections_text = f"""
            –í–ê–ñ–ù–´–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø –î–õ–Ø –ê–ù–ò–ú–ê–¶–ò–ò:
            {chr(10).join(f"- {issue}" for issue in fact_check['fact_check']['issues_found'])}
            """
        elif fact_check.get('fact_check', {}).get('corrections'):
            corrections_text = f"""
            –í–ê–ñ–ù–´–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø –î–õ–Ø –ê–ù–ò–ú–ê–¶–ò–ò:
            {chr(10).join(f"- {correction}" for correction in fact_check['fact_check']['corrections'])}
            """

        prompt = f"""
        Create structured content for YouTube Shorts animation based on the news.
        Return JSON in format:
        {{
            "header": {{
                "text": "Brief title for the top part - IN ENGLISH",
                "animation": "fadeIn",
                "duration": 1.5
            }},
            "body": {{
                "text": "Main news text - IN ENGLISH",
                "animation": "typewriter",
                "duration": 2.5
            }},
            "footer": {{
                "source": "News source",
                "date": "Date in DD.MM.YYYY format",
                "animation": "slideUp",
                "duration": 1.0
            }},
            "style": {{
                "theme": "dark|light",
                "accent_color": "#HEX_COLOR",
                "font_size": "medium"
            }}
        }}

        {corrections_text}

        –ò—Å—Ö–æ–¥–Ω–∞—è –Ω–æ–≤–æ—Å—Ç—å:
        –ó–∞–≥–æ–ª–æ–≤–æ–∫: {news_data.get('title', '')}
        –û–ø–∏—Å–∞–Ω–∏–µ: {news_data.get('description', '')}
        –ò—Å—Ç–æ—á–Ω–∏–∫: {news_data.get('source', '')}
        –î–∞—Ç–∞: {news_data.get('published', '')}
        """

        try:
            if self.force_direct_api:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º–æ–π API –ø—Ä–æ–≤–∞–π–¥–µ—Ä
                return self.direct_provider.generate_structured_content(news_data)
            elif USE_NEW_SDK and self.client:
                # –ù–æ–≤—ã–π SDK
                config = types.GenerateContentConfig(
                    temperature=0.5,
                    response_mime_type="application/json"
                )
                response = self.client.models.generate_content(
                    model=self.model,
                    contents=prompt,
                    config=config
                )
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º parts –∫–∞–∫ —Å–ø–∏—Å–æ–∫
                parts = response.candidates[0].content.parts
                if isinstance(parts, list):
                    text = parts[0].text.strip()
                else:
                    text = parts.text.strip()
                try:
                    result = json.loads(text)
                except json.JSONDecodeError:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–∞—Ä—Å–∏—Ç—å JSON –≤ generate_structured_content: {text[:100]}...")
                    return self._fallback_structured_content(news_data)
            else:
                # Fallback
                return self._fallback_structured_content(news_data)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ result - —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å
            if not isinstance(result, dict):
                logger.warning(f"LLM –≤–µ—Ä–Ω—É–ª –Ω–µ —Å–ª–æ–≤–∞—Ä—å –≤ generate_structured_content: {type(result)}")
                return self._fallback_structured_content(news_data)

            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            if not result.get('footer', {}).get('date'):
                # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É –∏–∑ published
                published = news_data.get('published', '')
                if isinstance(published, str) and 'T' in published:
                    try:
                        dt = datetime.fromisoformat(published.replace('Z', '+00:00'))
                        result['footer']['date'] = dt.strftime('%d.%m.%Y')
                    except:
                        result['footer']['date'] = datetime.now().strftime('%d.%m.%Y')
                else:
                    result['footer']['date'] = datetime.now().strftime('%d.%m.%Y')

            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–≤–µ—Ä–∫–µ —Ñ–∞–∫—Ç–æ–≤
            result['fact_verification'] = fact_check['fact_check']
            result['verification_sources'] = fact_check['verification_sources']

            return result

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ Gemini: {e}")
            return self._fallback_structured_content(news_data)

    def _fallback_seo_package(self, text: str) -> Dict[str, Any]:
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π SEO –ø–∞–∫–µ—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ LLM"""
        return {
            'title': f"Breaking: {text[:40]}..." if len(text) > 40 else f"Breaking: {text}",
            'description': '',
            'tags': ['news', 'breaking', 'shorts', 'updates', 'video'],
            'category': 'News & Politics'
        }

    def _fallback_structured_content(self, news_data: Dict) -> Dict[str, Any]:
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç"""
        return {
            "header": {
                "text": news_data.get('title', '')[:50] + "..." if len(news_data.get('title', '')) > 50 else news_data.get('title', ''),
                "animation": "fadeIn",
                "duration": 1.5
            },
            "body": {
                "text": news_data.get('description', '')[:100] + "..." if len(news_data.get('description', '')) > 100 else news_data.get('description', ''),
                "animation": "typewriter",
                "duration": 2.5
            },
            "footer": {
                "source": news_data.get('source', ''),
                "date": datetime.now().strftime('%d.%m.%Y'),
                "animation": "slideUp",
                "duration": 1.0
            },
            "style": {
                "theme": "dark",
                "accent_color": "#FF6B35",
                "font_size": "medium"
            }
        }

class LLMProcessor:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ LLM"""

    def __init__(self, config_path: str):
        self.config = self._load_config(config_path)
        self.provider = self._init_provider()
        # Validation/fact-guard rules from config (optional)
        self.validation_rules = (self.config.get('validation') or {}).get('rules', {})

    # ---------------------- FactGuard (lightweight) ----------------------
    def _fact_guard_normalize(self, text: str, published_iso: str = "") -> str:
        """Lightweight normalizer for official titles and common fact phrasing errors.

        Uses config-driven rules if provided. Falls back to safe defaults.
        This is NOT a hardcoded rewrite of content ‚Äì only safe, unambiguous
        replacements for well-known roles as of a given effective date.
        """
        if not text:
            return text

        try:
            import re
            normalized = text

            # 1) Roles from config (preferred)
            # Format example in config:
            # validation:
            #   rules:
            #     roles:
            #       - role: "President of the United States"
            #         person: "Donald Trump"
            #         effective_from: "2025-01-20"
            roles = (self.validation_rules.get('roles') or [])

            def date_ok(effective_from: str) -> bool:
                if not effective_from:
                    return True
                try:
                    from datetime import datetime
                    pub = datetime.fromisoformat(published_iso.replace('Z', '+00:00')) if published_iso else datetime.now()
                    eff = datetime.fromisoformat(effective_from + 'T00:00:00+00:00') if 'T' not in effective_from else datetime.fromisoformat(effective_from)
                    return pub >= eff
                except Exception:
                    return True

            for r in roles:
                role = (r.get('role') or '').lower()
                person = r.get('person') or ''
                eff = r.get('effective_from') or ''
                if not role or not person:
                    continue
                if not date_ok(eff):
                    continue
                # Common mislabels like "former <role> <person>" -> "<role> <person>"
                patterns = [
                    rf"former\s+{re.escape(person)}",
                    rf"former\s+{re.escape(role)}\s+{re.escape(person)}",
                    rf"ex[- ]{re.escape(role)}\s+{re.escape(person)}",
                    rf"ex[- ]{re.escape(person)}",
                ]
                for p in patterns:
                    normalized = re.sub(p, person, normalized, flags=re.IGNORECASE)
                # Also handle "former U.S. President Trump" -> "U.S. President Trump"
                alt_role = role.replace('president of the united states', 'u.s. president')
                patterns_alt = [
                    rf"former\s+{re.escape(alt_role)}\s+{re.escape(person)}",
                ]
                for p in patterns_alt:
                    normalized = re.sub(p, f"{alt_role} {person}", normalized, flags=re.IGNORECASE)

            # 2) Safe default for widely-known current role (if no config):
            if not roles:
                # As of 2025-01-20, Donald Trump is U.S. President
                # Normalize "former ... Trump" -> "President Trump"
                patterns_default = [
                    r"former\s+(u\.?s\.?\s+)?president\s+trump",
                    r"ex[- ](u\.?s\.?\s+)?president\s+trump",
                ]
                for p in patterns_default:
                    normalized = re.sub(p, "President Trump", normalized, flags=re.IGNORECASE)

            return normalized
        except Exception:
            return text

    def _load_config(self, config_path: str) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)

    def _init_provider(self) -> LLMProvider:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
        llm_config = self.config['llm']
        provider_name = llm_config['provider'].lower()

        if provider_name == 'gemini':
            api_key = os.getenv(llm_config['api_key_env'])
            if not api_key:
                raise ValueError(f"API –∫–ª—é—á –¥–ª—è {provider_name} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—á–∏—Å—Ç–∫—É –∫–ª—é—á–∞ –ø—Ä—è–º–æ –∑–¥–µ—Å—å
            api_key = api_key.strip()
            masked_key = f"{api_key[:4]}...{api_key[-4:]}"
            logger.info(f"üîë [DEBUG] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Gemini –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ —Å –∫–ª—é—á–æ–º: {masked_key}")

            return GeminiProvider(api_key, llm_config['model'], llm_config)

        elif provider_name == 'openai':
            # –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è OpenAI –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–∞ –ø–æ–∑–∂–µ
            raise NotImplementedError("OpenAI –ø—Ä–æ–≤–∞–π–¥–µ—Ä –ø–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω")

        elif provider_name == 'anthropic':
            # –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è Anthropic –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–∞ –ø–æ–∑–∂–µ
            raise NotImplementedError("Anthropic –ø—Ä–æ–≤–∞–π–¥–µ—Ä –ø–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω")

        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {provider_name}")

    def process_news_for_shorts(self, news_data: Dict) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è shorts –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ñ–∞–∫—Ç–æ–≤"""
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏: {news_data.get('title', '')[:50]}...")

        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ —Å –ø–æ–ª–Ω—ã–º –ø–∞–∫–µ—Ç–æ–º –¥–∞–Ω–Ω—ã—Ö
            if hasattr(self.provider, 'generate_complete_news_package'):
                logger.info("üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–∫–µ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö")
                complete_package = self.provider.generate_complete_news_package(news_data)
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                processed_data = {
                    'status': 'success',
                    'title': complete_package.get('content', {}).get('title', ''),
                    'summary': complete_package.get('content', {}).get('summary', ''),
                    'video_text': complete_package.get('content', {}).get('summary', ''),
                    'seo_package': complete_package.get('seo', {}),
                    'structured_content': complete_package,
                    'fact_check': complete_package.get('metadata', {}).get('fact_check', None)
                }
                
                logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω –ø–æ–ª–Ω—ã–π –ø–∞–∫–µ—Ç –¥–∞–Ω–Ω—ã—Ö —Å {complete_package.get('metadata', {}).get('confidence_score', 0):.1%} —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏")
                return processed_data
            
            # Fallback –∫ —Å—Ç–∞—Ä–æ–º—É –º–µ—Ç–æ–¥—É
            return self._process_news_legacy(news_data)

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏ ID {news_data.get('id')}: {e}")
            return {
                'news_id': news_data.get('id'),
                'status': 'error',
                'error': str(e),
                'processed_at': datetime.now().isoformat()
            }

    def _process_news_legacy(self, news_data: Dict) -> Dict[str, Any]:
        """–°—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        processed_data = {
            'status': 'success',
            'title': '',
            'summary': '',
            'video_text': '',
            'seo_package': {},
            'structured_content': {},
            'fact_check': None
        }

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        full_text = f"{news_data.get('description', '')}"

        # --- –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–∫—Ç–æ–≤ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞) ---
        if self.provider.grounding_is_available:
            logger.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–∫—Ç–æ–≤ –¥–ª—è –Ω–æ–≤–æ—Å—Ç–∏ {news_data.get('id', 'N/A')}...")
            fact_check_result = self.provider._verify_facts_with_search(full_text)
            processed_data['fact_check'] = fact_check_result

        # –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤ (–∏—Å—Ç–æ—á–Ω–∏–∫, –¥–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏)
        context = f"""
        –ò—Å—Ç–æ—á–Ω–∏–∫: {news_data.get('source', '')}
        –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {news_data.get('published', '')}
        –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {news_data.get('category', '')}
        """

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫—Ä–∞—Ç–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –≤–∏–¥–µ–æ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ñ–∞–∫—Ç–æ–≤
        try:
            short_text = self.provider.summarize_for_video(full_text, context)
            # Normalize roles/titles to avoid "former President Trump" kind of errors
            short_text = self._fact_guard_normalize(short_text, news_data.get('published', ''))
            logger.info(f"  ‚úÖ LLM summarize_for_video —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {len(short_text) if short_text else 0} —Å–∏–º–≤–æ–ª–æ–≤")
            processed_data['video_text'] = short_text
            processed_data['summary'] = short_text  # –î—É–±–ª–∏—Ä—É–µ–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        except Exception as e:
            logger.error(f"  ‚ùå –û—à–∏–±–∫–∞ summarize_for_video: {e}")
            # –°–æ–∑–¥–∞–µ–º –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è (–ø–µ—Ä–≤—ã–µ 300 —Å–∏–º–≤–æ–ª–æ–≤)
            description = news_data.get('description', 'Brief news summary')
            processed_data['video_text'] = description[:300] + ('...' if len(description) > 300 else '')
            processed_data['summary'] = processed_data['video_text']

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è SEO –ø–∞–∫–µ—Ç–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ñ–∞–∫—Ç–æ–≤
        try:
            source_url = news_data.get('url', '')
            seo_package = self.provider.generate_seo_package(full_text, source_url)
            logger.info(f"  ‚úÖ LLM generate_seo_package —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {bool(seo_package)}")
            processed_data['seo_package'] = seo_package
            if seo_package and 'title' in seo_package:
                # Normalize title as well
                processed_data['title'] = self._fact_guard_normalize(seo_package['title'], news_data.get('published', ''))
                logger.info(f"  ‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫: {processed_data['title'][:50]}...")
            else:
                # –°–æ–∑–¥–∞–µ–º –∫—Ä–∞—Ç–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ (–ø–µ—Ä–≤—ã–µ 60 —Å–∏–º–≤–æ–ª–æ–≤)
                original_title = news_data.get('title', '–ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–æ–≤–æ—Å—Ç–∏')
                processed_data['title'] = original_title[:60] + ('...' if len(original_title) > 60 else '')
        except Exception as e:
            logger.error(f"  ‚ùå –û—à–∏–±–∫–∞ generate_seo_package: {e}")
            # –°–æ–∑–¥–∞–µ–º –∫—Ä–∞—Ç–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ (–ø–µ—Ä–≤—ã–µ 60 —Å–∏–º–≤–æ–ª–æ–≤)
            original_title = news_data.get('title', '–ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–æ–≤–æ—Å—Ç–∏')
            processed_data['title'] = original_title[:60] + ('...' if len(original_title) > 60 else '')

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –∞–Ω–∏–º–∞—Ü–∏–∏
        try:
            if hasattr(self.provider, 'generate_structured_content'):
                structured_content = self.provider.generate_structured_content(news_data)
                logger.info(f"  ‚úÖ LLM generate_structured_content —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {bool(structured_content)}")
                processed_data['structured_content'] = structured_content
        except Exception as e:
            logger.error(f"  ‚ùå –û—à–∏–±–∫–∞ generate_structured_content: {e}")
            processed_data['structured_content'] = {}

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å ID {news_data.get('id')} —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ñ–∞–∫—Ç–æ–≤")
        return processed_data

    def batch_process_news(self, news_list: List[Dict]) -> List[Dict]:
        """–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π"""
        results = []

        for news in news_list:
            result = self.process_news_for_shorts(news)
            results.append(result)

        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(results)} –Ω–æ–≤–æ—Å—Ç–µ–π")
        return results

def main():
    """–¢–µ—Å—Ç–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    config_path = os.path.join(os.path.dirname(__file__), '..', 'config', 'config.yaml')

    try:
        processor = LLMProcessor(config_path)

        # –¢–µ—Å—Ç–æ–≤–∞—è –Ω–æ–≤–æ—Å—Ç—å
        test_news = {
            'id': 1,
            'title': '–ü—Ä–µ–∑–∏–¥–µ–Ω—Ç –°–®–ê –æ–±—ä—è–≤–∏–ª –æ –Ω–æ–≤—ã—Ö —Å–∞–Ω–∫—Ü–∏—è—Ö –ø—Ä–æ—Ç–∏–≤ –†–æ—Å—Å–∏–∏',
            'description': '–î–∂–æ –ë–∞–π–¥–µ–Ω –∑–∞—è–≤–∏–ª –æ –≤–≤–µ–¥–µ–Ω–∏–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –≤ –æ—Ç–≤–µ—Ç –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–±—ã—Ç–∏—è –Ω–∞ –£–∫—Ä–∞–∏–Ω–µ.',
            'source': 'Reuters',
            'published': datetime.now().isoformat(),
            'category': '–ü–æ–ª–∏—Ç–∏–∫–∞'
        }

        result = processor.process_news_for_shorts(test_news)

        print("–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
        print(json.dumps(result, indent=2, ensure_ascii=False))

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏: {e}")
        print(f"–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è {processor.config['llm']['api_key_env']}")

if __name__ == "__main__":
    main()
